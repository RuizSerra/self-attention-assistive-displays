</dt-appendix>
</body>
<script type="text/bibliography">



  @inproceedings{Tang2020,
    title = {Neuroevolution of Self-Interpretable Agents},
    url = {http://arxiv.org/abs/2003.08165},
    doi = {10.1145/3377930.3389847},
    abstract = {Inattentional blindness is the psychological phenomenon that causes one to miss things in plain sight. It is a consequence of the selective attention in perception that lets us remain focused on important parts of our world without distraction from irrelevant details. Motivated by selective attention, we study the properties of artificial agents that perceive the world through the lens of a self-attention bottleneck. By constraining access to only a small fraction of the visual input, we show that their policies are directly interpretable in pixel space. We find neuroevolution ideal for training self-attention architectures for vision-based reinforcement learning ({RL}) tasks, allowing us to incorporate modules that can include discrete, non-differentiable operations which are useful for our agent. We argue that self-attention has similar properties as indirect encoding, in the sense that large implicit weight matrices are generated from a small number of key-query parameters, thus enabling our agent to solve challenging vision based tasks with at least 1000x fewer parameters than existing methods. Since our agent attends to only task critical visual hints, they are able to generalize to environments where task irrelevant elements are modified while conventional methods fail. Videos of our results and source code available at https://attentionagent.github.io/},
    pages = {414--424},
    booktitle = {GECCO'20: Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
    author = {Tang, Yujin and Nguyen, Duong and Ha, David},
    urldate = {2020-05-01},
    date = {2020},
    keywords = {},
  }


  @article{Beattie2016,
    title = {DeepMind Lab},
    author = {Beattie, Charles and Leibo, Joel Z. and Teplyashin, Denis and Ward, Tom and Wainwright, Marcus and Küttler, Heinrich and Lefrancq, Andrew and Green, Simon and Valdés, Víctor and Sadik, Amir and Schrittwieser, Julian and Anderson, Keith and York, Sarah and Cant, Max and Cain, Adam and Bolton, Adrian and Gaffney, Stephen and King, Helen and Hassabis, Demis and Legg, Shane and Petersen, Stig},
    date = {2016},
    url = {http://arxiv.org/abs/1612.03801},
    abstract = {DeepMind Lab is a first-person 3D game platform designed for research and development of general artificial intelligence and machine learning systems. DeepMind Lab can be used to study how autonomous artificial agents may learn complex tasks in large, partially observed, and visually diverse worlds. DeepMind Lab has a simple and flexible API enabling creative task-designs and novel AI-designs to be explored and quickly iterated upon. It is powered by a fast and widely recognised game engine, and tailored for effective use by the research community.},
  }

  @article{Meijer1993,
  	title = {An Experimental System for Auditory Image},
    journaltitle = {IEEE Transactions on Biomedical Engineering},
    volume = {39},
  	number = {2},
  	author = {Meijer, Peter},
  	date = {1993},
    url = {https://www.seeingwithsound.com/voicebme.html},
  }



</script>
<script src="lib/blazy.js"></script>
<script>
  // blazy code
  var bLazy = new Blazy({
    success: function(){
      updateCounter();
    }
  });

  // not needed, only here to illustrate amount of loaded images
  var imageLoaded = 0;

  function updateCounter() {
    imageLoaded++;
    console.log("blazy image loaded: "+imageLoaded);
  }
</script>
