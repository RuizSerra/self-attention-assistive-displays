<!doctype html>
<meta charset="utf-8">
<style>
body {
  overflow-x: hidden;
}
.scroll-down {
  width: 80px;
  height: 40px;
  right: 10px;
  bottom: 10px;
  position: absolute;
  font-family: "Roboto","Helvetica Neue",Helvetica,Arial,sans-serif;
  font-size: 12px;
  font-weight: 300;
  color: #FFFFFF;
  opacity: 0;
  -webkit-transition: opacity 2s ease-in;
  -moz-transition: opacity 2s ease-in;
  -o-transition: opacity 2s ease-in;
  -ms-transition: opacity 2s ease-in;
  transition: opacity 2s ease-in;
}
.scroll-down span {
  margin-top: 5px;
  position: absolute;
  left: 50%;
  transform: translate(-100%, 0) rotate(45deg);
  transform-origin: 100% 100%;
  height: 2px;
  width: 10px;
  background: #FFFFFF;
}
.scroll-down span:nth-of-type(2) {
  transform-origin: 0 100%;
  transform: translate(0, 0) rotate(-45deg);
}
.spinner {
  position: absolute;
  height: 160px;
  width: 160px;
  -webkit-animation: rotation .6s infinite linear;
  -moz-animation: rotation .6s infinite linear;
  -o-animation: rotation .6s infinite linear;
  animation: rotation .6s infinite linear;
  border-left: 6px solid rgba(0, 174, 239, .15);
  border-right: 6px solid rgba(0, 174, 239, .15);
  border-bottom: 6px solid rgba(0, 174, 239, .15);
  border-top: 6px solid rgba(0, 174, 239, .8);
  border-radius: 100%;
  top: calc(50% - 100px);
  left: calc(50% - 80px);
  right: auto;
  bottom: auto;
}

@-webkit-keyframes rotation {
  from {
    -webkit-transform: rotate(0deg);
  }
  to {
    -webkit-transform: rotate(359deg);
  }
}
.transparent {
  opacity: 0;
}

figcaption {
  padding: 0.5em;
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
  text-align: left;
}

dt-article figcaption {
  padding: 0.5em;
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
  text-align: left;
}

dt-article figcaption a {
  color: rgba(0, 0, 0, 0.6);
}

dt-article figcaption b {
  font-weight: 600;
  color: rgba(0, 0, 0, 1.0);
}

*.unselectable {
    -moz-user-select: -moz-none;
    -khtml-user-select: none;
    -webkit-user-select: none;
    -o-user-select: none;
    user-select: none;
}
*.svgunselectable {
    -moz-user-select: -moz-none;
    -khtml-user-select: none;
    -webkit-user-select: none;
    -o-user-select: none;
    user-select: none;
    background: none;
    pointer-events: none;
}
</style>
<head>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta http-equiv="X-UA-Compatible" content="ie=edge" />

  <meta name="theme-color" content="#ffffff" />


  <!-- SEO -->
  <meta property="og:title" content="Learning vision processing for assistive displays through self-attention agents" />
  <meta property="og:type" content="article" />
  <meta property="og:description" content="Compact self-attention visual representations learnt through an evolutionary process." />
  <meta property="og:image" content="https://ruizserra.github.io/self-attention-assistive-displays/assets/png/TVCG-main-figure.png" />
  <meta property="og:url" content="https://ruizserra.github.io/self-attention-assistive-displays" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Learning vision processing for assistive displays through self-attention agents" />
  <meta name="twitter:description" content="Compact self-attention visual representations learnt through an evolutionary process." />
  <meta property="og:site_name" content="Jaime Ruiz Serra" />
  <meta name="twitter:image" content="https://ruizserra.github.io/self-attention-assistive-displays/assets/png/TVCG-main-figure.png" />

</head>
<link rel="stylesheet" href="css/katex.min.css">

<!--<script src="lib/jquery-1.12.4.min.js"></script>
<script src="lib/mobile-detect.min.js"></script>-->
<script src="lib/template.v1.js"></script>

<script type="text/front-matter">
  title: "Learning vision processing for assistive displays through self-attention agents"
  description: ""
</script>
<body>
  <div id="no_javasript_warning">
    <h3>This page requires Javascript. Please enable it for this webpage.</h3>
  </div>
  <script>
    document.getElementById("no_javasript_warning").style.display = "none";
  </script>


<dt-article id="dtbody">


<dt-byline class="l-page transparent"></dt-byline>
<h1>Learning vision processing for assistive displays through self-attention agents</h1>
<p></p>
<dt-byline class="l-page" id="authors_section" hidden>
<div class="byline">
  <div class="authors">
    <div class="author">
        <a class="name" href="http://jaime.rs/">Jaime Ruiz Serra</a>
        <a class="affiliation" href="https://www.swinburne.edu.au/research">Swinburne University</a>
    </div>
    <div class="author">
        <a class="name" href="">Jack White</a>
        <a class="affiliation" href="https://www.swinburne.edu.au/research">Swinburne University</a>
    </div>
    <div class="author">
        <a class="name" href="https://www.swinburne.edu.au/research/our-research/access-our-research/find-a-researcher-or-supervisor/researcher-profile/?id=spetrie">Stephen Petrie</a>
        <a class="affiliation" href="https://www.swinburne.edu.au/research/our-research/access-our-research/find-a-researcher-or-supervisor/researcher-profile/?id=spetrie">Swinburne University</a>
    </div>
    <div class="author">
        <a class="name" href="https://www.swinburne.edu.au/research/our-research/access-our-research/find-a-researcher-or-supervisor/researcher-profile/?id=tkam">Tatiana Kameneva</a>
        <a class="affiliation" href="https://www.swinburne.edu.au/research/our-research/access-our-research/find-a-researcher-or-supervisor/researcher-profile/?id=tkam">Swinburne University</a>
    </div>
    <div class="author">
        <a class="name" href="https://www.swinburne.edu.au/research/our-research/access-our-research/find-a-researcher-or-supervisor/researcher-profile/?id=cdmccarthy">Chris McCarthy</a>
        <a class="affiliation" href="https://www.swinburne.edu.au/research/our-research/access-our-research/find-a-researcher-or-supervisor/researcher-profile/?id=cdmccarthy">Swinburne University</a>
    </div>
  </div>
  <div class="date">
    <div class="month">September</div>
    <div class="year">2021</div>
  </div>
  <div class="date">
    <div class="month">IEEE-TVCG<dt-fn>IEEE Transactions on Visualization and Computer Graphics (under review)</dt-fn></div>
    <div class="year" style="color: #FF6C00;"><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=2945" target="_blank">paper</a></div>
  </div>
</div>
</dt-byline>
<h2>Problem statement</h2>
<div style="text-align: center;">
<img src="assets/png/TVCG-pipeline-1.png" style="margin: 0; width: 100%;" ></img>
<figcaption style="text-align: left; padding-top: 0;">
<span style="color: #00F">Assistive vision</span> consists of a camera that captures the real world, with
images processed by a video processing unit (VPU), converting them into scene
representations that can be rendered in assistive displays of different kinds.
We <span style="color: #FF9002">train a self-attention network in a RL context</span> to select important parts of
images for 3D navigation. Once trained, the SA network can be <span style="color: #C159B2">deployed</span>
to the visual prostheses’ VPU to perform the vision processing.
</figcaption>
</div>
<p>With the goal of simplifying visual representations of scenes
for navigation by selecting relevant features, we build upon
the work of Tang et al. <dt-cite key="Tang2020"></dt-cite>,
adapting the RL agent they introduced to enable training in a 3D navigation simulation environment. We
propose several methods to enhance the selected features,
and adapt the vision processing pipeline to present the obtained representations through different display modalities,
highlighting the method’s versatility. The resultant visualisations’ task-relevant features are enhanced, and those
irrelevant removed, effectively increasing the signal-to-noise ratio.</p>
<hr>
<h2>Training in simulation</h2>
<div style="text-align: center;">
<video class="b-lazy" src="assets/mp4/d2_10_0_overlay.mp4" type="video/mp4" autoplay muted playsinline loop style="margin: 0; width: 100%;" ></video>
<figcaption style="text-align: left; padding-top: 0;">
The self-attention models are trained in a reinforcement learning context by means of neuroevolution.
During training, the LSTM controller part of the network makes all decisions based solely on the
location of the top <i>K</i> most important image patches. This figure shows agent <i>d2</i> navigating environment
<i>NavMazeStatic01</i>.
</figcaption>
</div>
<p></p>
<div style="text-align: center;">
<img src="assets/png/d2_reward_vs_iteration.png" style="margin: 0; width: 100%;" ></img>
<figcaption style="text-align: left; padding-top: 0;">
The agents can learn to navigate the environment effectively with less than
100 million  training  observations (~200 iterations × 64 population/iter. ×
8 episodes/pop. × 900 observations/episode ≈ 92E6 observations), taking ~3h of wall time in our infrastructure.
This figure shows agent <i>d2</i> learning in environment <i>NavMazeStatic01</i>.
</figcaption>
</div>
<p></p>
<div style="text-align: center;">
<img src="assets/png/training-components.png" style="margin: 0; width: 100%;" ></img>
<figcaption style="text-align: left; padding-top: 0;">
To make the training process more scalable and marginally faster, we completely decoupled the CMA-ES
population from the training task queue. Task requests, including population member
identifier and agent parameters for the given population member are placed in a
queue and undertaken by compute workers on a FIFO basis. This makes the training
more flexible and suitable for distributed computing.
</figcaption>
</div>
<hr>
<h2>Vision processing in real-world scenes</h2>
<div style="text-align: center;">
<img src="assets/png/TVCG-main-figure.png" style="margin: 0; width: 100%;" ></img>
<figcaption style="text-align: left; padding-top: 0;">
The representations learnt in simulation translate to the real-world. Hyperparameters can
be adjusted in real time in the final application.
</figcaption>
</div>
<h3>Importance-luminance: ranking</h3>
<div style="text-align: center;">
<video class="b-lazy" src="assets/mp4/C1star_10_0_overlay.mp4" type="video/mp4" autoplay muted playsinline loop style="margin: 0; width: 100%;" ></video>
<figcaption style="text-align: left; padding-top: 0;">
Patch brightness is based on its importance ranking.
</figcaption>
</div>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>(</mo><mi>q</mi><mo separator="true">,</mo><msub><mi>Q</mi><mi>k</mi></msub><mo>)</mo><mo>=</mo><msub><mi>I</mi><mi>k</mi></msub><mi mathvariant="normal">/</mi><msub><mi>max</mi><mrow><mi>n</mi><mo>∈</mo><mi>N</mi></mrow></msub><mo>{</mo><msub><mi>I</mi><mi>n</mi></msub><mo>}</mo></mrow><annotation encoding="application/x-tex">f(q, Q_k) = I_k / \max_{n\in N}\{I_n\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.03588em;">q</span><span class="mpunct">,</span><span class="mord"><span class="mord mathit">Q</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mrel">=</span><span class="mord"><span class="mord mathit" style="margin-right:0.07847em;">I</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.07847em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathrm">/</span><span class="mop"><span class="mop">max</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">n</span><span class="mrel">∈</span><span class="mord mathit" style="margin-right:0.10903em;">N</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">{</span><span class="mord"><span class="mord mathit" style="margin-right:0.07847em;">I</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.07847em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">n</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">}</span></span></span></span></p>
<h3>Importance-luminance: log scale</h3>
<h3>Depth sampling: patch minimum</h3>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>(</mo><mi>q</mi><mo separator="true">,</mo><msub><mi>Q</mi><mi>k</mi></msub><mo>)</mo><mrow><mo>=</mo></mrow><msub><mi>min</mi><mrow><msup><mi>q</mi><mrow><mi mathvariant="normal">′</mi></mrow></msup><mo>∈</mo><msub><mi>Q</mi><mi>k</mi></msub></mrow></msub><mo>{</mo><mtext><mi mathvariant="normal">d</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">h</mi></mtext><mo>(</mo><msup><mi>q</mi><mrow><mi mathvariant="normal">′</mi></mrow></msup><mo>)</mo><mo>}</mo><mo>(</mo><msub><mi>I</mi><mi>k</mi></msub><mi mathvariant="normal">/</mi><msub><mi>max</mi><mrow><mi>n</mi><mo>∈</mo><mi>N</mi></mrow></msub><mo>{</mo><msub><mi>I</mi><mi>n</mi></msub><mo>}</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">f(q, Q_k) {=} \min_{q&#x27;\in Q_k}\{\text{depth}(q&#x27;)\} (I_k / \max_{n\in N}\{I_n\})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.751892em;"></span><span class="strut bottom" style="height:1.038em;vertical-align:-0.286108em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.03588em;">q</span><span class="mpunct">,</span><span class="mord"><span class="mord mathit">Q</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mord textstyle uncramped"><span class="mrel">=</span></span><span class="mop"><span class="mop">min</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">q</span><span class="vlist"><span style="top:-0.289em;margin-right:0.07142857142857144em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord scriptscriptstyle cramped"><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">∈</span><span class="mord"><span class="mord mathit">Q</span><span class="vlist"><span style="top:0.15122857142857138em;margin-right:0.07142857142857144em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">{</span><span class="text mord textstyle uncramped"><span class="mord mathrm">d</span><span class="mord mathrm">e</span><span class="mord mathrm">p</span><span class="mord mathrm">t</span><span class="mord mathrm">h</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">q</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">′</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mclose">}</span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.07847em;">I</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.07847em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathrm">/</span><span class="mop"><span class="mop">max</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">n</span><span class="mrel">∈</span><span class="mord mathit" style="margin-right:0.10903em;">N</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">{</span><span class="mord"><span class="mord mathit" style="margin-right:0.07847em;">I</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.07847em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">n</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">}</span><span class="mclose">)</span></span></span></span></p>
<h3>Depth sampling: per-pixel</h3>
<p>Sampling depth data from the input image at the patch location, using per-pixel depth data: <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>(</mo><mi>q</mi><mo separator="true">,</mo><msub><mi>Q</mi><mi>k</mi></msub><mo>)</mo><mrow><mo>=</mo></mrow><mtext><mi mathvariant="normal">d</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">h</mi></mtext><mo>(</mo><mi>q</mi><mo>)</mo><mo>(</mo><msub><mi>I</mi><mi>k</mi></msub><mi mathvariant="normal">/</mi><msub><mi>max</mi><mrow><mi>n</mi><mo>∈</mo><mi>N</mi></mrow></msub><mo>{</mo><msub><mi>I</mi><mi>n</mi></msub><mo>}</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">f(q, Q_k) {=} \text{depth}(q) (I_k / \max_{n\in N}\{I_n\})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.03588em;">q</span><span class="mpunct">,</span><span class="mord"><span class="mord mathit">Q</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mord textstyle uncramped"><span class="mrel">=</span></span><span class="text mord textstyle uncramped"><span class="mord mathrm">d</span><span class="mord mathrm">e</span><span class="mord mathrm">p</span><span class="mord mathrm">t</span><span class="mord mathrm">h</span></span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.03588em;">q</span><span class="mclose">)</span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.07847em;">I</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.07847em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathrm">/</span><span class="mop"><span class="mop">max</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">n</span><span class="mrel">∈</span><span class="mord mathit" style="margin-right:0.10903em;">N</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">{</span><span class="mord"><span class="mord mathit" style="margin-right:0.07847em;">I</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.07847em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">n</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">}</span><span class="mclose">)</span></span></span></span></p>
<div style="text-align: center;">
<video class="b-lazy" src="assets/mp4/rw-depth-pp-C4star_10_0_overlay.mp4" type="video/mp4" autoplay muted playsinline loop style="margin: 0; width: 100%;" ></video>
<figcaption style="text-align: left; padding-top: 0;">
Having selected the most important patches, we can sample per-pixel depth data for the final visualisation.
</figcaption>
</div>
<hr>
<h2>Display modalities</h2>
<h3>Simulated Phosphene Visualisation</h3>
<div style="text-align: center;">
<video class="b-lazy" src="assets/mp4/oko-k-thd-2-clip.mp4" type="video/mp4" autoplay muted playsinline loop style="margin: 0; width: 100%;" ></video>
<figcaption style="text-align: left; padding-top: 0;">
Simulated Phosphene Visualisation (SPV)
</figcaption>
</div>
<p></p>
<div style="text-align: left;">
<img src="assets/png/TVCG-SA-output-SPV.png" style="margin: 0; width: 100%;" ></img>
<figcaption style="text-align: left; padding-top: 0;">
SPV of different output modes (refer to Figure 5 in the paper).
</figcaption>
</div>
<h3>vOICe</h3>
<p><dt-cite key="Meijer1993"></dt-cite></p>
<hr>
<h2>Patch Analysis</h2>
<div style="text-align: center;">
<img src="assets/png/histogram_of_distances_to_centroids.png" style="margin: 0; width: 100%;" ></img>
<figcaption style="text-align: left; padding-top: 0;">
caption
</figcaption>
</div>
<div style="text-align: center;">
<img src="assets/png/histogram_of_importances_agents.png" style="margin: 0; width: 100%;" ></img>
<figcaption style="text-align: left; padding-top: 0;">
caption
</figcaption>
</div>
</dt-article>
<dt-appendix>
<h3>Acknowledgements</h3>
<p>The template for this supporting materials site is from <a href="https://github.com/attentionagent/attentionagent.github.io">Tang et al</a>.</p>
<p>The experiments in this work were performed on Swinburne University's <a href="https://supercomputing.swin.edu.au/ozstar/">OzStar high-performance computing system</a>.</p>
<h3 id="citation">Citation</h3>
<p>For attribution in academic contexts, please cite this work as:</p>
<pre class="citation short">Jaime Ruiz Serra and Jack White and Stephen Petrie and Tatiana Kameneva and Chris McCarthy,
Learning vision processing for assistive displays through self-attention agents, 2021.</pre>
<p>BibTeX citation</p>
<pre class="citation long">@article{RuizSerra2021,
  author = {Ruiz Serra, Jaime and
            White, Jack and
            Petrie, Stephen and
            Kameneva, Tatiana and
            McCarthy, Chris},
  title  = {Learning vision processing for assistive displays through self-attention agents},
  eprint = {},
  url    = {},
  note   = "\url{http://ruizserra.github.io/self-attention-assistive-displays}",
  year   = {2021}
}</pre>
<h3>Open Source Code</h3>
<p>Code to reproduce the results in this work TBD.</p>
<h3>Reuse</h3>
<p>Diagrams and text are licensed under Creative Commons Attribution <a href="https://creativecommons.org/licenses/by/4.0/">CC-BY 4.0</a> with the <a href="http://github.com/ruizserra/self-attention-assistive-displays/assets">source available on GitHub</a>, unless noted otherwise. The figures that have been reused from other sources don’t fall under this license and can be recognized by the citations in their caption.</p>
</dt-appendix>
</dt-appendix>
</body>
<script type="text/bibliography">







  @article{Stronks2013,
  	title = {The functional performance of the Argus {II} retinal prosthesis},
  	volume = {11},
  	pages = {23--30},
  	journaltitle = {Expert Review of Medical Devices},
  	author = {Stronks, H Christiaan and Dagnelie, Gislin},
  	date = {2013-05},
  }

  @article{Mills2017,
  	title = {Electronic retinal implants and artificial vision: journey and present},
  	volume = {31},
  	issn = {0950-222X},
  	pages = {1383--1398},
  	number = {10},
  	journaltitle = {Eye.},
  	author = {Mills, J O and Jalil, A and Stanga, P E},
  	date = {2017},
  	note = {Publisher: Nature Pub Group
  Place: London :},
  }

  @article{Stingl2013,
  	title = {Artificial vision with wirelessly powered subretinal electronic implant alpha-{IMS}},
  	volume = {280},
  	pages = {20130077},
  	journaltitle = {Proceedings of the Royal Society B: Biological Sciences},
  	author = {Stingl, Katarina and Bartz-Schmidt, Karl Ulrich and Besch, Dorothea and Braun, Angelika and Bruckmann, Anna and Gekeler, Florian and Greppmaier, Udo and Hipp, Stephanie and Hörtdörfer, Gernot and Kernstock, Christoph and Koitschev, Assen and Kusnyerik, Akos and Sachs, Helmut and Schatz, Andreas and Stingl, Krunoslav T and Peters, Tobias and Wilhelm, Barbara and Zrenner, Eberhart},
  	date = {2013-05},
  }

  @article{McCarthy2012,
  	title = {Time-to-contact maps for navigation with a low resolution visual prosthesis},
  	journaltitle = {Proceedings of the Annual International Conference of the {IEEE} Engineering in Medicine and Biology Society, {EMBS}},
  	author = {{McCarthy}, Chris and Barnes, Nick},
  	date = {2012},
  	file = {McCarthy_Barnes_2012_Time-to-contact maps for navigation with a low resolution visual prosthesis.pdf:/Users/jaime/Zotero/storage/HYLC4R4H/McCarthy_Barnes_2012_Time-to-contact maps for navigation with a low resolution visual prosthesis.pdf:application/pdf},
  }

  @article{Hu2014,
  	title = {Recognition of similar objects using simulated prosthetic vision},
  	volume = {38},
  	issn = {0160564X},
  	doi = {10.1111/aor.12147},
  	abstract = {Due to the limitations of existing techniques, even the most advanced visual prostheses, using several hundred electrodes to transmit signals to the visual pathway, restrict sensory function and visual information. To identify the bottlenecks and guide prosthesis designing, psychophysics simulations of a visual prosthesis in normally sighted individuals are desirable. In this study, psychophysical experiments of discriminating objects with similar profiles were used to test the effects of phosphene array parameters (spatial resolution, gray scale, distortion, and dropout rate) on visual information using simulated prosthetic vision. The results showed that the increase in spatial resolution and number of gray levels and the decrease in phosphene distortion and dropout rate improved recognition performance, and the accuracy is 78.5\% under the optimum condition (resolution: 32×32, gray level: 8, distortion: k=0, dropout: 0\%). In combined parameter tests, significant facial recognition accuracy was achieved for all the images with k=0.1 distortion and 10\% dropout. Compared with other experiments, we find that different objects do not show specific sensitivity to the changes of parameters and visual information is not nearly enough even under the optimum condition. The results suggests that higher spatial resolution and more gray levels are required for visual prosthetic devices and further research on image processing strategies to improve prosthetic vision is necessary, especially when the wearers have to accomplish more than simple visual tasks. © 2013 Wiley Periodicals, Inc. and International Center for Artificial Organs and Transplantation.},
  	pages = {159--167},
  	number = {2},
  	journaltitle = {Artificial Organs},
  	author = {Hu, Jie and Xia, Peng and Gu, Chaochen and Qi, Jin and Li, Sheng and Peng, Yinghong},
  	date = {2014},
  	pmid = {24033534},
  	keywords = {Object recognition, Psychophysics, Simulated prosthetic vision, Visual prosthesis},
  }

  @article{Kiral-Kornek2014,
  	title = {Improved visual performance in letter perception through edge orientation encoding in a retinal prosthesis simulation},
  	volume = {11},
  	issn = {17412560},
  	doi = {10.1088/1741-2560/11/6/066002},
  	number = {6},
  	journaltitle = {Journal of Neural Engineering},
  	author = {Kiral-Kornek, F I and Osullivan-Greene, E and Savage, C O and {McCarthy}, C and Grayden, D B and Burkitt, A N},
  	date = {2014},
  	note = {Publisher: Institute of Physics Publishing},
  }

  @article{Mace2015,
  	title = {Simulated Prosthetic Vision: The Benefits of Computer-Based Object Recognition and Localization},
  	volume = {39},
  	issn = {15251594},
  	doi = {10.1111/aor.12476},
  	abstract = {Clinical trials with blind patients implanted with a visual neuroprosthesis showed that even the simplest tasks were difficult to perform with the limited vision restored with current implants. Simulated prosthetic vision ({SPV}) is a powerful tool to investigate the putative functions of the upcoming generations of visual neuroprostheses. Recent studies based on {SPV} showed that several generations of implants will be required before usable vision is restored. However, none of these studies relied on advanced image processing. High-level image processing could significantly reduce the amount of information required to perform visual tasks and help restore visuomotor behaviors, even with current low-resolution implants. In this study, we simulated a prosthetic vision device based on object localization in the scene. We evaluated the usability of this device for object recognition, localization, and reaching. We showed that a very low number of electrodes (e.g., nine) are sufficient to restore visually guided reaching movements with fair timing (10s) and high accuracy. In addition, performance, both in terms of accuracy and speed, was comparable with 9 and 100 electrodes. Extraction of high level information (object recognition and localization) from video images could drastically enhance the usability of current visual neuroprosthesis. We suggest that this method-that is, localization of targets of interest in the scene-may restore various visuomotor behaviors. This method could prove functional on current low-resolution implants. The main limitation resides in the reliability of the vision algorithms, which are improving rapidly.},
  	pages = {E102--E113},
  	number = {7},
  	journaltitle = {Artificial Organs},
  	author = {Macé, Marc J.M. and Guivarch, Valérian and Denis, Grégoire and Jouffrais, Christophe},
  	date = {2015},
  	pmid = {25900238},
  	note = {Publisher: Blackwell Publishing Inc.},
  	keywords = {Simulated prosthetic vision, Computer vision, Blindness, Visual impairment, Visual neuroprosthesis},
  }

  @inproceedings{Horne2015,
  	title = {Semantic labelling to aid navigation in prosthetic vision},
  	volume = {2015-Novem},
  	isbn = {978-1-4244-9271-8},
  	doi = {10.1109/EMBC.2015.7319117},
  	abstract = {Current and near-term implantable prosthetic vision systems offer the potential to restore some visual function, but suffer from limited resolution and dynamic range of induced visual percepts. This can make navigating complex environments difficult for users. Using semantic labelling techniques, we demonstrate that a computer system can aid in obstacle avoidance, and localizing distant objects. Our system automatically classifies each pixel in a natural image into a semantic class, then produces an image from the induced visual percepts that highlights certain classes. This technique allows the user to clearly perceive the location of different types of objects in their field of view, and can be adapted for a range of navigation tasks.},
  	pages = {3379--3382},
  	booktitle = {Proceedings of the Annual International Conference of the {IEEE} Engineering in Medicine and Biology Society, {EMBS}},
  	publisher = {Institute of Electrical and Electronics Engineers Inc.},
  	author = {Horne, Lachlan and Alvarez, Jose M and {McCarthy}, Chris and Barnes, Nick},
  	date = {2015},
  	pmid = {26737017},
  	note = {{ISSN}: 1557170X},
  	file = {Horne et al_2015_Semantic labelling to aid navigation in prosthetic vision.pdf:/Users/jaime/Zotero/storage/4HRTPNZC/Horne et al_2015_Semantic labelling to aid navigation in prosthetic vision.pdf:application/pdf},
  }

  @article{Xia2015,
  	title = {Adaptation to Phosphene Parameters Based on Multi-Object Recognition Using Simulated Prosthetic Vision},
  	volume = {39},
  	issn = {15251594},
  	doi = {10.1111/aor.12504},
  	abstract = {Retinal prostheses for the restoration of functional vision are under development and visual prostheses targeting proximal stages of the visual pathway are also being explored. To investigate the experience with visual prostheses, psychophysical experiments using simulated prosthetic vision in normally sighted individuals are necessary. In this study, a helmet display with real-time images from a camera attached to the helmet provided the simulated vision, and experiments of recognition and discriminating multiple objects were used to evaluate visual performance under different parameters (gray scale, distortion, and dropout). The process of fitting and training with visual prostheses was simulated and estimated by adaptation to the parameters with time. The results showed that the increase in the number of gray scale and the decrease in phosphene distortion and dropout rate improved recognition performance significantly, and the recognition accuracy was 61.8±7.6\% under the optimum condition (gray scale: 8, distortion: k=0, dropout: 0\%). The adaption experiments indicated that the recognition performance was improved with time and the effect of adaptation to distortion was greater than dropout, which implies the difference of adaptation mechanism to the two parameters.},
  	pages = {1038--1045},
  	number = {12},
  	journaltitle = {Artificial Organs},
  	author = {Xia, Peng and Hu, Jie and Peng, Yinghong},
  	date = {2015},
  	pmid = {25912967},
  	keywords = {Neuroplasticity, Object recognition, Psychophysics, Retinal prostheses, Simulated prosthetic vision},
  }

  @article{Barnes2016,
  	title = {Vision function testing for a suprachoroidal retinal prosthesis: Effects of image filtering},
  	volume = {13},
  	issn = {17412552},
  	doi = {10.1088/1741-2560/13/3/036013},
  	abstract = {Objective. One strategy to improve the effectiveness of prosthetic vision devices is to process incoming images to ensure that key information can be perceived by the user. This paper presents the first comprehensive results of vision function testing for a suprachoroidal retinal prosthetic device utilizing of 20 stimulating electrodes. Further, we investigate whether using image filtering can improve results on a light localization task for implanted participants compared to minimal vision processing. No controlled implanted participant studies have yet investigated whether vision processing methods that are not task-specific can lead to improved results. Approach. Three participants with profound vision loss from retinitis pigmentosa were implanted with a suprachoroidal retinal prosthesis. All three completed multiple trials of a light localization test, and one participant completed multiple trials of acuity tests. The visual representations used were: Lanczos2 (a high quality Nyquist bandlimited downsampling filter); minimal vision processing ({MVP}); wide view regional averaging filtering ({WV}); scrambled; and, system off. Main results. Using Lanczos2, all three participants successfully completed a light localization task and obtained a significantly higher percentage of correct responses than using {MVP} () or with system off (). Further, in a preliminary result using Lanczos2, one participant successfully completed grating acuity and Landolt C tasks, and showed significantly better performance () compared to {WV}, scrambled and system off on the grating acuity task. Significance. Participants successfully completed vision tasks using a 20 electrode suprachoroidal retinal prosthesis. Vision processing with a Nyquist bandlimited image filter has shown an advantage for a light localization task. This result suggests that this and targeted, more advanced vision processing schemes may become important components of retinal prostheses to enhance performance. {ClinicalTrials}.gov Identifier: {NCT}01503576.},
  	pages = {36013},
  	number = {3},
  	journaltitle = {Journal of Neural Engineering},
  	author = {Barnes, Nick and Scott, Adele F and Lieby, Paulette and Petoe, Matthew A and {McCarthy}, Chris and Stacey, Ashley and Ayton, Lauren N and Sinclair, Nicholas C and Shivdasani, Mohit N and Lovell, Nigel H and {McDermott}, Hugh J and Walker, Janine G},
  	date = {2016-04},
  	pmid = {27108845},
  	note = {Publisher: {IOP} Publishing},
  	keywords = {computer vision, blindness, image filtering, retinal prosthetic, retinitis pigmentosa, vision processing for prosthetic vision},
  }

  @article{Barry2016,
  	title = {Hand-Camera Coordination Varies over Time in Users of the Argus® {II} Retinal Prosthesis System},
  	volume = {10},
  	issn = {1662-5137},
  	url = {https://www.frontiersin.org/article/10.3389/fnsys.2016.00041},
  	doi = {10.3389/fnsys.2016.00041},
  	pages = {41},
  	journaltitle = {Frontiers in Systems Neuroscience},
  	author = {Barry, Michael P and Dagnelie, Gislin},
  	date = {2016},
  }

  @inproceedings{Kheradvar2018,
  	title = {{SPV} experiments for a retinal visual prosthesis: Introducing a new feature: 'Blinking'},
  	volume = {2017-Novem},
  	isbn = {978-1-5386-4404-1},
  	url = {https://ieeexplore.ieee.org/document/8342352},
  	doi = {10.1109/IranianMVIP.2017.8342352},
  	abstract = {Researchers in the field of visual prostheses need a Simulated Prosthetic Vision ({SPV}) setup to evaluate their image processing algorithms on people with normal vision before implanting any retinal prostheses. In this paper, an {SPV} developed for a visual prosthesis is introduced and the associated experimental results are reported. These experiments are designed to examine the efficacy of two down sampling methods, the mode down sampling ({MDS}) and the nearest neighbor method. The experiments are conducted in a corridor including some obstacles. Three levels of difficulties are considered for each of the two methods and two measures are used to compare the efficiency of the methods: Percentage of Preferred Walking Speed ({PPWS}), and Total Hit Count ({THC}). The qualitative and quantitative results reported in this paper reveal that the controlled blinking of phosphenes would present additional information to help the patients.},
  	pages = {15--19},
  	booktitle = {Iranian Conference on Machine Vision and Image Processing, {MVIP}},
  	author = {Kheradvar, Benyamin and Mousavinia, Amir and Sodagar, Amir M},
  	date = {2018-05},
  	note = {{ISSN}: 21666784},
  	keywords = {Visual prosthesis, Image processing, Down sampling, Retinal implant, Simulated Prosthetic Vision ({SPV})},
  }

  @inproceedings{Perez-Yus2017,
  	title = {Depth and Motion Cues with Phosphene Patterns for Prosthetic Vision},
  	volume = {2018-Janua},
  	isbn = {978-1-5386-1034-3},
  	doi = {10.1109/ICCVW.2017.179},
  	abstract = {Recent research demonstrates that visual prostheses are able to provide visual perception to people with some kind of blindness. In visual prostheses, image information from the scene is transformed to a phosphene pattern to be sent to the implant. This is a complex problem where the main challenge is the very limited spatial and intensity resolution. Moreover, depth perception, which is relevant to perform agile navigation, is lost and codifying the semantic information to phosphene patterns remains an open problem. In this work, we consider the framework of perception for navigation where aspects such as obstacle avoidance are critical. We propose using a head-mounted {RGB}-D camera to detect free-space, obstacles and scene direction in front of the user. The main contribution is a new approach to represent depth information and provide motion cues by using particular phosphene patterns. The effectiveness of this approach is tested in simulation with real data from indoor environments.},
  	pages = {1516--1525},
  	booktitle = {Proceedings - 2017 {IEEE} International Conference on Computer Vision Workshops, {ICCVW} 2017},
  	author = {Perez-Yus, Alejandro and Bermudez-Cameo, Jesus and Guerrero, Jose J. and Lopez-Nicolas, Gonzalo},
  	date = {2017},
  }

  @inproceedings{Bermudez-Cameo2017,
  	location = {Cham},
  	title = {{RGB}-D computer vision techniques for simulated prosthetic vision},
  	volume = {10255 {LNCS}},
  	isbn = {978-3-319-58837-7},
  	doi = {10.1007/978-3-319-58838-4_47},
  	abstract = {Recent research on visual prosthesis demonstrates the possibility of providing visual perception to people with certain blindness. Bypassing the damaged part of the visual path, electrical stimulation provokes spot percepts known as phosphenes. Due to physiological and technological limitations the information received by patients has very low resolution and reduced dynamic range. In this context, the inclusion of new computer vision techniques to improve the semantic content in this information channel is an active and open key topic. In this paper, we present a system for Simulated Prosthetic Vision based on a head-mounted display with an {RGB}-D camera, and two tools, one focused on human interaction and the other oriented to navigation, exploring different proposals of phosphenic representations.},
  	pages = {427--436},
  	booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  	publisher = {Springer International Publishing},
  	author = {Bermudez-Cameo, Jesus and Badias-Herbera, Alberto and Guerrero-Viu, Manuel and Lopez-Nicolas, Gonzalo and Guerrero, Jose J},
  	date = {2017},
  	note = {{ISSN}: 16113349},
  	keywords = {Simulated prosthetic vision, Head-mounted displays, {RGB}-D vision},
  }

  @article{Irons2017,
  	title = {Face identity recognition in simulated prosthetic vision is poorer than previously reported and can be improved by caricaturing},
  	volume = {137},
  	issn = {0042-6989},
  	url = {http://www.sciencedirect.com/science/article/pii/S0042698917301116},
  	doi = {https://doi.org/10.1016/j.visres.2017.06.002},
  	pages = {61 -- 79},
  	journaltitle = {Vision Research},
  	author = {Irons, Jessica L and Gradden, Tamara and Zhang, Angel and He, Xuming and Barnes, Nick and Scott, Adele F and {McKone}, Elinor},
  	date = {2017},
  	keywords = {Caricaturing, Face recognition, Prosthetic vision, Retinal prosthesis},
  }

  @article{Vergnieux2017,
  	title = {Simplification of Visual Rendering in Simulated Prosthetic Vision Facilitates Navigation},
  	volume = {41},
  	issn = {15251594},
  	doi = {10.1111/aor.12868},
  	abstract = {Visual neuroprostheses are still limited and simulated prosthetic vision ({SPV}) is used to evaluate potential and forthcoming functionality of these implants. {SPV} has been used to evaluate the minimum requirement on visual neuroprosthetic characteristics to restore various functions such as reading, objects and face recognition, object grasping, etc. Some of these studies focused on obstacle avoidance but only a few investigated orientation or navigation abilities with prosthetic vision. The resolution of current arrays of electrodes is not sufficient to allow navigation tasks without additional processing of the visual input. In this study, we simulated a low resolution array (15 × 18 electrodes, similar to a forthcoming generation of arrays) and evaluated the navigation abilities restored when visual information was processed with various computer vision algorithms to enhance the visual rendering. Three main visual rendering strategies were compared to a control rendering in a wayfinding task within an unknown environment. The control rendering corresponded to a resizing of the original image onto the electrode array size, according to the average brightness of the pixels. In the first rendering strategy, vision distance was limited to 3, 6, or 9 m, respectively. In the second strategy, the rendering was not based on the brightness of the image pixels, but on the distance between the user and the elements in the field of view. In the last rendering strategy, only the edges of the environments were displayed, similar to a wireframe rendering. All the tested renderings, except the 3 m limitation of the viewing distance, improved navigation performance and decreased cognitive load. Interestingly, the distance-based and wireframe renderings also improved the cognitive mapping of the unknown environment. These results show that low resolution implants are usable for wayfinding if specific computer vision algorithms are used to select and display appropriate information regarding the environment.},
  	pages = {852--861},
  	number = {9},
  	journaltitle = {Artificial Organs},
  	author = {Vergnieux, Victor and Macé, Marc J.‐M. and Jouffrais, Christophe},
  	date = {2017},
  	pmid = {28321887},
  	keywords = {Navigation, Retinal implant, Blind, Computer vision, Spatial cognition, Visual neuroprostheses, Wayfinding},
  }

  @article{Kim2017,
  	title = {Spatiotemporal Pixelization to Increase the Recognition Score of Characters for Retinal Prostheses},
  	volume = {17},
  	issn = {14248220},
  	doi = {10.3390/s17102439},
  	abstract = {Most of the retinal prostheses use a head-fixed camera and a video processing unit. Some studies proposed various image processing methods to improve visual perception for patients. However, previous studies only focused on using spatial information. The present study proposes a spatiotemporal pixelization method mimicking fixational eye movements to generate stimulation images for artificial retina arrays by combining spatial and temporal information. Input images were sampled with a resolution that was four times higher than the number of pixel arrays. We subsampled this image and generated four different phosphene images. We then evaluated the recognition scores of characters by sequentially presenting phosphene images with varying pixel array sizes (6 × 6, 8 × 8 and 10 × 10) and stimulus frame rates (10 Hz, 15 Hz, 20 Hz, 30 Hz, and 60 Hz). The proposed method showed the highest recognition score at a stimulus frame rate of approximately 20 Hz. The method also significantly improved the recognition score for complex characters. This method provides a new way to increase practical resolution over restricted spatial resolution by merging the higher resolution image into high-frame time slots.},
  	pages = {2439},
  	number = {10},
  	journaltitle = {Sensors (Basel, Switzerland)},
  	author = {Kim, Hyun Seok and Park, Kwang Suk},
  	date = {2017},
  	pmid = {29073735},
  	keywords = {character recognition, retinal prosthesis, spatiotemporal, stimulus frame rates, subsampling},
  }

  @article{Li2017,
  	title = {A real-time image optimization strategy based on global saliency detection for artificial retinal prostheses},
  	volume = {415-416},
  	issn = {0020-0255},
  	url = {http://www.sciencedirect.com/science/article/pii/S0020025516317571},
  	doi = {https://doi.org/10.1016/j.ins.2017.06.014},
  	pages = {1 -- 18},
  	journaltitle = {Information Sciences},
  	author = {Li, Heng and Han, Tingting and Wang, Jing and Lu, Zhuofan and Cao, Xiaofei and Chen, Yao and Li, Liming and Zhou, Chuanqing and Chai, Xinyu},
  	date = {2017},
  	keywords = {Retinal prostheses, Simulated prosthetic vision, Eye-hand coordination, Saliency detection},
  }

  @article{Li2018,
  	title = {Image processing strategies based on saliency segmentation for object recognition under simulated prosthetic vision},
  	volume = {84},
  	issn = {0933-3657},
  	url = {http://www.sciencedirect.com/science/article/pii/S0933365716304195},
  	doi = {https://doi.org/10.1016/j.artmed.2017.11.001},
  	pages = {64 -- 78},
  	journaltitle = {Artificial Intelligence in Medicine},
  	author = {Li, Heng and Su, Xiaofan and Wang, Jing and Kan, Han and Han, Tingting and Zeng, Yajie and Chai, Xinyu},
  	date = {2018},
  	keywords = {Simulated prosthetic vision, Visual prosthesis, Saliency segmentation, Image processing strategy, Objects recognition},
  }

  @article{Sanchez-Garcia2020,
  	title = {Semantic and structural image segmentation for prosthetic vision},
  	volume = {15},
  	issn = {19326203},
  	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6988941/},
  	doi = {10.1371/journal.pone.0227677},
  	abstract = {Prosthetic vision is being applied to partially recover the retinal stimulation of visually impaired people. However, the phosphenic images produced by the implants have very limited information bandwidth due to the poor resolution and lack of color or contrast. The ability of object recognition and scene understanding in real environments is severely restricted for prosthetic users. Computer vision can play a key role to overcome the limitations and to optimize the visual information in the prosthetic vision, improving the amount of information that is presented. We present a new approach to build a schematic representation of indoor environments for simulated phosphene images. The proposed method combines a variety of convolutional neural networks for extracting and conveying relevant information about the scene such as structural informative edges of the environment and silhouettes of segmented objects. Experiments were conducted with normal sighted subjects with a Simulated Prosthetic Vision system. The results show good accuracy for object recognition and room identification tasks for indoor scenes using the proposed approach, compared to other image processing methods.},
  	number = {1},
  	journaltitle = {{PLoS} {ONE}},
  	author = {Sanchez-Garcia, Melani and Martinez-Cantin, Ruben and Guerrero, Jose J},
  	date = {2020-05},
  	pmid = {31995568},
  }

  @inproceedings{White2019,
  	title = {Deep reinforcement learning for task-based feature learning in prosthetic vision},
  	isbn = {978-1-5386-1311-5},
  	doi = {10.1109/EMBC.2019.8856541},
  	abstract = {Developing hand-crafted visual features to enhance perception with prosthetic vision devices can often miss important aspects of a given task. Retinal implants suffer from the need to create low-dimensional features for elaborate tasks such as navigation in complex environments. Using Deep Reinforcement Learning ({DRL}), visual features are learnt through task-based simulations that remove the ambiguity of inferring the visual information most crucial to a specific activity. Learning task-based features ensures that the visual information is salient to the tasks an implant recipient may be undertaking and eliminates potentially redundant features. In this paper, we focus specifically on basic orientation and mobility, and the methods for feature learning and visualisation in structured 3D environments. We propose a new model for learning visual features through task-based simulations and show that learnt features can be transferred directly to real {RGB}-D images. We demonstrate this new scalable approach for feature learning in simulation and open the possibility for more complex simulations of more complex tasks in the future.},
  	pages = {2809--2812},
  	booktitle = {Proceedings of the Annual International Conference of the {IEEE} Engineering in Medicine and Biology Society, {EMBS}},
  	author = {White, Jack and Kameneva, Tatiana and {McCarthy}, Chris},
  	date = {2019},
  	pmid = {31946477},
  	note = {{ISSN}: 1557170X},
  	file = {White et al_2019_Deep reinforcement learning for task-based feature learning in prosthetic vision.pdf:/Users/jaime/Zotero/storage/STUV2PQU/White et al_2019_Deep reinforcement learning for task-based feature learning in prosthetic vision.pdf:application/pdf},
  }

  @inproceedings{Ying2018,
  	title = {Recognition of virtual maze scene under simulated prosthetic vision},
  	pages = {1--5},
  	booktitle = {2018 Tenth International Conference on Advanced Computational Intelligence ({ICACI})},
  	author = {Ying, Z and Xiulin, G and Qi, L and Guangqi, J},
  	date = {2018},
  }

  @book{Altman1999,
  	title = {Constrained Markov decision processes},
  	publisher = {Chapman \& Hall/Crc},
  	author = {Altman, Eitan},
  	date = {1999},
  }

  @article{Arulkumaran2017,
  	title = {Deep Reinforcement Learning: A Brief Survey},
  	volume = {34},
  	pages = {26--38},
  	journaltitle = {{IEEE} Signal Processing Magazine},
  	author = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
  	date = {2017-05},
  }

  @book{Russell2016,
  	title = {Artificial intelligence : a modern approach},
  	publisher = {Pearson},
  	author = {Russell, Stuart J and Norvig, Peter},
  	date = {2016},
  }

  @misc{Zeiler2013,
  	title = {Visualizing and Understanding Convolutional Networks},
  	url = {https://arxiv.org/abs/1311.2901},
  	author = {Zeiler, Matthew D and Fergus, Rob},
  	date = {2013},
  }

  @misc{Lipton2015,
  	title = {A Critical Review of Recurrent Neural Networks for Sequence Learning},
  	url = {https://arxiv.org/abs/1506.00019},
  	author = {Lipton, Zachary C and Berkowitz, John and Elkan, Charles},
  	date = {2015},
  }

  @misc{Yosinski2015,
  	title = {Understanding Neural Networks Through Deep Visualization},
  	url = {https://arxiv.org/abs/1506.06579},
  	author = {Yosinski, Jason and Clune, Jeff and Nguyen, Anh and Fuchs, Thomas and Lipson, Hod},
  	date = {2015},
  }

  @misc{Schulman2017,
  	title = {Proximal Policy Optimization Algorithms},
  	url = {https://arxiv.org/abs/1707.06347},
  	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  	date = {2017},
  }

  @article{Luo2017,
  	title = {Understanding the Effective Receptive Field in Deep Convolutional Neural Networks},
  	url = {https://arxiv.org/abs/1701.04128},
  	journaltitle = {{arXiv}:1701.04128 [cs]},
  	author = {Luo, Wenjie and Li, Yujia and Urtasun, Raquel and Zemel, Richard},
  	date = {2017-05},
  }

  @book{Sutton2018,
  	title = {Reinforcement learning : an introduction},
  	publisher = {The Mit Press},
  	author = {Sutton, Richard S and Barto, Andrew},
  	date = {2018},
  }

  @article{Mnih2015,
  	title = {Human-level control through deep reinforcement learning},
  	volume = {518},
  	pages = {529--533},
  	journaltitle = {Nature},
  	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  	date = {2015-05},
  }

  @article{Ivanov2019,
  	title = {Modern Deep Reinforcement Learning Algorithms},
  	url = {https://arxiv.org/abs/1906.10025},
  	journaltitle = {{arXiv}:1906.10025 [cs, stat]},
  	author = {Ivanov, Sergey and D'yakonov, Alexander},
  	date = {2019-05},
  }

  @inproceedings{Tang2020,
  	title = {Neuroevolution of Self-Interpretable Agents},
  	url = {http://arxiv.org/abs/2003.08165},
  	doi = {10.1145/3377930.3389847},
  	abstract = {Inattentional blindness is the psychological phenomenon that causes one to miss things in plain sight. It is a consequence of the selective attention in perception that lets us remain focused on important parts of our world without distraction from irrelevant details. Motivated by selective attention, we study the properties of artificial agents that perceive the world through the lens of a self-attention bottleneck. By constraining access to only a small fraction of the visual input, we show that their policies are directly interpretable in pixel space. We find neuroevolution ideal for training self-attention architectures for vision-based reinforcement learning ({RL}) tasks, allowing us to incorporate modules that can include discrete, non-differentiable operations which are useful for our agent. We argue that self-attention has similar properties as indirect encoding, in the sense that large implicit weight matrices are generated from a small number of key-query parameters, thus enabling our agent to solve challenging vision based tasks with at least 1000x fewer parameters than existing methods. Since our agent attends to only task critical visual hints, they are able to generalize to environments where task irrelevant elements are modified while conventional methods fail. Videos of our results and source code available at https://attentionagent.github.io/},
  	pages = {414--424},
  	booktitle = {{GECCO} '20: Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
  	author = {Tang, Yujin and Nguyen, Duong and Ha, David},
  	urldate = {2020-05-01},
  	date = {2020},
  	keywords = {★},
  	file = {Tang et al_2020_Neuroevolution of Self-Interpretable Agents.pdf:/Users/jaime/Zotero/storage/2G77BZ5U/Tang et al_2020_Neuroevolution of Self-Interpretable Agents.pdf:application/pdf},
  }

  @article{Sorokin2015,
  	title = {Deep Attention Recurrent Q-Network},
  	url = {http://arxiv.org/abs/1512.01693},
  	abstract = {A deep learning approach to reinforcement learning led to a general learner able to train on visual input to play a variety of arcade games at the human and superhuman levels. Its creators at the Google {DeepMind}'s team called the approach: Deep Q-Network ({DQN}). We present an extension of {DQN} by "soft" and "hard" attention mechanisms. Tests of the proposed Deep Attention Recurrent Q-Network ({DARQN}) algorithm on multiple Atari 2600 games show level of performance superior to that of {DQN}. Moreover, built-in attention mechanisms allow a direct online monitoring of the training process by highlighting the regions of the game screen the agent is focusing on when making decisions.},
  	author = {Sorokin, Ivan and Seleznev, Alexey and Pavlov, Mikhail and Fedorov, Aleksandr and Ignateva, Anastasiia},
  	urldate = {2020-05-02},
  	date = {2015-12-05},
  	keywords = {★},
  	file = {Sorokin et al_2015_Deep Attention Recurrent Q-Network.pdf:/Users/jaime/Zotero/storage/FQ7D75K7/Sorokin et al_2015_Deep Attention Recurrent Q-Network.pdf:application/pdf},
  }

  @article{Zambaldi,
  	title = {Deep Reinforcement Learning with Relational Inductive Biases},
  	abstract = {We introduce an approach for augmenting model-free deep reinforcement learning agents with a mechanism for relational reasoning over structured representations, which improves performance, learning efficiency, generalization, and interpretabil-ity. Our architecture encodes an image as a set of vectors, and applies an iterative message-passing procedure to discover and reason about relevant entities and relations in a scene. In six of seven {StarCraft} {II} Learning Environment mini-games, our agent achieved state-of-the-art performance, and surpassed human grandmaster-level on four. In a novel navigation and planning task, our agent's performance and learning efficiency far exceeded non-relational baselines, it was able to generalize to more complex scenes than it had experienced during training. Moreover, when we examined its learned internal representations, they reflected important structure about the problem and the agent's intentions. The main contribution of this work is to introduce techniques for representing and reasoning about states in model-free deep reinforcement learning agents via relational inductive biases. Our experiments show this approach can offer advantages in efficiency, generalization, and interpretability, and can scale up to meet some of the most challenging test environments in modern artificial intelligence.},
  	author = {Zambaldi, Vinicius and Raposo, David and Santoro, Adam and Bapst, Victor and Li, Yujia and Babuschkin, Igor and Tuyls, Karl and Reichert, David and Lillicrap, Timothy and Lockhart, Edward and Shanahan, Murray and Langston, Victoria and Pascanu, Razvan and Botvinick, Matthew and Vinyals, Oriol and Battaglia, Peter},
  	file = {Zambaldi et al_Deep Reinforcement Learning with Relational Inductive Biases.pdf:/Users/jaime/Zotero/storage/8PBRWK6C/Zambaldi et al_Deep Reinforcement Learning with Relational Inductive Biases.pdf:application/pdf},
  }

  @article{Chai2019,
  	title = {Patchwork: A Patch-wise Attention Network for Efficient Object Detection and Segmentation in Video Streams},
  	volume = {2019-Octob},
  	url = {http://arxiv.org/abs/1904.01784},
  	abstract = {Recent advances in single-frame object detection and segmentation techniques have motivated a wide range of works to extend these methods to process video streams. In this paper, we explore the idea of hard attention aimed for latency-sensitive applications. Instead of reasoning about every frame separately, our method selects and only processes a small sub-window of the frame. Our technique then makes predictions for the full frame based on the sub-windows from previous frames and the update from the current sub-window. The latency reduction by this hard attention mechanism comes at the cost of degraded accuracy. We made two contributions to address this. First, we propose a specialized memory cell that recovers lost context when processing sub-windows. Secondly, we adopt a Q-learning-based policy training strategy that enables our approach to intelligently select the sub-windows such that the staleness in the memory hurts the performance the least. Our experiments suggest that our approach reduces the latency by approximately four times without significantly sacrificing the accuracy on the {ImageNet} {VID} video object detection dataset and the {DAVIS} video object segmentation dataset. We further demonstrate that we can reinvest the saved computation into other parts of the network, and thus resulting in an accuracy increase at a comparable computational cost as the original system and beating other recently proposed state-of-the-art methods in the low latency range.},
  	pages = {3414--3423},
  	journaltitle = {Proceedings of the {IEEE} International Conference on Computer Vision},
  	author = {Chai, Yuning},
  	urldate = {2020-05-02},
  	date = {2019-04-03},
  	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
  	file = {Chai_2019_Patchwork.pdf:/Users/jaime/Zotero/storage/BUJZXYB6/Chai_2019_Patchwork.pdf:application/pdf},
  }

  @article{Bello2019,
  	title = {Attention Augmented Convolutional Networks},
  	volume = {2019-Octob},
  	url = {http://arxiv.org/abs/1904.09925},
  	abstract = {Convolutional networks have been the paradigm of choice in many computer vision applications. The convolution operation however has a significant weakness in that it only operates on a local neighborhood, thus missing global information. Self-attention, on the other hand, has emerged as a recent advance to capture long range interactions, but has mostly been applied to sequence modeling and generative modeling tasks. In this paper, we consider the use of self-attention for discriminative visual tasks as an alternative to convolutions. We introduce a novel two-dimensional relative self-attention mechanism that proves competitive in replacing convolutions as a stand-alone computational primitive for image classification. We find in control experiments that the best results are obtained when combining both convolutions and self-attention. We therefore propose to augment convolutional operators with this self-attention mechanism by concatenating convolutional feature maps with a set of feature maps produced via self-attention. Extensive experiments show that Attention Augmentation leads to consistent improvements in image classification on {ImageNet} and object detection on {COCO} across many different models and scales, including {ResNets} and a state-of-the art mobile constrained network, while keeping the number of parameters similar. In particular, our method achieves a \$1.3{\textbackslash}\%\$ top-1 accuracy improvement on {ImageNet} classification over a {ResNet}50 baseline and outperforms other attention mechanisms for images such as Squeeze-and-Excitation. It also achieves an improvement of 1.4 {mAP} in {COCO} Object Detection on top of a {RetinaNet} baseline.},
  	pages = {3285--3294},
  	journaltitle = {Proceedings of the {IEEE} International Conference on Computer Vision},
  	author = {Bello, Irwan and Zoph, Barret and Vaswani, Ashish and Shlens, Jonathon and Le, Quoc V.},
  	urldate = {2020-05-02},
  	date = {2019-04-22},
  	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
  	keywords = {★},
  	file = {Bello et al_2019_Attention Augmented Convolutional Networks.pdf:/Users/jaime/Zotero/storage/FSBUNIAY/Bello et al_2019_Attention Augmented Convolutional Networks.pdf:application/pdf},
  }

  @article{Ding2019,
  	title = {Improving Semantic Segmentation of Aerial Images Using Patch-based Attention},
  	url = {http://arxiv.org/abs/1911.08877},
  	abstract = {The trade-off between feature representation power and spatial localization accuracy is crucial for the dense classification/semantic segmentation of aerial images. High-level features extracted from the late layers of a neural network are rich in semantic information, yet have blurred spatial details; low-level features extracted from the early layers of a network contain more pixel-level information, but are isolated and noisy. It is therefore difficult to bridge the gap between high and low-level features due to their difference in terms of physical information content and spatial distribution. In this work, we contribute to solve this problem by enhancing the feature representation in two ways. On the one hand, a patch attention module ({PAM}) is proposed to enhance the embedding of context information based on a patch-wise calculation of local attention. On the other hand, an attention embedding module ({AEM}) is proposed to enrich the semantic information of low-level features by embedding local focus from high-level features. Both of the proposed modules are light-weight and can be applied to process the extracted features of convolutional neural networks ({CNNs}). Experiments show that, by integrating the proposed modules into the baseline Fully Convolutional Network ({FCN}), the resulting local attention network ({LANet}) greatly improves the performance over the baseline and outperforms other attention based methods on two aerial image datasets.},
  	author = {Ding, Lei and Tang, Hao and Bruzzone, Lorenzo},
  	urldate = {2020-05-02},
  	date = {2019-11-20},
  	file = {Ding et al_2019_Improving Semantic Segmentation of Aerial Images Using Patch-based Attention.pdf:/Users/jaime/Zotero/storage/4AJAHGME/Ding et al_2019_Improving Semantic Segmentation of Aerial Images Using Patch-based Attention.pdf:application/pdf},
  }

  @article{Gessert2020,
  	title = {Skin Lesion Classification Using {CNNs} with Patch-Based Attention and Diagnosis-Guided Loss Weighting},
  	volume = {67},
  	issn = {15582531},
  	doi = {10.1109/TBME.2019.2915839},
  	abstract = {This paper addresses two key problems of skin lesion classification. The first problem is the effective use of high-resolution images with pretrained standard architectures for image classification. The second problem is the high-class imbalance encountered in real-world multi-class datasets. Methods: To use high-resolution images, we propose a novel patch-based attention architecture that provides global context between small, high-resolution patches. We modify three pretrained architectures and study the performance of patch-based attention. To counter class imbalance problems, we compare oversampling, balanced batch sampling, and class-specific loss weighting. Additionally, we propose a novel diagnosis-guided loss weighting method that takes the method used for ground-truth annotation into account. Results: Our patch-based attention mechanism outperforms previous methods and improves the mean sensitivity by {\textbackslash}text\{7\}{\textbackslash}\%. Class balancing significantly improves the mean sensitivity and we show that our diagnosis-guided loss weighting method improves the mean sensitivity by {\textbackslash}text\{3\}{\textbackslash}\% over normal loss balancing. Conclusion: The novel patch-based attention mechanism can be integrated into pretrained architectures and provides global context between local patches while outperforming other patch-based methods. Hence, pretrained architectures can be readily used with high-resolution images without downsampling. The new diagnosis-guided loss weighting method outperforms other methods and allows for effective training when facing class imbalance. Significance: The proposed methods improve automatic skin lesion classification. They can be extended to other clinical applications where high-resolution image data and class imbalance are relevant.},
  	pages = {495--503},
  	number = {2},
  	journaltitle = {{IEEE} Transactions on Biomedical Engineering},
  	author = {Gessert, Nils and Sentker, Thilo and Madesta, Frederic and Schmitz, Rudiger and Kniep, Helge and Baltruschat, Ivo and Werner, Rene and Schlaefer, Alexander},
  	urldate = {2020-05-02},
  	date = {2020-02-01},
  	note = {Publisher: {IEEE} Computer Society},
  	keywords = {deep learning, attention, dermoscopy, Skin lesion classification},
  	file = {Gessert et al_2020_Skin Lesion Classification Using CNNs with Patch-Based Attention and.pdf:/Users/jaime/Zotero/storage/H3N9984V/Gessert et al_2020_Skin Lesion Classification Using CNNs with Patch-Based Attention and.pdf:application/pdf},
  }

  @article{Schmidhuber1997,
  	title = {Discovering neural nets with low Kolmogorov complexity and high generalization capability},
  	volume = {10},
  	issn = {08936080},
  	doi = {10.1016/S0893-6080(96)00127-X},
  	abstract = {Some basic concepts of algorithmic complexity theory relevant to machine learning are reviewed along with the Solomon-Levin distribution (or universal prior) which deals with the prior problem. The universal prior leads to a probabilistic method for finding algorithmically simple problem solutions with high generalization capability. The method is based on Levin complexity and inspired by Levin's optimal universal search algorithm. For a given problem, solution candidates are computed by efficient self sizing programs that influence their own runtime and storage size. The method, at least with certain toy problems where it is computationally feasible, can lead to unmatchable generalization results.},
  	pages = {857--873},
  	number = {5},
  	journaltitle = {Neural Networks},
  	author = {Schmidhuber, Jürgen},
  	urldate = {2020-05-02},
  	date = {1997-07-01},
  	pmid = {12662875},
  	note = {Publisher: Elsevier Science Ltd},
  	keywords = {Generalization, Kolmogorov complexity, Levin complexity, Neural networks, Self-sizing programs, Solomonoff-Levin distribution, Universal search},
  }

  @book{Blum2013,
  	title = {{GECCO}'13 : proceedings of the 2013 Genetic and Evolutionary Computation Conference Companion : July 6-10, 2013, Amsterdam, the Netherlands},
  	isbn = {978-1-4503-1964-5},
  	abstract = {Includes index.},
  	pagetotal = {1766},
  	publisher = {{ACM}},
  	author = {Blum, Christian and Alba, Enrique. and {Association for Computing Machinery. SIGEVO.}},
  	date = {2013},
  	file = {Blum et al_2013_GECCO'13.pdf:/Users/jaime/Zotero/storage/79862QRX/Blum et al_2013_GECCO'13.pdf:application/pdf},
  }

  @inproceedings{Hu2019,
  	title = {Local relation networks for image recognition},
  	volume = {2019-Octob},
  	isbn = {978-1-72814-803-8},
  	doi = {10.1109/ICCV.2019.00356},
  	abstract = {The convolution layer has been the dominant feature extractor in computer vision for years. However, the spatial aggregation in convolution is basically a pattern matching process that applies fixed filters which are inefficient at modeling visual elements with varying spatial distributions. This paper presents a new image feature extractor, called the local relation layer, that adaptively determines aggregation weights based on the compositional relationship of local pixel pairs. With this relational approach, it can composite visual elements into higher-level entities in a more efficient manner that benefits semantic inference. A network built with local relation layers, called the Local Relation Network ({LR}-Net), is found to provide greater modeling capacity than its counterpart built with regular convolution on large-scale recognition tasks such as {ImageNet} classification.},
  	pages = {3463--3472},
  	booktitle = {Proceedings of the {IEEE} International Conference on Computer Vision},
  	publisher = {Institute of Electrical and Electronics Engineers Inc.},
  	author = {Hu, Han and Zhang, Zheng and Xie, Zhenda and Lin, Stephen},
  	urldate = {2020-05-02},
  	date = {2019-10-01},
  	note = {{ISSN}: 15505499},
  	file = {Hu et al_2019_Local relation networks for image recognition.pdf:/Users/jaime/Zotero/storage/8F8C24XB/Hu et al_2019_Local relation networks for image recognition.pdf:application/pdf},
  }

  @article{Brockman2016,
  	title = {{OpenAI} Gym},
  	url = {http://arxiv.org/abs/1606.01540},
  	abstract = {{OpenAI} Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of {OpenAI} Gym and the design decisions that went into the software.},
  	author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  	urldate = {2020-05-02},
  	date = {2016-06-05},
  	file = {Brockman et al_2016_OpenAI Gym.pdf:/Users/jaime/Zotero/storage/ALGJ7NJP/Brockman et al_2016_OpenAI Gym.pdf:application/pdf},
  }

  @online{zotero-661,
  	title = {Announcing the Obstacle Tower Challenge winners and open source release - Unity Technologies Blog},
  	url = {https://blogs.unity3d.com/2019/08/07/announcing-the-obstacle-tower-challenge-winners-and-open-source-release/},
  	urldate = {2020-05-02},
  }

  @article{Gare2010,
  	title = {Toward an Ecological Civilization: The Science, Ethics, and Politics of Eco-Poiesis},
  	volume = {39},
  	issn = {0360-6503},
  	doi = {10.5840/process20103912},
  	pages = {5--38},
  	number = {1},
  	journaltitle = {Process Studies},
  	author = {Gare, Arran},
  	date = {2010},
  	keywords = {civilization, common good, environmental philosophy, hierarchy theory, human ecology, philosophy, political philosophy, politics, process philosophy, religious studies, social science},
  	file = {Gare_2010_Toward an Ecological Civilization.pdf:/Users/jaime/Zotero/storage/V8NT4AEB/Gare_2010_Toward an Ecological Civilization.pdf:application/pdf},
  }

  @article{McLaren2009,
  	title = {Climate Change and Some Other Implications of Vibratory Existence},
  	volume = {5},
  	issn = {1832-9101},
  	url = {http://cosmosandhistory.org/index.php/journal/article/view/146/253},
  	abstract = {Modern process philosophy began when Alfred North Whitehead realized that existence is primarily vibratory, not points but processes. vibrations are best understood as sound waves, or through using auditory metaphors rather than visual ones. our Universe is more like music than matter, but how does this help us better understand it? In this paper I use the example of the large ocean current oscillators that help drive our climate systems to reveal the more effective nature of auditory approaches. Through an auditory approach, we can better understand the ways these oscillations constrain and interact with other levels of oscillations as well as how they might be destroyed by other levels. This can then lead to us extending our ethics to the conservation of these oscillations},
  	pages = {134},
  	number = {2},
  	journaltitle = {Cosmos and History: The Journal of Natural and Social Philosophy},
  	author = {{McLaren}, G.},
  	date = {2009},
  	file = {McLaren_2009_Climate Change and Some Other Implications of Vibratory Existence.pdf:/Users/jaime/Zotero/storage/JVRSCZJC/McLaren_2009_Climate Change and Some Other Implications of Vibratory Existence.pdf:application/pdf},
  }

  @online{Chomsky1971,
  	title = {Human Nature: Justice versus Power, Noam Chomsky debates with Michel Foucault},
  	url = {https://chomsky.info/1971xxxx/},
  	author = {Chomsky, Noam and Foucault, Michel},
  	urldate = {2020-05-04},
  	date = {1971},
  }

  @article{Rosso2007,
  	title = {Distinguishing noise from chaos},
  	volume = {99},
  	issn = {00319007},
  	doi = {10.1103/PhysRevLett.99.154102},
  	abstract = {Chaotic systems share with stochastic processes several properties that make them almost undistinguishable. In this communication we introduce a representation space, to be called the complexity-entropy causality plane. Its horizontal and vertical axis are suitable functionals of the pertinent probability distribution, namely, the entropy of the system and an appropriate statistical complexity measure, respectively. These two functionals are evaluated using the Bandt-Pompe recipe to assign a probability distribution function to the time series generated by the system. Several well-known model-generated time series, usually regarded as being of either stochastic or chaotic nature, are analyzed so as to illustrate the approach. The main achievement of this communication is the possibility of clearly distinguishing between them in our representation space, something that is rather difficult otherwise. © 2007 The American Physical Society.},
  	pages = {1--4},
  	number = {15},
  	journaltitle = {Physical Review Letters},
  	author = {Rosso, O. A. and Larrondo, H. A. and Martin, M. T. and Plastino, A. and Fuentes, M. A.},
  	date = {2007},
  	file = {Rosso et al_2007_Distinguishing noise from chaos.pdf:/Users/jaime/Zotero/storage/3FEDTA2N/Rosso et al_2007_Distinguishing noise from chaos.pdf:application/pdf},
  }

  @article{McLaren2020b,
  	title = {Anticipating uncertainty: living with the possibility of pandemics},
  	url = {https://www.linkedin.com/posts/glenn-mclaren-ab585046_anticipating-uncertainty-activity-6658594262483038208-ZIpl},
  	author = {{McLaren}, Glenn},
  	date = {2020},
  }

  @book{Macintyre2011,
  	title = {A short history of ethics : a history of moral philosophy from the Homeric Age to the twentieth century},
  	isbn = {978-0-268-01759-0},
  	publisher = {University Of Notre Dame Press {PP} - Notre Dame, Ind.},
  	author = {Macintyre, Alasdair C},
  	date = {2011},
  }

  @misc{McLaren2020a,
  	title = {Towards an Ecological Ethics},
  	publisher = {Swinburne University of Technology},
  	author = {{McLaren}, Glenn},
  	date = {2020},
  	note = {Place: Melbourne},
  }

  @book{Das2013,
  	title = {The Quantum Guide to Life: How the Laws of Physics Explain Our Lives from Laziness to Love},
  	isbn = {978-1-62636-380-9},
  	url = {https://books.google.com.au/books?id=LgMnAgAAQBAJ},
  	publisher = {Skyhorse Publishing},
  	author = {Das, K K},
  	date = {2013},
  }

  @book{Hawking2009,
  	title = {A brief history of time: from big bang to black holes},
  	isbn = {1-4090-9236-4},
  	publisher = {Random House},
  	author = {Hawking, Stephen},
  	date = {2009},
  }

  @article{Wehrl1978,
  	title = {General properties of entropy},
  	volume = {50},
  	issn = {00346861},
  	doi = {10.1103/RevModPhys.50.221},
  	abstract = {It is rather paradoxical that, although entropy is one of the most important quantities in physics, its main properties are rarely listed in the usual textbooks on statistical mechanics. In this paper we try to fill this gap by discussing these properties, as, for instance, invariance, additivity, concavity, subadditivity, strong subadditivity, continuity, etc., in detail, with reference to their implications in statistical mechanics. In addition, we consider related concepts such as relative entropy, skew entropy, dynamical entropy, etc. Taking into account that statistical mechanics deals with large, essentially infinite systems, we finally will get a glimpse of systems with infinitely many degrees of freedom. © 1978 American Physical Society.},
  	pages = {221--260},
  	number = {2},
  	journaltitle = {Reviews of Modern Physics},
  	author = {Wehrl, Alfred},
  	date = {1978},
  }

  @article{Bookchin1987,
  	title = {Social Ecology versus "Deep Ecology": A Challenge for the Ecology Movement},
  	url = {http://www.environment.gen.tr/deep-ecology/64-social-ecology-versus-deep-ecology.html},
  	abstract = {[Originally published in Green Perspectives: Newsletter of the Green Program Project, nos. 4-5 (summer 1987). In the original, the term deep ecology appeared in quotation marks; they have been removed in this online posting.] The environmental movement has traveled a long way since those early Earth Day festivals when millions of school kids were ritualistically mobilized to clean up streets, while Arthur Godfrey, Barry Commoner, Paul Ehrlich, and a bouquet of manipulative legislators scolded their parents for littering the landscape with cans, newspapers, and bottles. The movement has gone beyond a naive belief that patchwork reforms and solemn vows by {EPA} bureaucrats to act more resolutely will seriously arrest the insane pace at which we are tearing down the planet. This shopworn Earth Day approach to engineering nature so that we can ravage the Earth with minimal effect on ourselves---an approach that I called environmentalism in the late 1960s, in contrast to social ecology---has shown signs of giving way to a more searching and radical mentality. Today the new word in vogue is ecology---be it deep ecology, human ecology, biocentric ecology, antihumanist ecology, or to use a term that is uniquely rich in meaning, social ecology. Happily, the new relevance of ecology reveals a growing dissatisfaction among thinking people with attempts to use our vast ecological problems for cheaply spectacular and politically manipulative ends. As our forests disappear due to mindless cutting and increasing acid rain, as the ozone layer thins out because of the widespread use of fluorocarbons, as toxic dumps 1 / 22 Social Ecology versus Deep Ecology multiply all over the planet, as highly dangerous, often radioactive pollutants enter into our air, water, and food chains---all, and innumerable other hazards that threaten the integrity of life itself, raise far more basic issues than any that can be resolved by Earth Day clean-ups and faint-hearted changes in existing environmental laws. For good reason, more and more people are trying to go beyond the vapid environmentalism of the early 1970s and develop a more fundamental, indeed a more radical, approach to the ecological crises that beleaguer us. They are looking for an ecological approach, one that is rooted in an ecological philosophy, ethics, sensibility, and image of nature, and ultimately for an ecological movement that will transform our domineering market society into a nonhierarchical cooperative society---a society that will live in harmony with nature because its members live in harmony with one another.},
  	pages = {1--22},
  	number = {4},
  	journaltitle = {Green Perspectives: Newsletter of the Green Progra Project},
  	author = {Bookchin, Murray},
  	date = {1987},
  	keywords = {Deep Ecology, Deep Malthusians, Earth Day, Green Perspectives, Murray Bookchin, Social Ecology, Social Ecology versus Deep Ecology},
  	file = {Bookchin_1987_Social Ecology versus Deep Ecology.pdf:/Users/jaime/Zotero/storage/39Q4KCXJ/Bookchin_1987_Social Ecology versus Deep Ecology.pdf:application/pdf},
  }

  @book{Pollan2013,
  	title = {Cooked: A Natural History of Transformation},
  	isbn = {978-0-14-312533-4},
  	publisher = {Allen Lane},
  	author = {Pollan, Michael},
  	date = {2013},
  }

  @online{Feynman1964,
  	title = {The Principle of Least Action},
  	url = {https://www.feynmanlectures.caltech.edu/II_19.html},
  	author = {Feynman, Richard},
  	urldate = {2020-05-04},
  	date = {1964},
  }

  @article{Hansen2001,
  	title = {Completely derandomized self-adaptation in evolution strategies.},
  	volume = {9},
  	issn = {10636560},
  	doi = {10.1162/106365601750190398},
  	abstract = {This paper puts forward two useful methods for self-adaptation of the mutation distribution - the concepts of derandomization and cumulation. Principle shortcomings of the concept of mutative strategy parameter control and two levels of derandomization are reviewed. Basic demands on the self-adaptation of arbitrary (normal) mutation distributions are developed. Applying arbitrary, normal mutation distributions is equivalent to applying a general, linear problem encoding. The underlying objective of mutative strategy parameter control is roughly to favor previously selected mutation steps in the future. If this objective is pursued rigorously, a completely derandomized self-adaptation scheme results, which adapts arbitrary normal mutation distributions. This scheme, called covariance matrix adaptation ({CMA}), meets the previously stated demands. It can still be considerably improved by cumulation - utilizing an evolution path rather than single search steps. Simulations on various test functions reveal local and global search properties of the evolution strategy with and without covariance matrix adaptation. Their performances are comparable only on perfectly scaled functions. On badly scaled, non-separable functions usually a speed up factor of several orders of magnitude is observed. On moderately mis-scaled functions a speed up factor of three to ten can be expected.},
  	pages = {159--195},
  	number = {2},
  	journaltitle = {Evolutionary computation},
  	author = {Hansen, N. and Ostermeier, A.},
  	date = {2001},
  	keywords = {covariance matrix adaptation, cumulation, cumulative path length control, de-, derandomized self-adaptation, evolu-, evolution strategy, randomization, self-adaptation, step size control, strategy parameter control, tion path},
  	file = {Hansen_Ostermeier_2001_Completely derandomized self-adaptation in evolution strategies.pdf:/Users/jaime/Zotero/storage/5KCLZCXH/Hansen_Ostermeier_2001_Completely derandomized self-adaptation in evolution strategies.pdf:application/pdf},
  }

  @online{Tang2019,
  	title = {How to run evolution strategies on Google Kubernetes Engine},
  	url = {https://cloud.google.com/blog/products/ai-machine-learning/how-to-run-evolution-strategies-on-google-kubernetes-engine},
  	author = {Tang, Yujin and Ha, David},
  	urldate = {2020-05-05},
  	date = {2019},
  }

  @book{Bostrom2014,
  	location = {London},
  	title = {Superintelligence: Paths, Dangers, Strategies},
  	isbn = {978-0-19-967811-2},
  	publisher = {Oxford University Press},
  	author = {Bostrom, Nick},
  	date = {2014},
  }

  @online{Urban2015,
  	title = {The {AI} Revolution: The Road to Superintelligence},
  	url = {https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html},
  	author = {Urban, Tim},
  	urldate = {2020-05-05},
  	date = {2015},
  }

  @thesis{Urban2015a,
  	title = {The {AI} Revolution: Our Immortality of Extinction},
  	type = {phdthesis},
  	author = {Urban, Tim},
  	date = {2015},
  }

  @article{Bostrom1998,
  	title = {How Long Before Superintelligence},
  	volume = {2},
  	abstract = {This paper outlines the case for believing that we will have superhuman artificial intelligence within the first third of the next century. It looks at different estimates of the processing power of the human brain; how long it will take until computer hardware achieve a similar performance; ways of creating the software through bottom-up approaches like the one used by biological brains; how difficult it will be for neuroscience figure out enough about how brains work to make this approach work; and how fast we can expect superintelligence to be developed once there is human-level artificial intelligence.},
  	journaltitle = {International Journal of Future Studies},
  	author = {Bostrom, Nick},
  	date = {1998},
  }

  @article{Zhang2019,
  	title = {{VR}-Goggles for Robots: Real-to-Sim Domain Adaptation for Visual Control},
  	volume = {4},
  	issn = {23773766},
  	doi = {10.1109/LRA.2019.2894216},
  	abstract = {In this letter, we deal with the reality gap from a novel perspective, targeting transferring deep reinforcement learning ({DRL}) policies learned in simulated environments to the real-world domain for visual control tasks. Instead of adopting the common solutions to the problem by increasing the visual fidelity of synthetic images output from simulators during the training phase, we seek to tackle the problem by translating the real-world image streams back to the synthetic domain during the deployment phase, to make the robot feel at home. We propose this as a lightweight, flexible, and efficient solution for visual control, as first, no extra transfer steps are required during the expensive training of {DRL} agents in simulation; second, the trained {DRL} agents will not be constrained to being deployable in only one specific real-world environment; and third, the policy training and the transfer operations are decoupled, and can be conducted in parallel. Besides this, we propose a simple yet effective shift loss that is agnostic to the downstream task, to constrain the consistency between subsequent frames which is important for consistent policy outputs. We validate the shift loss for artistic style transfer for videos and domain adaptation, and validate our visual control approach in indoor and outdoor robotics experiments.},
  	pages = {1148--1155},
  	number = {2},
  	journaltitle = {{IEEE} Robotics and Automation Letters},
  	author = {Zhang, Jingwei and Tai, Lei and Yun, Peng and Xiong, Yufeng and Liu, Ming and Boedecker, Joschka and Burgard, Wolfram},
  	date = {2019},
  	note = {Publisher: {IEEE}},
  	keywords = {Deep learning in robotics and automation, model learning for control, visual-based navigation},
  	file = {Zhang et al_2019_VR-Goggles for Robots.pdf:/Users/jaime/Zotero/storage/U58GML9M/Zhang et al_2019_VR-Goggles for Robots.pdf:application/pdf},
  }

  @article{Beattie2016,
  	title = {{DeepMind} Lab},
  	url = {http://arxiv.org/abs/1612.03801},
  	abstract = {{DeepMind} Lab is a first-person 3D game platform designed for research and development of general artificial intelligence and machine learning systems. {DeepMind} Lab can be used to study how autonomous artificial agents may learn complex tasks in large, partially observed, and visually diverse worlds. {DeepMind} Lab has a simple and flexible {API} enabling creative task-designs and novel {AI}-designs to be explored and quickly iterated upon. It is powered by a fast and widely recognised game engine, and tailored for effective use by the research community.},
  	pages = {1--11},
  	author = {Beattie, Charles and Leibo, Joel Z. and Teplyashin, Denis and Ward, Tom and Wainwright, Marcus and Küttler, Heinrich and Lefrancq, Andrew and Green, Simon and Valdés, Víctor and Sadik, Amir and Schrittwieser, Julian and Anderson, Keith and York, Sarah and Cant, Max and Cain, Adam and Bolton, Adrian and Gaffney, Stephen and King, Helen and Hassabis, Demis and Legg, Shane and Petersen, Stig},
  	date = {2016},
  	file = {Beattie et al_2016_DeepMind Lab.pdf:/Users/jaime/Zotero/storage/JSF33SE4/Beattie et al_2016_DeepMind Lab.pdf:application/pdf},
  }

  @article{Yudkowsky2004,
  	title = {Coherent extrapolated volition},
  	abstract = {This is an update to that part of Friendly {AI} theory that describes Friendliness, the objective or thing-we’re-trying-to-do. The information is current as of May 2004, and should not become dreadfully obsolete until late June, when I plan to have an unexpected insight. (Update: Actually, it took two days. Still, the text here isn’t too far from the mark.) Misleading terminology alert: I am still calling the Friendly Thingy an “Artificial Intelligence” or “superintelligence,” even though it would be more accurate to call it a Friendly Really Powerful Optimization Process},
  	number = {2004},
  	journaltitle = {Singularity Institute for Artificial Intelligence},
  	author = {Yudkowsky, Eliezer S.},
  	date = {2004},
  	file = {Yudkowsky_2004_Coherent extrapolated volition.pdf:/Users/jaime/Zotero/storage/UBDWTWDC/Yudkowsky_2004_Coherent extrapolated volition.pdf:application/pdf},
  }

  @incollection{Russell2010,
  	title = {Philosophical Foundations},
  	pages = {1020--1043},
  	booktitle = {Artificial Intelligence: a Modern Approach},
  	publisher = {Pearson},
  	author = {Russell, Stuart J and Norvig, Peter},
  	date = {2010},
  }

  @article{Hawley2019,
  	title = {Challenges for an Ontology of Artificial Intelligence},
  	url = {http://arxiv.org/abs/1903.03171},
  	abstract = {Of primary importance in formulating a response to the increasing prevalence and power of artificial intelligence ({AI}) applications in society are questions of ontology. Questions such as: What "are" these systems? How are they to be regarded? How does an algorithm come to be regarded as an agent? We discuss three factors which hinder discussion and obscure attempts to form a clear ontology of {AI}: (1) the various and evolving definitions of {AI}, (2) the tendency for pre-existing technologies to be assimilated and regarded as "normal," and (3) the tendency of human beings to anthropomorphize. This list is not intended as exhaustive, nor is it seen to preclude entirely a clear ontology, however, these challenges are a necessary set of topics for consideration. Each of these factors is seen to present a 'moving target' for discussion, which poses a challenge for both technical specialists and non-practitioners of {AI} systems development (e.g., philosophers and theologians) to speak meaningfully given that the corpus of {AI} structures and capabilities evolves at a rapid pace. Finally, we present avenues for moving forward, including opportunities for collaborative synthesis for scholars in philosophy and science.},
  	pages = {1--20},
  	issue = {Ml},
  	author = {Hawley, Scott H.},
  	date = {2019},
  	file = {Hawley_2019_Challenges for an Ontology of Artificial Intelligence.pdf:/Users/jaime/Zotero/storage/SC6XBHXI/Hawley_2019_Challenges for an Ontology of Artificial Intelligence.pdf:application/pdf},
  }

  @article{Luker1994,
  	title = {The philosophy of artificial intelligence},
  	volume = {26},
  	issn = {00978418},
  	doi = {10.1145/191033.191050},
  	abstract = {A collection of classic articles from the field of artificial intelligence ({AI}), The Philosophy of Artificial Intelligence would be a good complement to an introductory textbook on {AI} fundamentals. The back cover of the book states that the material is intended for the university student or general reader, but don't be fooled. Unless you are a student in a supportive class setting or a general reader who happens to have a degree in engineering, you are likely to find the content difficult. The first chapter, for example, assumes knowledge of calculus. However, if you have the right preparation, you'll be treated to fifteen important papers in {AI}-including Alan Turing's Computing Machinery and Intelligence article, which proposed the now well- known Turing test for determining whether a machine is intelligent.},
  	pages = {41--45},
  	number = {1},
  	journaltitle = {{ACM} {SIGCSE} Bulletin},
  	author = {Luker, Paul A. and Rothermel, Dennis},
  	date = {1994},
  	note = {{ISBN}: 0198248547},
  	file = {Luker_Rothermel_1994_The philosophy of artificial intelligence.pdf:/Users/jaime/Zotero/storage/PUCXAHJA/Luker_Rothermel_1994_The philosophy of artificial intelligence.pdf:application/pdf},
  }

  @article{Keeley1998,
  	title = {Artificial life for philosophers},
  	volume = {11},
  	issn = {09515089},
  	doi = {10.1080/09515089808573260},
  	abstract = {Artificial life ({ALife}) is the attempt to create artificial instances of life in a variety of media, but primarily within the digital computer. As such, the field brings together computationally-minded biologists and biologically-minded computer scientists. I argue that this new field is filled with interesting philosophical issues. However, there is a dearth of philosophers actively conducting research in this area. I discuss two books on the new field: Margaret A. Boden's The philosophy of artificial life and Christopher G. Langton's Artificial life: an overview. They cover three areas of philosophical interest: the definition of life, the relationship between life and mind, and the possibility of creating life within a computational environment. This discussion allows me to critique past work in the philosophy of {ALife} that tends to see the field as a proving ground for traditional arguments from the philosophy of artificial intelligence. Instead, I suggest, what is interesting about {ALife} is how it differs from artificial intelligence and that the most interesting philosophical issues in the area are those derived from biology, not psychology. I recommend that these two books taken together constitute an interesting introduction to {ALife} and the wealth of philosophical issues found therein.},
  	pages = {251--260},
  	number = {2},
  	journaltitle = {Philosophical Psychology},
  	author = {Keeley, Brian L.},
  	date = {1998},
  	file = {Keeley_1998_Artificial life for philosophers.pdf:/Users/jaime/Zotero/storage/URHS7ZFK/Keeley_1998_Artificial life for philosophers.pdf:application/pdf},
  }

  @article{Porter2016,
  	title = {A Methodology for the Assesment of {AI} Consciousness},
  	volume = {9782},
  	issn = {16113349},
  	doi = {10.1007/978-3-319-41649-6},
  	pages = {V},
  	journaltitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  	author = {Porter, Harry H.},
  	date = {2016},
  	note = {{ISBN}: 9783319416489},
  	keywords = {consciousness},
  	file = {Porter_2016_A Methodology for the Assesment of AI Consciousness.pdf:/Users/jaime/Zotero/storage/BQ4E72HD/Porter_2016_A Methodology for the Assesment of AI Consciousness.pdf:application/pdf},
  }

  @article{Chrisley2008,
  	title = {Philosophical foundations of artificial consciousness},
  	volume = {44},
  	issn = {09333657},
  	doi = {10.1016/j.artmed.2008.07.011},
  	abstract = {Objective: Consciousness is often thought to be that aspect of mind that is least amenable to being understood or replicated by artificial intelligence ({AI}). The first-personal, subjective, what-it-is-like-to-be-something nature of consciousness is thought to be untouchable by the computations, algorithms, processing and functions of {AI} method. Since {AI} is the most promising avenue toward artificial consciousness ({AC}), the conclusion many draw is that {AC} is even more doomed than {AI} supposedly is. The objective of this paper is to evaluate the soundness of this inference. Methods: The results are achieved by means of conceptual analysis and argumentation. Results and conclusions: It is shown that pessimism concerning the theoretical possibility of artificial consciousness is unfounded, based as it is on misunderstandings of {AI}, and a lack of awareness of the possible roles {AI} might play in accounting for or reproducing consciousness. This is done by making some foundational distinctions relevant to {AC}, and using them to show that some common reasons given for {AC} scepticism do not touch some of the (usually neglected) possibilities for {AC}, such as prosthetic, discriminative, practically necessary, and lagom (necessary-but-not-sufficient) {AC}. Along the way three strands of the author's work in {AC} - interactive empiricism, synthetic phenomenology, and ontologically conservative heterophenomenology - are used to illustrate and motivate the distinctions and the defences of {AC} they make possible. © 2008 Elsevier B.V. All rights reserved.},
  	pages = {119--137},
  	number = {2},
  	journaltitle = {Artificial Intelligence in Medicine},
  	author = {Chrisley, Ron},
  	date = {2008},
  	keywords = {Artificial consciousness, Heterophenomenology, Interactive empiricism, Machine consciousness, Prosthetic artificial intelligence, Synthetic phenomenology},
  	file = {Chrisley_2008_Philosophical foundations of artificial consciousness.pdf:/Users/jaime/Zotero/storage/MT756BUY/Chrisley_2008_Philosophical foundations of artificial consciousness.pdf:application/pdf},
  }

  @article{Gunkel2018,
  	title = {The other question: can and should robots have rights?},
  	volume = {20},
  	issn = {15728439},
  	url = {http://dx.doi.org/10.1007/s10676-017-9442-4},
  	doi = {10.1007/s10676-017-9442-4},
  	abstract = {This essay addresses the other side of the robot ethics debate, taking up and investigating the question “Can and should robots have rights?” The examination of this subject proceeds by way of three steps or movements. We begin by looking at and analyzing the form of the question itself. There is an important philosophical difference between the two modal verbs that organize the inquiry—can and should. This difference has considerable history behind it that influences what is asked about and how. Second, capitalizing on this verbal distinction, it is possible to identify four modalities concerning social robots and the question of rights. The second section will identify and critically assess these four modalities as they have been deployed and developed in the current literature. Finally, we will conclude by proposing another alternative, a way of thinking otherwise that effectively challenges the existing rules of the game and provides for other ways of theorizing moral standing that can scale to the unique challenges and opportunities that are confronted in the face of social robots.},
  	pages = {87--99},
  	number = {2},
  	journaltitle = {Ethics and Information Technology},
  	author = {Gunkel, David J.},
  	date = {2018},
  	note = {Publisher: Springer Netherlands
  {ISBN}: 0123456789},
  	keywords = {David Hume, Emmanuel Levinas, Ethics, Philosophy of technology, Rights, Robotics, Social robots},
  	file = {Gunkel_2018_The other question.pdf:/Users/jaime/Zotero/storage/EBD2Q4MC/Gunkel_2018_The other question.pdf:application/pdf;Gunkel2018_Article_TheOtherQuestionCanAndShouldRo.pdf:/Users/jaime/Zotero/storage/XXKDADLR/Gunkel2018_Article_TheOtherQuestionCanAndShouldRo.pdf:application/pdf},
  }

  @book{Gunkel2012,
  	title = {The Machine Question: Critical Perspectives on {AI}, Robots, and Ethics},
  	author = {Gunkel, David J.},
  	date = {2012},
  	file = {Gunkel_2012_The Machine Question.pdf:/Users/jaime/Zotero/storage/546DWK4L/Gunkel_2012_The Machine Question.pdf:application/pdf},
  }

  @book{Ellis1986,
  	title = {An Ontology of Consciousness},
  	isbn = {978-90-481-8298-5},
  	author = {Ellis, Ralph},
  	date = {1986},
  	doi = {10.1007/978-94-017-0715-2},
  }

  @article{Nagel1974,
  	title = {What Is It Like to Be a Bat ?},
  	volume = {83},
  	doi = {10.2307/2176743},
  	pages = {435--450},
  	number = {4},
  	journaltitle = {The Philosophical Review},
  	author = {Nagel, Thomas},
  	date = {1974},
  	note = {{ISBN}: 0031810820090},
  	file = {Nagel_1974_What Is It Like to Be a Bat.pdf:/Users/jaime/Zotero/storage/YCM9DBRA/Nagel_1974_What Is It Like to Be a Bat.pdf:application/pdf},
  }

  @article{Maille2020,
  	title = {Reconciling current theories of consciousness},
  	volume = {40},
  	issn = {15292401},
  	doi = {10.1523/JNEUROSCI.2740-19.2020},
  	pages = {1994--1996},
  	number = {10},
  	journaltitle = {Journal of Neuroscience},
  	author = {Maillé, Sébastien and Lynn, Michael},
  	date = {2020},
  	pmid = {32132221},
  	file = {Maillé_Lynn_2020_Reconciling current theories of consciousness.pdf:/Users/jaime/Zotero/storage/NRCNEH36/Maillé_Lynn_2020_Reconciling current theories of consciousness.pdf:application/pdf},
  }

  @article{Hardin1968,
  	title = {The tragedy of the commons},
  	volume = {162},
  	issn = {00027685},
  	doi = {10.7135/upo9781843318637.006},
  	pages = {47--52},
  	issue = {December},
  	journaltitle = {International Environmental Governance},
  	author = {Hardin, Garrett},
  	date = {1968},
  	note = {{ISBN}: 9781315092546},
  	file = {Hardin_1968_The tragedy of the commons.pdf:/Users/jaime/Zotero/storage/HBPGTXQX/Hardin_1968_The tragedy of the commons.pdf:application/pdf},
  }

  @article{Gare2009,
  	title = {Philosophical Anthropology, Ethics and Political Philosophy in an Age of Impending Catastrophe.},
  	volume = {5},
  	pages = {1--17},
  	number = {2},
  	journaltitle = {Cosmos \& History},
  	author = {Gare, A},
  	date = {2009},
  	keywords = {aristotle, hierarchy theory, human ecology, political philosophy, biosemiotics, c, culture, ethics, hegel, herder, hobbes, marx, peirce, philosophical anthropology, s, schelling},
  	file = {Gare_2009_Philosophical Anthropology, Ethics and Political Philosophy in an Age of.pdf:/Users/jaime/Zotero/storage/7I7VQX26/Gare_2009_Philosophical Anthropology, Ethics and Political Philosophy in an Age of.pdf:application/pdf},
  }

  @book{Leech2012,
  	location = {London},
  	title = {Capitalism: A Structural Genocide},
  	isbn = {978-1-78032-202-5},
  	pagetotal = {194},
  	publisher = {Zed Books},
  	author = {Leech, Garry},
  	date = {2012},
  	file = {Leech_2012_Capitalism.pdf:/Users/jaime/Zotero/storage/X7KW8F6D/Leech_2012_Capitalism.pdf:application/pdf},
  }

  @article{Brandt1983,
  	title = {The Real \& Alleged Problems of Utilitarianism},
  	volume = {13},
  	issn = {00930334},
  	doi = {10.2307/3561774},
  	abstract = {This article aims to summarize, for non-philosophers, what seems a persuasive form of rule-utilitarianism, to identify some problems of the theory which seem not wholly resolved, and to defend the theory against some popular objections. among the problems discussed are the implications of the theory for professional ethics, governments, and institutions; the choice between hedonistic and 'preference' conceptions of utility; and the implications for population control and obligations to future generations.},
  	pages = {37},
  	number = {2},
  	journaltitle = {The Hastings Center Report},
  	author = {Brandt, Richard B.},
  	date = {1983},
  	file = {Brandt_1983_The Real & Alleged Problems of Utilitarianism.pdf:/Users/jaime/Zotero/storage/UKPXZITG/Brandt_1983_The Real & Alleged Problems of Utilitarianism.pdf:application/pdf},
  }

  @article{Gare1996,
  	title = {Nihilism Inc.: Environmental Destruction and the Metaphysics of Sustainability},
  	journaltitle = {Ecological Press},
  	author = {Gare, Arran},
  	date = {1996},
  	file = {Gare_1996_Nihilism Inc.pdf:/Users/jaime/Zotero/storage/B4F9IGXZ/Gare_1996_Nihilism Inc.pdf:application/pdf},
  }

  @article{Rand2012,
  	title = {Spontaneous giving and calculated greed},
  	volume = {489},
  	issn = {00280836},
  	url = {http://dx.doi.org/10.1038/nature11467},
  	doi = {10.1038/nature11467},
  	abstract = {Cooperation is central to human social behaviour. However, choosing to cooperate requires individuals to incur a personal cost to benefit others. Here we explore the cognitive basis of cooperative decision-making in humans using a dual-process framework. We ask whether people are predisposed towards selfishness, behaving cooperatively only through active self-control; or whether they are intuitively cooperative, with reflection and prospective reasoning favouring rational self-interest. To investigate this issue, we perform ten studies using economic games. We find that across a range of experimental designs, subjects who reach their decisions more quickly are more cooperative. Furthermore, forcing subjects to decide quickly increases contributions, whereas instructing them to reflect and forcing them to decide slowly decreases contributions. Finally, an induction that primes subjects to trust their intuitions increases contributions compared with an induction that promotes greater reflection. To explain these results, we propose that cooperation is intuitive because cooperative heuristics are developed in daily life where cooperation is typically advantageous. We then validate predictions generated by this proposed mechanism. Our results provide convergent evidence that intuition supports cooperation in social dilemmas, and that reflection can undermine these cooperative impulses. © 2012 Macmillan Publishers Limited. All rights reserved.},
  	pages = {427--430},
  	number = {7416},
  	journaltitle = {Nature},
  	author = {Rand, David G. and Greene, Joshua D. and Nowak, Martin A.},
  	date = {2012},
  	pmid = {22996558},
  	note = {Publisher: Nature Publishing Group},
  	file = {Rand et al_2012_Spontaneous giving and calculated greed.pdf:/Users/jaime/Zotero/storage/N632G9EP/Rand et al_2012_Spontaneous giving and calculated greed.pdf:application/pdf},
  }

  @book{Segall2020,
  	location = {White Plains, {NY}},
  	title = {Buddhism and Human Flourishing: A Modern Western Perspective},
  	isbn = {978-3-030-37026-8},
  	pagetotal = {200},
  	publisher = {Palgrave {MacMillan}},
  	author = {Segall, Seth Zuihō},
  	date = {2020},
  	doi = {10.1007/978-3-030-37027-5},
  	keywords = {aristotle, buddhism, eudaimonia, virtue},
  	file = {Segall_2020_Buddhism and Human Flourishing.pdf:/Users/jaime/Zotero/storage/HZQ9AESZ/Segall_2020_Buddhism and Human Flourishing.pdf:application/pdf},
  }

  @article{WorldBankGroup2018,
  	title = {Poverty and Shared Prosperity 2018: Piecing Together the Poverty Puzzle},
  	doi = {10.1596/978-1-4648-1330-6},
  	abstract = {The Poverty and Shared Prosperity series provides a global audience with the latest and most accurate estimates on trends in global poverty and shared prosperity.},
  	pages = {1--31},
  	journaltitle = {Piecing Together the Poverty Puzzle},
  	author = {{World Bank Group}},
  	date = {2018},
  	note = {{ISBN}: 9781464813306},
  	file = {World Bank Group_2018_Poverty and Shared Prosperity 2018.pdf:/Users/jaime/Zotero/storage/N4AZJ6GZ/World Bank Group_2018_Poverty and Shared Prosperity 2018.pdf:application/pdf},
  }

  @misc{McLaren2020,
  	title = {{PHI}30008 – Ethics: Lecture slides},
  	publisher = {Swinburne University of Technology},
  	author = {{McLaren}, Glenn},
  	date = {2020},
  	note = {Place: Melbourne},
  }

  @online{Konstan2018,
  	title = {Epicurus},
  	titleaddon = {The Stanford Encyclopedia of Philosophy},
  	author = {Konstan, David},
  	urldate = {2020-05-30},
  	date = {2018},
  }

  @online{Monbiot2016,
  	title = {Neoliberalism – The Ideology at the Root of All Our Problems},
  	url = {https://www.theguardian.com/books/2016/apr/15/neoliberalism-ideology-problem-george-monbiot},
  	titleaddon = {The Guardian},
  	author = {Monbiot, George},
  	urldate = {2020-05-30},
  	date = {2016},
  }

  @movie{Norberg-Hodge2012,
  	title = {The Economics of Happiness},
  	author = {Norberg-Hodge, Helena},
  	date = {2012},
  	note = {Place: Germany},
  }

  @article{Vaswani2017,
  	title = {Attention is all you need},
  	volume = {2017-Decem},
  	issn = {10495258},
  	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.0 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature.},
  	pages = {5999--6009},
  	issue = {Nips},
  	journaltitle = {Advances in Neural Information Processing Systems},
  	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Łukasz and Polosukhin, Illia},
  	date = {2017},
  	file = {Vaswani et al_2017_Attention is all you need.pdf:/Users/jaime/Zotero/storage/BIT3DVT6/Vaswani et al_2017_Attention is all you need.pdf:application/pdf},
  }

  @article{Hansen2016,
  	title = {The {CMA} Evolution Strategy: A Tutorial},
  	url = {http://arxiv.org/abs/1604.00772},
  	abstract = {This tutorial introduces the {CMA} Evolution Strategy ({ES}), where {CMA} stands for Covariance Matrix Adaptation. The {CMA}-{ES} is a stochastic, or randomized, method for real-parameter (continuous domain) optimization of non-linear, non-convex functions. We try to motivate and derive the algorithm from intuitive concepts and from requirements of non-linear, non-convex search in continuous domain.},
  	author = {Hansen, Nikolaus},
  	date = {2016},
  	file = {Hansen_2016_The CMA Evolution Strategy.pdf:/Users/jaime/Zotero/storage/WXBHHN4S/Hansen_2016_The CMA Evolution Strategy.pdf:application/pdf},
  }

  @book{Matlin2008,
  	title = {Cognition},
  	publisher = {Wiley},
  	author = {Matlin, Margaret W},
  	date = {2008},
  }

  @article{Brace2019,
  	title = {Altered States of Consciousness: Natural Gateway to an Ecological Civilization?},
  	volume = {40},
  	issn = {10688471},
  	doi = {10.1037/teo0000123},
  	abstract = {An understanding of the roles of altered states of consciousness in disrupting entrenched and dysfunctional ways of thinking may give us hope for a correction to many of humanity's seemingly intractable problems. This article considers altered states induced via substances known as psychedelics at various levels of abstraction through the lenses of history, biology and botany, neurophysiology, psychology, and philosophy. The induction of these altered states in a therapeutic setting is already showing great promise for the treatment of certain psychological dysfunctions, but could a broader understanding of their effects help us to see how they might contribute to a civilization that is more focused on the harmonious integration of humanity with the rest of the natural world? Considering their pervasiveness across human cultures, as well as their deliberate induction by other species, their contribution to the development of Western philosophy, and their impact on the cognitive styles, processes, and deeply held views of individuals who have experienced them, the author calls for greater recognition of their positive potential and encourages a cautious but vigorous approach to further interdisciplinary study.},
  	pages = {69--84},
  	number = {2},
  	journaltitle = {Journal of Theoretical and Philosophical Psychology},
  	author = {Brace, Peter},
  	date = {2019},
  	keywords = {Altered states of consciousness, Ecology, History, Philosophy, Psychedelics},
  	file = {Brace_2019_Altered States of Consciousness.pdf:/Users/jaime/Zotero/storage/52Q5JFNX/Brace_2019_Altered States of Consciousness.pdf:application/pdf},
  }

  @article{Christiano2017,
  	title = {Deep reinforcement learning from human preferences},
  	volume = {2017-Decem},
  	issn = {10495258},
  	abstract = {For sophisticated reinforcement learning ({RL}) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex {RL} tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than 1\% of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art {RL} systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any which have been previously learned from human feedback.},
  	pages = {4300--4308},
  	journaltitle = {Advances in Neural Information Processing Systems},
  	author = {Christiano, Paul F. and Leike, Jan and Brown, Tom B. and Martic, Miljan and Legg, Shane and Amodei, Dario},
  	date = {2017},
  	file = {Christiano et al_2017_Deep reinforcement learning from human preferences.pdf:/Users/jaime/Zotero/storage/3GH4C628/Christiano et al_2017_Deep reinforcement learning from human preferences.pdf:application/pdf},
  }

  @article{Salimans2017,
  	title = {Evolution Strategies as a Scalable Alternative to Reinforcement Learning},
  	url = {https://openai.com/blog/evolution-strategies/},
  	author = {Salimans, Tim and Ho, Jonathan and Chen, Peter and Sutskever, Ilya and Schulman, John and Brockman, Greg and Sidor, Szymon},
  	urldate = {2020-08-20},
  	date = {2017},
  	file = {Karpathy et al_2017_Evolution Strategies as a Scalable Alternative to Reinforcement Learning.pdf:/Users/jaime/Zotero/storage/E48VQ627/Karpathy et al_2017_Evolution Strategies as a Scalable Alternative to Reinforcement Learning.pdf:application/pdf},
  }

  @article{Musk2019,
  	title = {An integrated brain-machine interface platform with thousands of channels},
  	volume = {21},
  	issn = {14388871},
  	doi = {10.2196/16194},
  	abstract = {Brain-machine interfaces hold promise for the restoration of sensory and motor function and the treatment of neurological disorders, but clinical brain-machine interfaces have not yet been widely adopted, in part, because modest channel counts have limited their potential. In this white paper, we describe Neuralink's first steps toward a scalable high-bandwidth brain-machine interface system. We have built arrays of small and flexible electrode “threads,” with as many as 3072 electrodes per array distributed across 96 threads. We have also built a neurosurgical robot capable of inserting six threads (192 electrodes) per minute. Each thread can be individually inserted into the brain with micron precision for avoidance of surface vasculature and targeting specific brain regions. The electrode array is packaged into a small implantable device that contains custom chips for low-power on-board amplification and digitization: The package for 3072 channels occupies less than 23×18.5×2 mm3. A single {USB}-C cable provides full-bandwidth data streaming from the device, recording from all channels simultaneously. This system has achieved a spiking yield of up to 70\% in chronically implanted electrodes. Neuralink's approach to brain-machine interface has unprecedented packaging density and scalability in a clinically relevant package.},
  	pages = {1--14},
  	number = {10},
  	journaltitle = {Journal of Medical Internet Research},
  	author = {Musk, Elon},
  	date = {2019},
  	pmid = {31642810},
  	keywords = {Brain-machine interface, Motor function, Neurology, Sensory function},
  }

  @article{Choi2017,
  	title = {Multi-focus attention network for efficient deep reinforcement learning},
  	volume = {{WS}-17-01 -},
  	url = {http://arxiv.org/abs/1712.04603},
  	abstract = {Deep reinforcement learning ({DRL}) has shown incredible performance in learning various tasks to the human level. However, unlike human perception, current {DRL} models connect the entire low-level sensory input to the state-action values rather than exploiting the relationship between and among entities that constitute the sensory input. Because of this difference, {DRL} needs vast amount of experience samples to learn. In this paper, we propose a Multi-focus Atten-tion Network ({MANet}) which mimics human ability to spatially abstract the low-level sensory input into multiple entities and attend to them simultaneously. The proposed method first divides the low-level input into several segments which we refer to as partial states. After this segmentation, parallel attention layers attend to the partial states relevant to solving the task. Our model estimates state-action values using these attended partial states. In our experiments, {MANet} attains highest scores with significantly less experience samples. Additionally, the model shows higher performance compared to the Deep Q-network and the single attention model as benchmarks. Furthermore, we extend our model to attentive communication model for performing multi-agent cooperative tasks, in multi-agent cooperative task experiments, our model shows 20\% faster learning than existing state-of-the-art model.},
  	pages = {952--958},
  	journaltitle = {{AAAI} Workshop - Technical Report},
  	author = {Choi, Jinyoung and Lee, Beom-Jin Jin and Zhang, Byoung-Tak Tak},
  	urldate = {2020-05-02},
  	date = {2017-12-12},
  	note = {Publisher: {AI} Access Foundation
  {ISBN}: 9781577357865},
  	file = {Choi et al_2017_Multi-focus attention network for efficient deep reinforcement learning.pdf:/Users/jaime/Zotero/storage/N33Y27LY/Choi et al_2017_Multi-focus attention network for efficient deep reinforcement learning.pdf:application/pdf;Choi, Lee, Zhang - 2017 - Multi-focus Attention Network for Efficient Deep Reinforcement Learning.pdf:/Users/jaime/Zotero/storage/BYQZQKGG/Choi, Lee, Zhang - 2017 - Multi-focus Attention Network for Efficient Deep Reinforcement Learning.pdf:application/pdf;Choi, Lee, Zhang - 2017 - Multi-focus Attention Network for Efficient Deep Reinforcement Learning(2).pdf:/Users/jaime/Zotero/storage/D2MFDDCZ/Choi, Lee, Zhang - 2017 - Multi-focus Attention Network for Efficient Deep Reinforcement Learning(2).pdf:application/pdf},
  }

  @article{Manchin,
  	title = {Reinforcement Learning with Attention that Works: A Self-Supervised Approach Image-based semantic modelling View project Feasibility Study: Development and demonstration of virtual reality simulation training for the {BHPB} Olympic Dam Site Inductions. View},
  	url = {https://www.researchgate.net/publication/332292503},
  	doi = {10.13140/RG.2.2.36603.77608},
  	abstract = {Attention models have had a significant positive impact on deep learning across a range of tasks. However previous attempts at integrating attention with reinforcement learning have failed to produce significant improvements. We propose the first combination of self attention and reinforcement learning that is capable of producing significant improvements, including new state of the art results in the Arcade Learning Environment. Unlike the selective attention models used in previous attempts, which constrain the attention via preconceived notions of importance, our implementation utilises the Markovian properties inherent in the state input. Our method produces a faithful visualisation of the policy, focusing on the behaviour of the agent. Our experiments demonstrate that the trained policies use multiple simultaneous foci of attention, and are able to modulate attention over time to deal with situations of partial observability.},
  	author = {Manchin, Anthony and Van Den Hengel, Anton and Abbasnejad, Ehsan},
  	keywords = {Attention ·, Deep, Learning ·, Reinforcement, Vi-sualisation},
  	file = {Manchin et al_Reinforcement Learning with Attention that Works.pdf:/Users/jaime/Zotero/storage/HN4RQR2E/Manchin et al_Reinforcement Learning with Attention that Works.pdf:application/pdf},
  }

  @article{Jasper2016,
  	title = {Practical Bayes optimization},
  	doi = {10.1017/9781316266175.015},
  	abstract = {The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a "black art" requiring expert experience, rules of thumb, or sometimes brute-force search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm\{{\textbackslash}textquoteright\}s generalization performance is modeled as a sample from a Gaussian process ({GP}). We show that certain choices for the nature of the {GP}, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expert-level performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured {SVMs} and convolutional neural networks.},
  	pages = {159--170},
  	journaltitle = {The Cambridge Companion to Saul Bellow},
  	author = {{Jasper}},
  	date = {2016},
  	note = {{ISBN}: 9781316266175},
  	file = {Jasper_2016_Practical Bayes optimization.pdf:/Users/jaime/Zotero/storage/LTW7PHJJ/Jasper_2016_Practical Bayes optimization.pdf:application/pdf},
  }

  @article{Jaderberg2017,
  	title = {Population Based Training of Neural Networks},
  	url = {http://arxiv.org/abs/1711.09846},
  	abstract = {Neural networks dominate the modern machine learning landscape, but their training and success still suffer from sensitivity to empirical choices of hyperparameters such as model architecture, loss function, and optimisation algorithm. In this work we present {\textbackslash}emph\{Population Based Training ({PBT})\}, a simple asynchronous optimisation algorithm which effectively utilises a fixed computational budget to jointly optimise a population of models and their hyperparameters to maximise performance. Importantly, {PBT} discovers a schedule of hyperparameter settings rather than following the generally sub-optimal strategy of trying to find a single fixed set to use for the whole course of training. With just a small modification to a typical distributed hyperparameter training framework, our method allows robust and reliable training of models. We demonstrate the effectiveness of {PBT} on deep reinforcement learning problems, showing faster wall-clock convergence and higher final performance of agents by optimising over a suite of hyperparameters. In addition, we show the same method can be applied to supervised learning for machine translation, where {PBT} is used to maximise the {BLEU} score directly, and also to training of Generative Adversarial Networks to maximise the Inception score of generated images. In all cases {PBT} results in the automatic discovery of hyperparameter schedules and model selection which results in stable training and better final performance.},
  	author = {Jaderberg, Max and Dalibard, Valentin and Osindero, Simon and Czarnecki, Wojciech M. and Donahue, Jeff and Razavi, Ali and Vinyals, Oriol and Green, Tim and Dunning, Iain and Simonyan, Karen and Fernando, Chrisantha and Kavukcuoglu, Koray},
  	date = {2017},
  	file = {Jaderberg et al_2017_Population Based Training of Neural Networks.pdf:/Users/jaime/Zotero/storage/B3YGHRVK/Jaderberg et al_2017_Population Based Training of Neural Networks.pdf:application/pdf},
  }

  @article{Tay2020,
  	title = {Efficient Transformers: A Survey},
  	url = {http://arxiv.org/abs/2009.06732},
  	abstract = {Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of {\textbackslash}emph\{"X-former"\} models have been proposed - Reformer, Linformer, Performer, Longformer, to name a few - which improve upon the original Transformer architecture, many of which make improvements around computational and memory {\textbackslash}emph\{efficiency\}. With the aim of helping the avid researcher navigate this flurry, this paper characterizes a large and thoughtful selection of recent efficiency-flavored "X-former" models, providing an organized and comprehensive overview of existing work and models across multiple domains.},
  	pages = {1--28},
  	issue = {August 2020},
  	author = {Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
  	date = {2020},
  	keywords = {deep learning, atten-, natural language processing, transformer models},
  	file = {Tay et al_2020_Efficient Transformers.pdf:/Users/jaime/Zotero/storage/VSXA5VUT/Tay et al_2020_Efficient Transformers.pdf:application/pdf},
  }

  @article{Henderson2018,
  	title = {Deep reinforcement learning that matters},
  	abstract = {In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning ({RL}). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep {RL} methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep {RL} more reproducible. We aim to spur discussion about how to ensure continued progress in the field by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.},
  	pages = {3207--3214},
  	journaltitle = {32nd {AAAI} Conference on Artificial Intelligence, {AAAI} 2018},
  	author = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
  	date = {2018},
  	note = {{ISBN}: 9781577358008},
  }

  @article{Schmidhuber1991,
  	title = {Learning to Generate Artificial Fovea Trajectories for Target Detection},
  	author = {Schmidhuber, Jürgen and Huber, Rudolf},
  	date = {1991},
  	file = {Schmidhuber_Huber_1991_Learning to Generate Artificial Fovea Trajectories for Target Detection.pdf:/Users/jaime/Zotero/storage/L3EZ2WNI/Schmidhuber_Huber_1991_Learning to Generate Artificial Fovea Trajectories for Target Detection.pdf:application/pdf},
  }

  @article{Ramachandran2019,
  	title = {Stand-Alone Self-Attention in Vision Models},
  	url = {http://arxiv.org/abs/1906.05909},
  	abstract = {Convolutions are a fundamental building block of modern computer vision systems. Recent approaches have argued for going beyond convolutions in order to capture long-range dependencies. These efforts focus on augmenting convolutional models with content-based interactions, such as self-attention and non-local means, to achieve gains on a number of vision tasks. The natural question that arises is whether attention can be a stand-alone primitive for vision models instead of serving as just an augmentation on top of convolutions. In developing and testing a pure self-attention vision model, we verify that self-attention can indeed be an effective stand-alone layer. A simple procedure of replacing all instances of spatial convolutions with a form of self-attention applied to {ResNet} model produces a fully self-attentional model that outperforms the baseline on {ImageNet} classification with 12\% fewer {FLOPS} and 29\% fewer parameters. On {COCO} object detection, a pure self-attention model matches the {mAP} of a baseline {RetinaNet} while having 39\% fewer {FLOPS} and 34\% fewer parameters. Detailed ablation studies demonstrate that self-attention is especially impactful when used in later layers. These results establish that stand-alone self-attention is an important addition to the vision practitioner's toolbox.},
  	pages = {1--13},
  	issue = {{NeurIPS}},
  	author = {Ramachandran, Prajit and Parmar, Niki and Vaswani, Ashish and Bello, Irwan and Levskaya, Anselm and Shlens, Jonathon},
  	date = {2019},
  	file = {Ramachandran et al_2019_Stand-Alone Self-Attention in Vision Models.pdf:/Users/jaime/Zotero/storage/MRCQ3VQZ/Ramachandran et al_2019_Stand-Alone Self-Attention in Vision Models.pdf:application/pdf},
  }

  @article{Hochreiter1997,
  	title = {Long Short-Term Memory},
  	volume = {9},
  	issn = {08997667},
  	doi = {10.1162/neco.1997.9.8.1735},
  	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory ({LSTM}). Truncating the gradient where this does not do harm, {LSTM} can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. {LSTM} is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, {LSTM} leads to many more successful runs, and learns much faster. {LSTM} also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  	pages = {1735--1780},
  	number = {8},
  	journaltitle = {Neural Computation},
  	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  	date = {1997},
  	pmid = {9377276},
  }

  @article{Chuang2014,
  	title = {Retinal implants: A systematic review},
  	volume = {98},
  	issn = {14682079},
  	doi = {10.1136/bjophthalmol-2013-303708},
  	abstract = {Retinal implants present an innovative way of restoring sight in degenerative retinal diseases. Previous reviews of research progress were written by groups developing their own devices. This systematic review objectively compares selected models by examining publications describing five representative retinal prostheses: Argus {II}, Boston Retinal Implant Project, Epi-Ret 3, Intelligent Medical Implants ({IMI}) and Alpha-{IMS} (Retina Implant {AG}). Publications were analysed using three criteria for interim success: clinical availability, vision restoration potential and long-term biocompatibility. Clinical availability: Argus {II} is the only device with {FDA} approval. Argus {II} and Alpha-{IMS} have both received the European {CE} Marking. All others are in clinical trials, except the Boston Retinal Implant, which is in animal studies. Vision restoration: resolution theoretically correlates with electrode number. Among devices with external cameras, the Boston Retinal Implant leads with 100 electrodes, followed by Argus {II} with 60 electrodes and visual acuity of 20/1262. Instead of an external camera, Alpha-{IMS} uses a photodiode system dependent on natural eye movements and can deliver visual acuity up to 20/546. Long-term compatibility: {IMI} offers iterative learning; Epi-Ret 3 is a fully intraocular device; Alpha-{IMS} uses intraocular photosensitive elements. Merging the results of these three criteria, Alpha-{IMS} is the most likely to achieve long-term success decades later, beyond current clinical availability.},
  	pages = {852--856},
  	number = {7},
  	journaltitle = {British Journal of Ophthalmology},
  	author = {Chuang, Alice T. and Margo, Curtis E. and Greenberg, Paul B.},
  	date = {2014},
  	pmid = {24403565},
  	file = {Chuang et al_2014_Retinal implants.pdf:/Users/jaime/Zotero/storage/UJ8PM7B4/Chuang et al_2014_Retinal implants.pdf:application/pdf},
  }

  @article{Mnih2014,
  	title = {Recurrent models of visual attention},
  	volume = {3},
  	issn = {10495258},
  	abstract = {Applying convolutional neural networks to large images is computationally expensive because the amount of computation scales linearly with the number of image pixels. We present a novel recurrent neural network model that is capable of extracting information from an image or video by adaptively selecting a sequence of regions or locations and only processing the selected regions at high resolution. Like convolutional neural networks, the proposed model has a degree of translation invariance built-in, but the amount of computation it performs can be controlled independently of the input image size. While the model is non-differentiable, it can be trained using reinforcement learning methods to learn task-specific policies. We evaluate our model on several image classification tasks, where it significantly outperforms a convolutional neural network baseline on cluttered images, and on a dynamic visual control problem, where it learns to track a simple object without an explicit training signal for doing so.},
  	pages = {2204--2212},
  	issue = {January},
  	journaltitle = {Advances in Neural Information Processing Systems},
  	author = {Mnih, Volodymyr and Heess, Nicolas and Graves, Alex and Kavukcuoglu, Koray},
  	date = {2014},
  	file = {Mnih et al_2014_Recurrent models of visual attention.pdf:/Users/jaime/Zotero/storage/VEISM5HX/Mnih et al_2014_Recurrent models of visual attention.pdf:application/pdf},
  }

  @article{Moiseyev1991,
  	title = {Noogenesis—The Fundamental Problem of Our Time},
  	volume = {32},
  	issn = {15561844},
  	doi = {10.1080/02604027.1991.9972260},
  	pages = {197--206},
  	number = {4},
  	journaltitle = {World Futures},
  	author = {Moiseyev, N. N.},
  	date = {1991},
  	keywords = {contradictions, global evolution, global reason, noosphere, transition},
  	file = {Moiseyev_1991_Noogenesis—The Fundamental Problem of Our Time.pdf:/Users/jaime/Zotero/storage/D7J3M5UK/Moiseyev_1991_Noogenesis—The Fundamental Problem of Our Time.pdf:application/pdf},
  }

  @article{Graves2014,
  	title = {Neural Turing Machines},
  	url = {http://arxiv.org/abs/1410.5401},
  	abstract = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
  	pages = {1--26},
  	author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
  	date = {2014},
  }

  @article{Khetarpal2018,
  	title = {Attend Before you Act: Leveraging human visual attention for continual learning},
  	url = {http://arxiv.org/abs/1807.09664},
  	abstract = {When humans perform a task, such as playing a game, they selectively pay attention to certain parts of the visual input, gathering relevant information and sequentially combining it to build a representation from the sensory data. In this work, we explore leveraging where humans look in an image as an implicit indication of what is salient for decision making. We build on top of the {UNREAL} architecture in {DeepMind} Lab's 3D navigation maze environment. We train the agent both with original images and foveated images, which were generated by overlaying the original images with saliency maps generated using a real-time spectral residual technique. We investigate the effectiveness of this approach in transfer learning by measuring performance in the context of noise in the environment.},
  	author = {Khetarpal, Khimya and Precup, Doina},
  	date = {2018},
  }

  @article{Abraham1987,
  	title = {Mathematics and evolution: A manifesto},
  	volume = {23},
  	issn = {15561844},
  	doi = {10.1080/02604027.1988.9972049},
  	abstract = {This paper deals with various possibilities for the role of mathematical modeling and computer simulation in attempting to deal with the crises of evolution. Brief introductions to some concepts of holarchic dynamics are included. © 1987 Gordon and Breach, Science Publishers, Inc.},
  	pages = {237--261},
  	number = {4},
  	journaltitle = {World Futures},
  	author = {Abraham, Ralph H},
  	date = {1987},
  }

  @article{Hou2007,
  	title = {Saliency Detection: A Spectral Residual Approach},
  	url = {http://arxiv.org/abs/1807.09664},
  	number = {800},
  	author = {Hou, Xiaodi and Zhang, Liqing},
  	date = {2007},
  	note = {{ISBN}: 1424411807},
  }

  @article{Cornia2018,
  	title = {Predicting human eye fixations via an {LSTM}-Based saliency attentive model},
  	volume = {27},
  	issn = {10577149},
  	doi = {10.1109/TIP.2018.2851672},
  	abstract = {Data-driven saliency has recently gained a lot of attention thanks to the use of convolutional neural networks for predicting gaze fixations. In this paper, we go beyond standard approaches to saliency prediction, in which gaze maps are computed with a feed-forward network, and present a novel model which can predict accurate saliency maps by incorporating neural attentive mechanisms. The core of our solution is a convolutional long short-term memory that focuses on the most salient regions of the input image to iteratively refine the predicted saliency map. In addition, to tackle the center bias typical of human eye fixations, our model can learn a set of prior maps generated with Gaussian functions. We show, through an extensive evaluation, that the proposed architecture outperforms the current state-of-the-art on public saliency prediction datasets. We further study the contribution of each key component to demonstrate their robustness on different scenarios.},
  	pages = {5142--5154},
  	number = {10},
  	journaltitle = {{IEEE} Transactions on Image Processing},
  	author = {Cornia, Marcella and Baraldi, Lorenzo and Serra, Giuseppe and Cucchiara, Rita},
  	date = {2018},
  	keywords = {Saliency, deep learning, convolutional neural networks, human eye fixations},
  	file = {Cornia et al_2018_Predicting human eye fixations via an LSTM-Based saliency attentive model.pdf:/Users/jaime/Zotero/storage/U2ITJ62U/Cornia et al_2018_Predicting human eye fixations via an LSTM-Based saliency attentive model.pdf:application/pdf},
  }

  @article{Jaderberg2018,
  	title = {Reinforcement Learning with Unsupervised Auxiliary Tasks},
  	issn = {0004-6361},
  	doi = {10.1051/0004-6361/201527329},
  	pages = {1--9},
  	author = {Jaderberg, Max and Mnih, Volodymyr},
  	date = {2018},
  	pmid = {23459267},
  	note = {{ISBN}: 2004012439},
  	file = {Jaderberg_Mnih_2018_Reinforcement Learning with Unsupervised Auxiliary Tasks.pdf:/Users/jaime/Zotero/storage/SGYK7KMD/Jaderberg_Mnih_2018_Reinforcement Learning with Unsupervised Auxiliary Tasks.pdf:application/pdf},
  }

  @inproceedings{Mott2019,
  	title = {Towards interpretable reinforcement learning using attention augmented agents},
  	volume = {32},
  	url = {http://arxiv.org/abs/1906.02500},
  	abstract = {Inspired by recent work in attention models for image captioning and question answering, we present a soft attention model for the reinforcement learning domain. This model uses a soft, top-down attention mechanism to create a bottleneck in the agent, forcing it to focus on task-relevant information by sequentially querying its view of the environment. The output of the attention mechanism allows direct observation of the information used by the agent to select its actions, enabling easier interpretation of this model than of traditional models. We analyze different strategies that the agents learn and show that a handful of strategies arise repeatedly across different games. We also show that the model learns to query separately about space and content (“where” vs. “what”). We demonstrate that an agent using this mechanism can achieve performance competitive with state-of-the-art models on {ATARI} tasks while still being interpretable.},
  	booktitle = {Advances in Neural Information Processing Systems},
  	author = {Mott, Alex and Zoran, Daniel and Chrzanowski, Mike and Wierstra, Daan and Rezende, Danilo J.},
  	urldate = {2020-05-02},
  	date = {2019-06-06},
  	note = {Issue: {NeurIPS}
  {ISSN}: 10495258},
  	keywords = {★},
  	file = {Mott et al_2019_Towards interpretable reinforcement learning using attention augmented agents.pdf:/Users/jaime/Zotero/storage/XZE92N24/Mott et al_2019_Towards interpretable reinforcement learning using attention augmented agents.pdf:application/pdf;Mott et al. - 2019 - Towards Interpretable Reinforcement Learning Using Attention Augmented Agents.pdf:/Users/jaime/Zotero/storage/2M6C572J/Mott et al. - 2019 - Towards Interpretable Reinforcement Learning Using Attention Augmented Agents.pdf:application/pdf},
  }

  @article{Mishkin1983,
  	title = {Object vision and spatial vision: two cortical pathways},
  	volume = {6},
  	issn = {01662236},
  	doi = {10.1016/0166-2236(83)90190-X},
  	abstract = {Evidence is reviewed indicating that striate cortex in the monkey is the source of two multisynaptic corticocortical pathways. One courses ventrally, interconnecting the striate, prestriate, and inferior temporal areas, and enables the visual identification of objects. The other runs dorsally, interconnecting the striate, prestriate, and inferior parietal areas, and allows instead the visual location of objects. How the information carried in these two separate pathways is reintegrated has become an important question for future research. © 1983.},
  	pages = {414--417},
  	issue = {C},
  	journaltitle = {Trends in Neurosciences},
  	author = {Mishkin, Mortimer and Ungerleider, Leslie G. and Macko, Kathleen A.},
  	date = {1983},
  	file = {Mishkin et al_1983_Object vision and spatial vision.pdf:/Users/jaime/Zotero/storage/P2AUJH7G/Mishkin et al_1983_Object vision and spatial vision.pdf:application/pdf},
  }

  @article{Goodale1992,
  	title = {Separate visual pathways for perception and action},
  	doi = {10.1016/0166-2236(92)90344-8},
  	abstract = {Accumulating neuropsychological, electrophysiological and behavioural evidence suggests that the neural substrates of visual perception may be quite distinct from those underlying the visual control of actions. In other words, the set of object descriptions that permit identification and recognition may be computed inde- pendently of the set of descriptions that allow an observer to shape the hand appropriately to pick up an object. We propose that the ventral stream of projections from the striate cortex to the inferotemporal cortex plays the major role in the perceptual identification of objects, while the dorsal stream projecting from the striate cortex to the posterior parietal region mediates the required sensorimotor transformations for visually guided actions directed at such objects},
  	author = {Goodale, Melvyn and Milner, David},
  	date = {1992},
  	file = {Goodale_Milner_1992_Separate visual pathways for perception and action.pdf:/Users/jaime/Zotero/storage/U4DMIJRS/Goodale_Milner_1992_Separate visual pathways for perception and action.pdf:application/pdf},
  }

  @online{Ha2017,
  	title = {A Visual Guide to Evolution Strategies},
  	url = {https://blog.otoro.net/2017/10/29/visual-evolution-strategies/},
  	author = {Ha, David},
  	urldate = {2020-09-27},
  	date = {2017},
  }

  @article{Bashashati2007,
  	title = {A survey of signal processing algorithms in brain-computer interfaces based on electrical brain signals},
  	volume = {4},
  	issn = {17412560},
  	doi = {10.1088/1741-2560/4/2/R03},
  	abstract = {Brain-computer interfaces ({BCIs}) aim at providing a non-muscular channel for sending commands to the external world using the electroencephalographic activity or other electrophysiological measures of the brain function. An essential factor in the successful operation of {BCI} systems is the methods used to process the brain signals. In the {BCI} literature, however, there is no comprehensive review of the signal processing techniques used. This work presents the first such comprehensive survey of all {BCI} designs using electrical signal recordings published prior to January 2006. Detailed results from this survey are presented and discussed. The following key research questions are addressed: (1) what are the key signal processing components of a {BCI}, (2) what signal processing algorithms have been used in {BCIs} and (3) which signal processing techniques have received more attention? © 2007 {IOP} Publishing Ltd.},
  	number = {2},
  	journaltitle = {Journal of Neural Engineering},
  	author = {Bashashati, Ali and Fatourechi, Mehrdad and Ward, Rabab K. and Birch, Gary E.},
  	date = {2007},
  	pmid = {17409474},
  	file = {Bashashati et al_2007_A survey of signal processing algorithms in brain-computer interfaces based on.pdf:/Users/jaime/Zotero/storage/7QCBPBCT/Bashashati et al_2007_A survey of signal processing algorithms in brain-computer interfaces based on.pdf:application/pdf},
  }

  @article{Lotte2018,
  	title = {A review of classification algorithms for {EEG}-based brain-computer interfaces: A 10 year update},
  	volume = {15},
  	issn = {17412552},
  	doi = {10.1088/1741-2552/aab2f2},
  	abstract = {Objective. Most current electroencephalography ({EEG})-based brain-computer interfaces ({BCIs}) are based on machine learning algorithms. There is a large diversity of classifier types that are used in this field, as described in our 2007 review paper. Now, approximately ten years after this review publication, many new algorithms have been developed and tested to classify {EEG} signals in {BCIs}. The time is therefore ripe for an updated review of {EEG} classification algorithms for {BCIs}. Approach. We surveyed the {BCI} and machine learning literature from 2007 to 2017 to identify the new classification approaches that have been investigated to design {BCIs}. We synthesize these studies in order to present such algorithms, to report how they were used for {BCIs}, what were the outcomes, and to identify their pros and cons. Main results. We found that the recently designed classification algorithms for {EEG}-based {BCIs} can be divided into four main categories: adaptive classifiers, matrix and tensor classifiers, transfer learning and deep learning, plus a few other miscellaneous classifiers. Among these, adaptive classifiers were demonstrated to be generally superior to static ones, even with unsupervised adaptation. Transfer learning can also prove useful although the benefits of transfer learning remain unpredictable. Riemannian geometry-based methods have reached state-of-the-art performances on multiple {BCI} problems and deserve to be explored more thoroughly, along with tensor-based methods. Shrinkage linear discriminant analysis and random forests also appear particularly useful for small training samples settings. On the other hand, deep learning methods have not yet shown convincing improvement over state-of-the-art {BCI} methods. Significance. This paper provides a comprehensive overview of the modern classification algorithms used in {EEG}-based {BCIs}, presents the principles of these methods and guidelines on when and how to use them. It also identifies a number of challenges to further advance {EEG} classification in {BCI}.},
  	number = {3},
  	journaltitle = {Journal of Neural Engineering},
  	author = {Lotte, F and Bougrain, L and Cichocki, A and Clerc, M and Congedo, M and Rakotomamonjy, A and Yger, F},
  	date = {2018},
  	pmid = {29488902},
  	note = {Publisher: {IOP} Publishing},
  	keywords = {brain-computer interfaces, classification, deep learning, electroencephalography, Riemannian geometry, spatial filtering, transfer learning},
  	file = {Lotte et al_2018_A review of classification algorithms for EEG-based brain-computer interfaces.pdf:/Users/jaime/Zotero/storage/4EXB438P/Lotte et al_2018_A review of classification algorithms for EEG-based brain-computer interfaces.pdf:application/pdf},
  }

  @article{Pregenzer1999,
  	title = {Frequency component selection for an {EEG}-based brain to computer interface},
  	volume = {7},
  	issn = {10636528},
  	doi = {10.1109/86.808944},
  	abstract = {A new communication channel for severely handicapped people could be opened with a direct brain to computer interface ({BCI}). Such a system classifies electrical brain signals online. In a series of training sessions, where electroencephalograph ({EEG}) signals are recorded on the intact scalp, a classifier is trained to discriminate a limited number of different brain states. In a subsequent series of feedback sessions, where the subject is confronted with the classification results, the subject tries to reduce the number of misclassifications. In this study the relevance of different spectral components is analyzed: 1) on the training sessions to select optimal frequency bands for the feedback sessions and 2) on the feedback sessions to monitor changes.},
  	pages = {413--419},
  	number = {4},
  	journaltitle = {{IEEE} Transactions on Rehabilitation Engineering},
  	author = {Pregenzer, M. and Pfurtscheller, G.},
  	date = {1999},
  	pmid = {10609628},
  	keywords = {Brain to computer interface ({BCI}), Classification, Distinctive sensitive learning vector quantization, Feature selection},
  	file = {Pregenzer_Pfurtscheller_1999_Frequency component selection for an EEG-based brain to computer interface.pdf:/Users/jaime/Zotero/storage/CKKDT8Z9/Pregenzer_Pfurtscheller_1999_Frequency component selection for an EEG-based brain to computer interface.pdf:application/pdf},
  }

  @article{Stanley2019,
  	title = {Why Open-Endedness Matters},
  	volume = {25},
  	issn = {15309185},
  	doi = {10.1162/artl_a_00294},
  	abstract = {Rather than acting as a review or analysis of the field, this essay focuses squarely on the motivations for investigating open-endedness and the opportunities it opens up. It begins by contemplating the awesome accomplishments of evolution in nature and the profound implications if such a process could be ignited on a computer. Some of the milestones in our understanding so far are then discussed, finally closing by highlighting the grand challenge of formalizing open-endedness as a computational process that can be encoded as an algorithm. The main contribution is to articulate why open-endedness deserves a place alongside artificial intelligence as one of the great computational challenges, and opportunities, of our time.},
  	pages = {232--235},
  	number = {3},
  	journaltitle = {Artificial Life},
  	author = {Stanley, Kenneth O.},
  	date = {2019},
  	pmid = {31397603},
  	keywords = {Novelty search, Open-ended evolution, Artificial intelligence, Machine learning, Open-ended algorithms, Open-endedness, Quality diversity},
  	file = {Stanley_2019_Why Open-Endedness Matters.pdf:/Users/jaime/Zotero/storage/TQ93CY26/Stanley_2019_Why Open-Endedness Matters.pdf:application/pdf},
  }

  @article{Lv2020,
  	title = {Improving Target-driven Visual Navigation with Attention on 3D Spatial Relationships},
  	url = {http://arxiv.org/abs/2005.02153},
  	abstract = {Embodied artificial intelligence ({AI}) tasks shift from tasks focusing on internet images to active settings involving embodied agents that perceive and act within 3D environments. In this paper, we investigate the target-driven visual navigation using deep reinforcement learning ({DRL}) in 3D indoor scenes, whose navigation task aims to train an agent that can intelligently make a series of decisions to arrive at a pre-specified target location from any possible starting positions only based on egocentric views. However, most navigation methods currently struggle against several challenging problems, such as data efficiency, automatic obstacle avoidance, and generalization. Generalization problem means that agent does not have the ability to transfer navigation skills learned from previous experience to unseen targets and scenes. To address these issues, we incorporate two designs into classic {DRL} framework: attention on 3D knowledge graph ({KG}) and target skill extension ({TSE}) module. On the one hand, our proposed method combines visual features and 3D spatial representations to learn navigation policy. On the other hand, {TSE} module is used to generate sub-targets which allow agent to learn from failures. Specifically, our 3D spatial relationships are encoded through recently popular graph convolutional network ({GCN}). Considering the real world settings, our work also considers open action and adds actionable targets into conventional navigation situations. Those more difficult settings are applied to test whether {DRL} agent really understand its task, navigating environment, and can carry out reasoning. Our experiments, performed in the {AI}2-{THOR}, show that our model outperforms the baselines in both {SR} and {SPL} metrics, and improves generalization ability across targets and scenes.},
  	pages = {1--12},
  	author = {Lv, Yunlian and Xie, Ning and Shi, Yimin and Wang, Zijiao and Shen, Heng Tao},
  	date = {2020},
  	file = {Lv et al_2020_Improving Target-driven Visual Navigation with Attention on 3D Spatial.pdf:/Users/jaime/Zotero/storage/G279EGKT/Lv et al_2020_Improving Target-driven Visual Navigation with Attention on 3D Spatial.pdf:application/pdf},
  }

  @article{Kubler2020,
  	title = {The history of {BCI}: From a vision for the future to real support for personhood in people with locked-in syndrome},
  	volume = {13},
  	issn = {18745504},
  	doi = {10.1007/s12152-019-09409-4},
  	abstract = {The history of brain-computer interfaces ({BCI}) developed from a mere idea in the days of early digital technology to today’s highly sophisticated approaches for signal detection, recording, and analysis. In the 1960s, electroencephalography ({EEG}) was tied to the laboratory due to equipment and recording requirements. Today, amplifiers exist that are built in the electrode cap and are so resistant to movement artefacts that data collection in the field is no longer a critical issue. Within 60 years, the field has moved from simple and artefact-sensitive {EEG} recording to making real the vision of brain-computer communication. In the last 40 years, direct brain-computer interaction went from simple communication programs to sophisticated {BCI}-controlled applications. In the past two decades, much research was conducted with locked-in individuals, and since the 2010s, independent home use by exemplary patients has been demonstrated. In these patients with locked-in syndrome ({LIS}), {BCI} were installed at their home and long-term usage was established, resulting in increased quality of life ({QOL}). Maintaining communication in disorders leading to {LIS} contributes significantly to the patients’ sense of being full persons. {BCI} as an assistive technology will likely be perceived as integral part of the self: insofar as it can prevent total loss of communication and the ensuing social isolation, it enables essential conditions for the subjective and intersubjective experience of personhood.},
  	pages = {163--180},
  	number = {2},
  	journaltitle = {Neuroethics},
  	author = {Kübler, Andrea},
  	date = {2020},
  	note = {Publisher: Neuroethics},
  	keywords = {Communication, Quality of life ({QOL}), User-Centred design, Amyotrophic lateral sclerosis ({ALS}), brain-compute},
  	file = {Kübler_2020_The history of BCI.pdf:/Users/jaime/Zotero/storage/IYQVPNFS/Kübler_2020_The history of BCI.pdf:application/pdf},
  }

  @article{Zhang2020,
  	title = {An Explainable 3D Residual Self-Attention Deep Neural Network {FOR} Joint Atrophy Localization and Alzheimer's Disease Diagnosis using Structural {MRI}},
  	volume = {14},
  	url = {http://arxiv.org/abs/2008.04024},
  	abstract = {Computer-aided early diagnosis of Alzheimer's disease ({AD}) and its prodromal form mild cognitive impairment ({MCI}) based on structure Magnetic Resonance Imaging ({sMRI}) has provided a cost-effective and objective way for early prevention and treatment of disease progression, leading to improved patient care. In this work, we have proposed a novel computer-aided approach for early diagnosis of {AD} by introducing an explainable 3D Residual Attention Deep Neural Network (3D {ResAttNet}) for end-to-end learning from {sMRI} scans. Different from the existing approaches, the novelty of our approach is three-fold: 1) A Residual Self-Attention Deep Neural Network has been proposed to capture local, global and spatial information of {MR} images to improve diagnostic performance; 2) An explainable method using Gradient-based Localization Class Activation mapping (Grad-{CAM}) has been introduced to improve the explainable of the proposed method; 3) This work has provided a full end-to-end learning solution for automated disease diagnosis. Our proposed 3D {ResAttNet} method has been evaluated on a large cohort of subjects from real dataset for two changeling classification tasks (i.e. Alzheimer's disease ({AD}) vs. Normal cohort ({NC}) and progressive {MCI} ({pMCI}) vs. stable {MCI} ({sMCI})). The experimental results show that the proposed approach outperforms the state-of-the-art models with significant performance improvement. The accuracy for {AD} vs. {NC} and {sMCI} vs. {pMCI} task are 97.1\% and 84.1\% respectively. The explainable mechanism in our approach regions is able to identify and highlight the contribution of the important brain parts (hippocampus, lateral ventricle and most parts of the cortex) for transparent decisions.},
  	pages = {1--10},
  	number = {8},
  	author = {Zhang, Xin and Han, Liangxiu and Zhu, Wenyong and Sun, Liang and Zhang, Daoqiang},
  	date = {2020},
  	file = {Zhang et al_2020_An Explainable 3D Residual Self-Attention Deep Neural Network FOR Joint Atrophy.pdf:/Users/jaime/Zotero/storage/GN9FQZAN/Zhang et al_2020_An Explainable 3D Residual Self-Attention Deep Neural Network FOR Joint Atrophy.pdf:application/pdf},
  }

  @article{Mirowski2017,
  	title = {Learning to navigate in complex environments},
  	abstract = {Learning to navigate in complex environments with dynamic elements is an important milestone in developing {AI} agents. In this work we formulate the navigation question as a reinforcement learning problem and show that data efficiency and task performance can be dramatically improved by relying on additional auxiliary tasks leveraging multimodal sensory inputs. In particular we consider jointly learning the goal-driven reinforcement learning problem with auxiliary depth prediction and loop closure classification tasks. This approach can learn to navigate from raw sensory input in complicated 3D mazes, approaching human-level performance even under conditions where the goal location changes frequently. We provide detailed analysis of the agent behaviour1, its ability to localise, and its network activity dynamics, showing that the agent implicitly learns key navigation abilities.},
  	journaltitle = {5th International Conference on Learning Representations, {ICLR} 2017 - Conference Track Proceedings},
  	author = {Mirowski, Piotr and Pascanu, Razvan and Viola, Fabio and Soyer, Hubert and Ballard, Andrew J. and Banino, Andrea and Denil, Misha and Goroshin, Ross and Sifre, Laurent and Kavukcuoglu, Koray and Kumaran, Dharshan and Hadsell, Raia},
  	date = {2017},
  	file = {Mirowski et al_2017_Learning to navigate in complex environments.pdf:/Users/jaime/Zotero/storage/6EQWB6NM/Mirowski et al_2017_Learning to navigate in complex environments.pdf:application/pdf},
  }

  @article{Zhu2017,
  	title = {Target-driven visual navigation in indoor scenes using deep reinforcement learning},
  	issn = {10504729},
  	doi = {10.1109/ICRA.2017.7989381},
  	abstract = {Two less addressed issues of deep reinforcement learning are (1) lack of generalization capability to new goals, and (2) data inefficiency, i.e., the model requires several (and often costly) episodes of trial and error to converge, which makes it impractical to be applied to real-world scenarios. In this paper, we address these two issues and apply our model to target-driven visual navigation. To address the first issue, we propose an actor-critic model whose policy is a function of the goal as well as the current state, which allows better generalization. To address the second issue, we propose the {AI}2-{THOR} framework, which provides an environment with high-quality 3D scenes and a physics engine. Our framework enables agents to take actions and interact with objects. Hence, we can collect a huge number of training samples efficiently. We show that our proposed method (1) converges faster than the state-of-the-art deep reinforcement learning methods, (2) generalizes across targets and scenes, (3) generalizes to a real robot scenario with a small amount of fine-tuning (although the model is trained in simulation), (4) is end-to-end trainable and does not need feature engineering, feature matching between frames or 3D reconstruction of the environment.},
  	pages = {3357--3364},
  	journaltitle = {Proceedings - {IEEE} International Conference on Robotics and Automation},
  	author = {Zhu, Yuke and Mottaghi, Roozbeh and Kolve, Eric and Lim, Joseph J. and Gupta, Abhinav and Fei-Fei, Li and Farhadi, Ali},
  	date = {2017},
  	note = {{ISBN}: 9781509046331},
  	file = {Zhu et al_2017_Target-driven visual navigation in indoor scenes using deep reinforcement.pdf:/Users/jaime/Zotero/storage/VNYJ24XI/Zhu et al_2017_Target-driven visual navigation in indoor scenes using deep reinforcement.pdf:application/pdf},
  }

  @book{Dawkins2003,
  	title = {The evolution of evolvability},
  	isbn = {978-0-12-428765-5},
  	url = {http://dx.doi.org/10.1016/B978-012428765-5/50046-3},
  	abstract = {This chapter illustrates the use of artificial life, not as a formal model of real life but as a generator of insight in the understanding of real life. Genetics is the study of the relationships between genotypes in successive generations. Embryology is the study of the relationship between genotype and phenotype in any one generation. The fundamental principle of embryology in real life was formulated by Weismann. In every generation, the genes of that generation influence the phenotype of that generation. The success of that phenotype determines whether or not the genes that it bears, a set that largely overlaps with the genes that influenced its development, shall go forward to the next generation. The chapter explores the far-reaching consequences of the fact that these sets do not necessarily have to overlap. Any individual born, therefore, inherits genes that have succeeded in building a long series of successful phenotypes, for the simple reason that failed phenotypes do not pass on their genes. It is important to understand that genes do two quite distinct things. They participate in embryology, influencing the development of the phenotype in a given generation; and they participate in genetics, getting themselves copied down the generations. It is too often not realized-even by some of those that wear the labels geneticist or embryologist-that there is a radical separation between the disciplines of genetics and embryology. © 2003 Elsevier Ltd. All rights reserved.},
  	pagetotal = {239-255},
  	publisher = {Woodhead Publishing Limited},
  	author = {Dawkins, Richard},
  	date = {2003},
  	doi = {10.1016/B978-012428765-5/50046-3},
  	note = {Publication Title: On Growth, Form and Computers
  Issue: 1893},
  	file = {Dawkins_2003_The evolution of evolvability.pdf:/Users/jaime/Zotero/storage/92HHPKHG/Dawkins_2003_The evolution of evolvability.pdf:application/pdf},
  }

  @article{Muryy2020,
  	title = {Lessons from reinforcement learning for biological representations of space},
  	volume = {174},
  	issn = {18785646},
  	url = {https://doi.org/10.1016/j.visres.2020.05.009},
  	doi = {10.1016/j.visres.2020.05.009},
  	abstract = {Neuroscientists postulate 3D representations in the brain in a variety of different coordinate frames (e.g. ‘head-centred’, ‘hand-centred’ and ‘world-based’). Recent advances in reinforcement learning demonstrate a quite different approach that may provide a more promising model for biological representations underlying spatial perception and navigation. In this paper, we focus on reinforcement learning methods that reward an agent for arriving at a target image without any attempt to build up a 3D ‘map’. We test the ability of this type of representation to support geometrically consistent spatial tasks such as interpolating between learned locations using decoding of feature vectors. We introduce a hand-crafted representation that has, by design, a high degree of geometric consistency and demonstrate that, in this case, information about the persistence of features as the camera translates (e.g. distant features persist) can improve performance on the geometric tasks. These examples avoid Cartesian (in this case, 2D) representations of space. Non-Cartesian, learned representations provide an important stimulus in neuroscience to the search for alternatives to a ‘cognitive map’.},
  	pages = {79--93},
  	issue = {October 2019},
  	journaltitle = {Vision Research},
  	author = {Muryy, Alex and Siddharth, N. and Nardelli, Nantas and Glennerster, Andrew and Torr, Philip H.S.},
  	date = {2020},
  	pmid = {32683096},
  	note = {Publisher: Elsevier},
  	keywords = {Navigation, Deep Reinforcement Learning, 3D spatial representation, Moving observer, Parallax, View-based},
  	file = {Muryy et al_2020_Lessons from reinforcement learning for biological representations of space.pdf:/Users/jaime/Zotero/storage/8Q9XJEVZ/Muryy et al_2020_Lessons from reinforcement learning for biological representations of space.pdf:application/pdf},
  }

  @article{Ponce2020,
  	title = {Distributed evolutionary learning control for mobile robot navigation based on virtual and physical agents},
  	volume = {102},
  	issn = {1569190X},
  	url = {https://doi.org/10.1016/j.simpat.2019.102058},
  	doi = {10.1016/j.simpat.2019.102058},
  	abstract = {This paper presents a distributed evolutionary learning control based on social wound treatment for mobile robot navigation using an integrated multi-robot system comprised of simulated and physical robots. To do so, this work proposes an extension of the population-based metaheuristic wound treatment optimization ({WTO}) method into a distributed scheme. In addition, this distributed {WTO} method is implemented on the multi-robot system allowing them to experience the environment in their own and communicate their findings, resulting in an emergence intelligence. We implemented our proposal using the combination of five simulated robots with one physical robot for tuning a navigation controller to move freely in a workspace. Results showed that the solution found by this multi-robot system aims using the output controller in the physical robot for successfully achieving the goal to move the robot around a U-maze, without applying any transfer learning approach. We consider this proposal useful in evolutionary robotics, and of great importance to decrease the gap related to transfer knowledge in robotics from simulation to reality.},
  	pages = {102058},
  	issue = {September},
  	journaltitle = {Simulation Modelling Practice and Theory},
  	author = {Ponce, Hiram and Moya-Albor, Ernesto and Martínez-Villaseñor, Lourdes and Brieva, Jorge},
  	date = {2020},
  	note = {Publisher: Elsevier},
  	keywords = {Evolutionary robotics, Control navigation, Decentralized systems, Metaheuristic optimization, Multi-robot system},
  	file = {Ponce et al_2020_Distributed evolutionary learning control for mobile robot navigation based on.pdf:/Users/jaime/Zotero/storage/45Y8KYVT/Ponce et al_2020_Distributed evolutionary learning control for mobile robot navigation based on.pdf:application/pdf},
  }

  @article{Verbancsics2013,
  	title = {Generative {NeuroEvolution} for Deep Learning},
  	url = {http://arxiv.org/abs/1312.5355},
  	abstract = {An important goal for the machine learning ({ML}) community is to create approaches that can learn solutions with human-level capability. One domain where humans have held a significant advantage is visual processing. A significant approach to addressing this gap has been machine learning approaches that are inspired from the natural systems, such as artificial neural networks ({ANNs}), evolutionary computation ({EC}), and generative and developmental systems ({GDS}). Research into deep learning has demonstrated that such architectures can achieve performance competitive with humans on some visual tasks; however, these systems have been primarily trained through supervised and unsupervised learning algorithms. Alternatively, research is showing that evolution may have a significant role in the development of visual systems. Thus this paper investigates the role neuro-evolution ({NE}) can take in deep learning. In particular, the Hypercube-based {NeuroEvolution} of Augmenting Topologies is a {NE} approach that can effectively learn large neural structures by training an indirect encoding that compresses the {ANN} weight pattern as a function of geometry. The results show that {HyperNEAT} struggles with performing image classification by itself, but can be effective in training a feature extractor that other {ML} approaches can learn from. Thus {NeuroEvolution} combined with other {ML} methods provides an intriguing area of research that can replicate the processes in nature.},
  	pages = {1--9},
  	author = {Verbancsics, Phillip and Harguess, Josh},
  	date = {2013},
  	file = {Verbancsics_Harguess_2013_Generative NeuroEvolution for Deep Learning.pdf:/Users/jaime/Zotero/storage/6YDZSDSG/Verbancsics_Harguess_2013_Generative NeuroEvolution for Deep Learning.pdf:application/pdf},
  }

  @article{Lehnert2019,
  	title = {Retina-inspired Visual Module for Robot Navigation in Complex Environments},
  	volume = {2019-July},
  	doi = {10.1109/IJCNN.2019.8851896},
  	abstract = {Reinforcement learning ({RL}) has been widely used to implement autonomous navigation in artificial agents, where the goal is to learn a behaviour which maximizes the reward, through interaction with the environment. Most of the recent architectures used in autonomous agents obtain information from the environment using visual modules implemented by convolutional neural networks, where the visual features resulting from learning are unknown or uncertain, which impose limitations considering the large number of parameters to be learned by the entire system. Research in retina physiology has been able to characterize it not as a single light-electrical transductor but as a complex device performing a variety of computations of the visual information, preparing the data for further stages of processing in the visual system. We propose an {RL} architecture that uses retina physiology knowledge to fed the convolutional neural network, avoiding the learning stage in the sensory input. The performance of the proposed architecture was evaluated using the {DeepMind} Lab environment simulating an agent moving inside two different maze scenarios. The results obtained reveal promising extension of the inclusion of biological- plausible mechanisms inside artificial intelligence applications.},
  	pages = {1--8},
  	issue = {July},
  	journaltitle = {Proceedings of the International Joint Conference on Neural Networks},
  	author = {Lehnert, Hans and Escobar, Maria Jose and Araya, Mauricio},
  	date = {2019},
  	note = {{ISBN}: 9781728119854},
  	file = {Lehnert et al_2019_Retina-inspired Visual Module for Robot Navigation in Complex Environments.pdf:/Users/jaime/Zotero/storage/HDMY23T5/Lehnert et al_2019_Retina-inspired Visual Module for Robot Navigation in Complex Environments.pdf:application/pdf},
  }

  @article{Lehnert2019a,
  	title = {Bio-Inspired Deep Reinforcement Learning for Autonomous Navigation of Artificial Agents},
  	volume = {17},
  	issn = {15480992},
  	doi = {10.1109/TLA.2019.9011549},
  	abstract = {Autonomous navigation of artificial agents is a challenging task for changing and complex environments. Reinforcement learning ({RL}) algorithms are widely used for autonomous navigation, where the agent, through the interaction with the environment, learns the behaviors needed to maximize the reward. Recent architectures extract information from the environment using convolutional neural networks, where the visual features needed to maximize the reward are unknown and uncertain, and then, increasing the number of parameters learned by the entire system. Moreover, the presence of sparse rewards complicates, even more, the task generating unstable results in the learning problem. The work here presented is twofold. First, we show the advantages of using retina physiology knowledge to design a visual sensor feeding the {RL} network. Secondly, based on intrinsic motivation, we propose the use of auxiliary tasks to deal with sparse rewards, generating a continuous learning process. We define two auxiliary tasks, state, and action predictions, forcing the network to learn characteristics of environment; and also, to detect which of them are valuable for the task. These two contributions were implemented in the {DeepMind} Lab environment simulating an agent moving inside two different maze scenarios. The results obtained reveal a promising extension of the inclusion of biological-plausible mechanisms inside artificial intelligence applications. Moreover, to include auxiliary tasks improves the performance adding robustness to the system.},
  	pages = {2037--2044},
  	number = {12},
  	journaltitle = {{IEEE} Latin America Transactions},
  	author = {Lehnert, Hans and Araya, Mauricio and Carrasco-Davis, Rodrigo and Escobar, Maria Jose},
  	date = {2019},
  	keywords = {Reinforcement Learning, Autonomous Navigation, Visual Models},
  	file = {Lehnert et al_2019_Bio-Inspired Deep Reinforcement Learning for Autonomous Navigation of.pdf:/Users/jaime/Zotero/storage/569XBG8Z/Lehnert et al_2019_Bio-Inspired Deep Reinforcement Learning for Autonomous Navigation of.pdf:application/pdf},
  }

  @article{Cohen2007,
  	title = {Prosthetic interfaces with the visual system: Biological issues},
  	volume = {4},
  	issn = {17412560},
  	doi = {10.1088/1741-2560/4/2/R02},
  	abstract = {The design of effective visual prostheses for the blind represents a challenge for biomedical engineers and neuroscientists. Significant progress has been made in the miniaturization and processing power of prosthesis electronics; however development lags in the design and construction of effective machine-brain interfaces with visual system neurons. This review summarizes what has been learned about stimulating neurons in the human and primate retina, lateral geniculate nucleus and visual cortex. Each level of the visual system presents unique challenges for neural interface design. Blind patients with the retinal degenerative disease retinitis pigmentosa ({RP}) are a common population in clinical trials of visual prostheses. The visual performance abilities of normals and {RP} patients are compared. To generate pattern vision in blind patients, the visual prosthetic interface must effectively stimulate the retinotopically organized neurons in the central visual field to elicit patterned visual percepts. The development of more biologically compatible methods of stimulating visual system neurons is critical to the development of finer spatial percepts. Prosthesis electrode arrays need to adapt to different optimal stimulus locations, stimulus patterns, and patient disease states. © 2007 {IOP} Publishing Ltd.},
  	number = {2},
  	journaltitle = {Journal of Neural Engineering},
  	author = {Cohen, Ethan D.},
  	date = {2007},
  	pmid = {17409473},
  	file = {Cohen_2007_Prosthetic interfaces with the visual system.pdf:/Users/jaime/Zotero/storage/HIINEVP4/Cohen_2007_Prosthetic interfaces with the visual system.pdf:application/pdf},
  }

  @article{Kulhanek2019,
  	title = {Vision-based navigation using deep reinforcement learning},
  	doi = {10.1109/ECMR.2019.8870964},
  	abstract = {Deep reinforcement learning ({RL}) has been successfully applied to a variety of game-like environments. However, the application of deep {RL} to visual navigation with realistic environments is a challenging task. We propose a novel learning architecture capable of navigating an agent, e.g. a mobile robot, to a target given by an image. To achieve this, we have extended the batched A2C algorithm with auxiliary tasks designed to improve visual navigation performance. We propose three additional auxiliary tasks: predicting the segmentation of the observation image and of the target image and predicting the depth-map. These tasks enable the use of supervised learning to pre-train a major part of the network and to reduce the number of training steps substantially. The training performance has been further improved by increasing the environment complexity gradually over time. An efficient neural network structure is proposed, which is capable of learning for multiple targets in multiple environments. Our method navigates in continuous state spaces and on the {AI}2-{THOR} environment simulator surpasses the performance of state-of-the-art goal-oriented visual navigation methods from the literature.},
  	pages = {1--8},
  	journaltitle = {2019 European Conference on Mobile Robots, {ECMR} 2019 - Proceedings},
  	author = {Kulhanek, Jonas and Derner, Erik and De Bruin, Tim and Babuska, Robert},
  	date = {2019},
  	note = {Publisher: {IEEE}
  {ISBN}: 9781728136059},
  	keywords = {Deep reinforcement learning, Actor-critic, Auxiliary tasks, Robot navigation},
  	file = {Kulhanek et al_2019_Vision-based navigation using deep reinforcement learning.pdf:/Users/jaime/Zotero/storage/RAHVH25A/Kulhanek et al_2019_Vision-based navigation using deep reinforcement learning.pdf:application/pdf},
  }

  @article{Mnih2013,
  	title = {Playing Atari with Deep Reinforcement Learning},
  	abstract = {We present the first deep learning model to successfully learn control policies di- rectly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learn- ing Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
  	journaltitle = {Deep Reinforcement Learning: Fundamentals, Research and Applications},
  	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  	date = {2013},
  	note = {{ISBN}: 9789811540950},
  	keywords = {{DQN}},
  	file = {Mnih et al_2013_Playing Atari with Deep Reinforcement Learning.pdf:/Users/jaime/Zotero/storage/TCSSVXTF/Mnih et al_2013_Playing Atari with Deep Reinforcement Learning.pdf:application/pdf},
  }

  @article{Dulac-Arnold2019,
  	title = {Challenges of Real-World Reinforcement Learning},
  	url = {http://arxiv.org/abs/1904.12901},
  	abstract = {Reinforcement learning ({RL}) has proven its worth in a series of artificial domains, and is beginning to show some successes in real-world scenarios. However, much of the research advances in {RL} are often hard to leverage in real-world systems due to a series of assumptions that are rarely satisfied in practice. We present a set of nine unique challenges that must be addressed to productionize {RL} to real world problems. For each of these challenges, we specify the exact meaning of the challenge, present some approaches from the literature, and specify some metrics for evaluating that challenge. An approach that addresses all nine challenges would be applicable to a large number of real world problems. We also present an example domain that has been modified to present these challenges as a testbed for practical {RL} research.},
  	author = {Dulac-Arnold, Gabriel and Mankowitz, Daniel and Hester, Todd},
  	date = {2019},
  	file = {Dulac-Arnold et al_2019_Challenges of Real-World Reinforcement Learning.pdf:/Users/jaime/Zotero/storage/MA4CB54X/Dulac-Arnold et al_2019_Challenges of Real-World Reinforcement Learning.pdf:application/pdf},
  }

  @article{Ha2018,
  	title = {World models},
  	abstract = {We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment.},
  	author = {Ha, David and Schmidhuber, Jürgen},
  	date = {2018},
  	file = {Ha_Schmidhuber_2018_World models.pdf:/Users/jaime/Zotero/storage/KJFIAUU3/Ha_Schmidhuber_2018_World models.pdf:application/pdf},
  }

  @book{Sewak2019,
  	title = {Deep Reinforcement Learning},
  	isbn = {9789811540943},
  	abstract = {We discuss deep reinforcement learning in an overview style. We draw a big picture, filled with details. We discuss six core elements, six important mechanisms, and twelve applications, focusing on contemporary work, and in historical contexts. We start with background of artificial intelligence, machine learning, deep learning, and reinforcement learning ({RL}), with resources. Next we discuss {RL} core elements, including value function, policy, reward, model, exploration vs. exploitation, and representation. Then we discuss important mechanisms for {RL}, including attention and memory, unsupervised learning, hierarchical {RL}, multi-agent {RL}, relational {RL}, and learning to learn. After that, we discuss {RL} applications, including games, robotics, natural language processing ({NLP}), computer vision, finance, business management, healthcare, education, energy, transportation, computer systems, and, science, engineering, and art. Finally we summarize briefly, discuss challenges and opportunities, and close with an epilogue.},
  	author = {Sewak, Mohit},
  	date = {2019},
  	doi = {10.1007/978-981-13-8285-7},
  	note = {Publication Title: Deep Reinforcement Learning},
  	file = {Sewak_2019_Deep Reinforcement Learning.pdf:/Users/jaime/Zotero/storage/YND2KT85/Sewak_2019_Deep Reinforcement Learning.pdf:application/pdf},
  }

  @article{Mishra2018,
  	title = {A Simple Neural Attentive Meta-Learner},
  	abstract = {Deep neural networks excel in regimes with large amounts of data, but tend to struggle when data is scarce or when they need to adapt quickly to changes in the task. In response, recent work in meta-learning proposes training a meta-learner on a distribution of similar tasks, in the hopes of generalization to novel but related tasks by learning a high-level strategy that captures the essence of the problem it is asked to solve. However, many recent meta-learning approaches are extensively hand-designed, either using architectures specialized to a particular application, or hard-coding algorithmic components that constrain how the meta-learner solves the task. We propose a class of simple and generic meta-learner architectures that use a novel combination of temporal convolutions and soft attention; the former to aggregate information from past experience and the latter to pinpoint specific pieces of information. In the most extensive set of meta-learning experiments to date, we evaluate the resulting Simple Neural {AttentIve} Learner (or {SNAIL}) on several heavily-benchmarked tasks. On all tasks, in both supervised and reinforcement learning, {SNAIL} attains state-of-the-art performance by significant margins.},
  	pages = {1--17},
  	author = {Mishra, Nikhil},
  	date = {2018},
  	file = {Mishra_2018_A Simple Neural Attentive Meta-Learner.pdf:/Users/jaime/Zotero/storage/QID4TW5Y/Mishra_2018_A Simple Neural Attentive Meta-Learner.pdf:application/pdf},
  }

  @article{McCarthy2014,
  	title = {Mobility and low contrast trip hazard avoidance using augmented depth},
  	volume = {12},
  	issn = {1741-2552},
  	url = {https://doi.org/10.1088/1741-2560/12/1/016003},
  	doi = {10.1088/1741-2560/12/1/016003},
  	number = {1},
  	journaltitle = {Journal of Neural Engineering},
  	author = {{McCarthy}, Chris and Walker, Janine G and Lieby, Paulette and Scott, Adele and Barnes, Nick},
  	urldate = {2020-05-02},
  	date = {2014-11-26},
  	note = {Publisher: {IOP} Publishing},
  	file = {McCarthy et al_2014_Mobility and low contrast trip hazard avoidance using augmented depth.pdf:/Users/jaime/Zotero/storage/46E6FWS3/McCarthy et al_2014_Mobility and low contrast trip hazard avoidance using augmented depth.pdf:application/pdf},
  }

  @article{Schulman2015,
  	title = {Trust region policy optimization},
  	volume = {3},
  	abstract = {In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization ({TRPO}). This algorithm is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, {TRPO} tends to give monotonic improvement, with little tuning of hyperparameters.},
  	pages = {1889--1897},
  	journaltitle = {32nd International Conference on Machine Learning, {ICML} 2015},
  	author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael and Abbeel, Pieter},
  	date = {2015},
  	note = {{ISBN}: 9781510810587},
  	file = {Schulman et al_2015_Trust region policy optimization.pdf:/Users/jaime/Zotero/storage/LDESXW74/Schulman et al_2015_Trust region policy optimization.pdf:application/pdf},
  }

  @article{Ostry2016,
  	title = {Neoliberalism: Oversold? - {IMF}},
  	volume = {53},
  	issn = {1098-6596},
  	abstract = {Instead of delivering growth, some neoliberal policies have increased inequality, in turn jeopardizing durable expansion},
  	pages = {38--41},
  	number = {2},
  	journaltitle = {Imf Finance \& Development},
  	author = {Ostry, Jonathan D and Loungani, Prakash and Furceri, Davide},
  	date = {2016},
  	pmid = {25246403},
  	note = {{ISBN}: 9788578110796},
  }

  @article{Gallup1982,
  	title = {Self‐awareness and the emergence of mind in primates},
  	volume = {2},
  	issn = {10982345},
  	doi = {10.1002/ajp.1350020302},
  	abstract = {To date humans, chimpanzees, and orangutans are the only species which have been shown capable of recognizing themselves in mirrors. Several species of macaques have now been provided with years of continuous exposure to mirrors, but they still persist in reacting to their reflection as if they were seeing other monkeys. Even gibbons (apes) and gorillas (great apes) seem incapable of learning that their behavior is the source of the behavior depicted in the image. Most primates, therefore, appear to lack a cognitive category for processing mirrored information about themselves. The implications of these data for traditional views of consciousness are considered briefly, and a recent attempt to develop an operant analog to self‐recognition is critically evaluated. Finally, an attempt is made to show that self‐awareness, consciousness, and mind are not mutually exclusive cognitive categories and that the emergence of self‐awareness may be equivalent to the emergence of mind. Several indices of “mind” which can be applied to nonhuman species are discussed in the context of an attempt to develop a comparative psychology of mind. Copyright © 1982 Wiley‐Liss, Inc., A Wiley Company},
  	pages = {237--248},
  	number = {3},
  	journaltitle = {American Journal of Primatology},
  	author = {Gallup, Gordon G},
  	date = {1982},
  	keywords = {consciousness, introspection, mind, mirrors, self‐awareness, self‐recognition},
  	file = {Gallup_1982_Self‐awareness and the emergence of mind in primates.pdf:/Users/jaime/Zotero/storage/IXC95KGL/Gallup_1982_Self‐awareness and the emergence of mind in primates.pdf:application/pdf},
  }

  @article{Rohmer2013,
  	title = {V-{REP}: A versatile and scalable robot simulation framework},
  	issn = {21530858},
  	doi = {10.1109/IROS.2013.6696520},
  	abstract = {From exploring planets to cleaning homes, the reach and versatility of robotics is vast. The integration of actuation, sensing and control makes robotics systems powerful, but complicates their simulation. This paper introduces a versatile, scalable, yet powerful general-purpose robot simulation framework called V-{REP}. The paper discusses the utility of a portable and flexible simulation framework that allows for direct incorporation of various control techniques. This renders simulations and simulation models more accessible to a general-public, by reducing the simulation model deployment complexity. It also increases productivity by offering built-in and ready-to-use functionalities, as well as a multitude of programming approaches. This allows for a multitude of applications including rapid algorithm development, system verification, rapid prototyping, and deployment for cases such as safety/remote monitoring, training and education, hardware control, and factory automation simulation. © 2013 {IEEE}.},
  	pages = {1321--1326},
  	journaltitle = {{IEEE} International Conference on Intelligent Robots and Systems},
  	author = {Rohmer, Eric and Singh, Surya P.N. and Freese, Marc},
  	date = {2013},
  	note = {{ISBN}: 9781467363587},
  	file = {Rohmer et al_2013_V-REP.pdf:/Users/jaime/Zotero/storage/59DMX3UW/Rohmer et al_2013_V-REP.pdf:application/pdf},
  }

  @article{Lehman2011,
  	title = {Abandoning objectives: Evolution through the search for novelty alone},
  	volume = {19},
  	issn = {10636560},
  	doi = {10.1162/EVCO_a_00025},
  	abstract = {In evolutionary computation, the fitness function normally measures progress toward an objective in the search space, effectively acting as an objective function. Through deception, such objective functions may actually prevent the objective from being reached. While methods exist to mitigate deception, they leave the underlying pathology untreated: Objective functions themselves may actively misdirect search toward dead ends. This paper proposes an approach to circumventing deception that also yields a new perspective on open-ended evolution. Instead of either explicitly seeking an objective or modeling natural evolution to capture open-endedness, the idea is to simply search for behavioral novelty. Even in an objective-based problem, such novelty search ignores the objective. Because many points in the search space collapse to a single behavior, the search for novelty is often feasible. Furthermore, because there are only so many simple behaviors, the search for novelty leads to increasing complexity. By decoupling open-ended search from artificial life worlds, the search for novelty is applicable to real world problems. Counterintuitively, in the maze navigation and biped walking tasks in this paper, novelty search significantly outperforms objectivebased search, suggesting the strange conclusion that some problems are best solved by methods that ignore the objective. The main lesson is the inherent limitation of the objective-based paradigm and the unexploited opportunity to guide search through other means. © 2011 by the Massachusetts Institute of Technology.},
  	pages = {189--222},
  	number = {2},
  	journaltitle = {Evolutionary Computation},
  	author = {Lehman, Joel and Stanley, Kenneth O.},
  	date = {2011},
  	pmid = {20868264},
  	keywords = {Deception, Evolutionary algorithms, Neuroevolution, Novelty search, Open-ended evolution},
  	file = {Lehman_Stanley_2011_Abandoning objectives.pdf:/Users/jaime/Zotero/storage/2PBCS8D4/Lehman_Stanley_2011_Abandoning objectives.pdf:application/pdf},
  }

  @article{Lozano2003,
  	title = {Towards reactive navigation and attention skills for 3D intelligent characters},
  	volume = {2687},
  	issn = {16113349},
  	doi = {10.1007/3-540-44869-1_27},
  	abstract = {This paper presents a neural design which is able to provide the necessary reactive navigation and attention skills for 3D embodied agents (virtual humanoids or characters). Based on Grossbergs neural model of conditioning [6], as recently implemented by Chang and Gaudiando [7], and according to the Adaptative Resonance Theory ({ART}) and the neuroscientific concepts associated, the neural design introduced has been divided in two main phases. Firstly, an environmentcategorization phase, where an on-line pattern recognition and categorization of the current agent sensory input data is carried out by a self organizing neural network, which will finally provide the agents short term memory layer({STM}). Secondly, and based on the classical conditioning paradigm, the model will associate the interesting {STM} states, from the navigation or attention points of view, to finally simulate these necessary skills for 3D characters or humanoids. Finally, we will show some experimental navigational results, through the integration of the model presented in 3D virtual environments. © Springer-Verlag Berlin Heidelberg 2003.},
  	pages = {209--216},
  	journaltitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  	author = {Lozano, Miguel and Grimaldo, Francisco and Villaplana, Javier},
  	date = {2003},
  	note = {{ISBN}: 354040211X},
  	file = {Lozano et al_2003_Towards reactive navigation and attention skills for 3D intelligent characters.pdf:/Users/jaime/Zotero/storage/ZQXXYHF7/Lozano et al_2003_Towards reactive navigation and attention skills for 3D intelligent characters.pdf:application/pdf},
  }

  @article{Yang2018,
  	title = {Train and equip firefighters with cognitive virtual and augmented reality},
  	doi = {10.1109/CIC.2018.00068},
  	abstract = {It is important to reduce loss caused by fires through improved operations performed by virtual and augmented reality ({VR}/{AR}) trained and equipped fire-fighters. This paper collaborates with firefighting professionals to training firefighting skills with {VR}/{AR} systems. The system is also integrated with computational models and decision tools to provide situational awareness and address challenges faced by firefighters on the fire ground.},
  	pages = {453--459},
  	journaltitle = {Proceedings - 4th {IEEE} International Conference on Collaboration and Internet Computing, {CIC} 2018},
  	author = {Yang, Li and Liang, Yu and Wu, Dalei and Gault, Jim},
  	date = {2018},
  	note = {Publisher: {IEEE}
  {ISBN}: 9781538695029},
  	keywords = {Fire dynamic simulation, Firefighting, Virtual and augmented reality},
  	file = {Yang et al_2018_Train and equip firefighters with cognitive virtual and augmented reality.pdf:/Users/jaime/Zotero/storage/GGIYMDIR/Yang et al_2018_Train and equip firefighters with cognitive virtual and augmented reality.pdf:application/pdf},
  }

  @article{Rambach2020,
  	title = {A survey on applications of augmented, mixed and virtual reality for nature and environment},
  	url = {http://arxiv.org/abs/2008.12024},
  	abstract = {Augmented reality ({AR}), virtual reality ({VR}) and mixed reality ({MR}) are technologies of great potential due to the engaging and enriching experiences they are capable of providing. Their use is rapidly increasing in diverse fields such as medicine, manufacturing or entertainment. However, the possibilities that {AR}, {VR} and {MR} offer in the area of environmental applications are not yet widely explored. In this paper we present the outcome of a survey meant to discover and classify existing {AR}/{VR}/{MR} applications that can benefit the environment or increase awareness on environmental issues. We performed an exhaustive search over several online publication access platforms and past proceedings of major conferences in the fields of {AR}/{VR}/{MR}. Identified relevant papers were filtered based on novelty, technical soundness, impact and topic relevance, and classified into different categories. Referring to the selected papers, we discuss how the applications of each category are contributing to environmental protection, preservation and sensitization purposes. We further analyse these approaches as well as possible future directions in the scope of existing and upcoming {AR}/{VR}/{MR} enabling technologies.},
  	pages = {1--20},
  	author = {Rambach, Jason and Lilligreen, Gergana and Schäfer, Alexander and Bankanal, Ramya and Wiebel, Alexander and Stricker, Didier},
  	date = {2020},
  	keywords = {ar, augmented reality, ecology, environment, mixed reality, mr, nature, virtual reality, vr},
  	file = {Rambach et al_2020_A survey on applications of augmented, mixed and virtual reality for nature and.pdf:/Users/jaime/Zotero/storage/DCXIJ9DY/Rambach et al_2020_A survey on applications of augmented, mixed and virtual reality for nature and.pdf:application/pdf},
  }

  @article{Nishida2016,
  	title = {Population size adaptation for the {CMA}-{ES} based on the estimation accuracy of the natural gradient},
  	doi = {10.1145/2908812.2908864},
  	abstract = {We propose a novel strategy to adapt the population size, i.e. the number of candidate solutions per iteration, for the rank-μ update covariance matrix adaptation evolution strategy ({CMA}-{ES}). Our strategy is based on the interpretation of the rank-μ update {CMA}-{ES} as the stochastic natural gradient approach on the parameter space of the sampling distribution. We introduce a measurement of the accuracy of the current estimate of the natural gradient. We propose a novel strategy to adapt the population size according to the accuracy measure. The proposed strategy is evaluated on test functions including rugged functions and noisy functions where a larger population size is known to help to find a better solution. The experimental results show the advantage of the adaptation of the population size over a fixed population size. It is also compared with the state-of-the-art uncertainty handling strategy for the {CMA}-{ES}, namely {UH}-{CMA}-{ES}, on noisy test functions.},
  	pages = {237--244},
  	journaltitle = {{GECCO} 2016 - Proceedings of the 2016 Genetic and Evolutionary Computation Conference},
  	author = {Nishida, Kouhei and Akimoto, Youhei},
  	date = {2016},
  	note = {{ISBN}: 9781450342063},
  	keywords = {Covariance matrix adaptation, Natural gradient, Noisy optimization, Population size adaptation, Ruggedness},
  	file = {Nishida_Akimoto_2016_Population size adaptation for the CMA-ES based on the estimation accuracy of.pdf:/Users/jaime/Zotero/storage/437ZJWLG/Nishida_Akimoto_2016_Population size adaptation for the CMA-ES based on the estimation accuracy of.pdf:application/pdf},
  }

  @article{Henderson2018a,
  	title = {Meaning guides attention in real-world scene images: Evidence from eye movements and meaning maps},
  	volume = {18},
  	issn = {15347362},
  	doi = {10.1167/18.6.10},
  	abstract = {We compared the influence of meaning and of salience on attentional guidance in scene images. Meaning was captured by ''meaning maps'' representing the spatial distribution of semantic information in scenes. Meaning maps were coded in a format that could be directly compared to maps of image salience generated from image features. We investigated the degree to which meaning versus image salience predicted human viewers' spatiotemporal distribution of attention over scenes. Extending previous work, here the distribution of attention was operationalized as duration-weighted fixation density. The results showed that both meaning and image salience predicted the duration-weighted distribution of attention, but that when the correlation between meaning and salience was statistically controlled, meaning accounted for unique variance in attention whereas salience did not. This pattern was observed in early as well as late fixations, fixations including and excluding the centers of the scenes, and fixations following short as well as long saccades. The results strongly suggest that meaning guides attention in real-world scenes. We discuss the results from the perspective of a cognitive-relevance theory of attentional guidance.},
  	pages = {1--18},
  	number = {6},
  	journaltitle = {Journal of Vision},
  	author = {Henderson, John M. and Hayes, Taylor R.},
  	date = {2018},
  	pmid = {30029216},
  	keywords = {Eye movements, Attention, Scene perception},
  }

  @article{Carmigniani2011,
  	title = {Augmented reality technologies, systems and applications},
  	volume = {51},
  	issn = {13807501},
  	doi = {10.1007/s11042-010-0660-6},
  	abstract = {This paper surveys the current state-of-the-art of technology, systems and applications in Augmented Reality. It describes work performed by many different research groups, the purpose behind each new Augmented Reality system, and the difficulties and problems encountered when building some Augmented Reality applications. It surveys mobile augmented reality systems challenges and requirements for successful mobile systems. This paper summarizes the current applications of Augmented Reality and speculates on future applications and where current research will lead Augmented Reality's development. Challenges augmented reality is facing in each of these applications to go from the laboratories to the industry, as well as the future challenges we can forecast are also discussed in this paper. Section 1 gives an introduction to what Augmented Reality is and the motivations for developing this technology. Section 2 discusses Augmented Reality Technologies with computer vision methods, {AR} devices, interfaces and systems, and visualization tools. The mobile and wireless systems for Augmented Reality are discussed in Section 3. Four classes of current applications that have been explored are described in Section 4. These applications were chosen as they are the most famous type of applications encountered when researching {AR} apps. The future of augmented reality and the challenges they will be facing are discussed in Section 5. © 2010 Springer Science+Business Media, {LLC}.},
  	pages = {341--377},
  	number = {1},
  	journaltitle = {Multimedia Tools and Applications},
  	author = {Carmigniani, Julie and Furht, Borko and Anisetti, Marco and Ceravolo, Paolo and Damiani, Ernesto and Ivkovic, Misa},
  	date = {2011},
  	keywords = {{AR}, Augmented reality, Augmented reality applications, Augmented reality iphone4, Augmented reality on mobile devices, Augmented reality systems, Augmented reality technologies},
  	file = {Carmigniani et al_2011_Augmented reality technologies, systems and applications.pdf:/Users/jaime/Zotero/storage/DIDQGD3B/Carmigniani et al_2011_Augmented reality technologies, systems and applications.pdf:application/pdf},
  }

  @article{Hausknecht2014,
  	title = {A neuroevolution approach to general atari game playing},
  	volume = {6},
  	issn = {1943068X},
  	doi = {10.1109/TCIAIG.2013.2294713},
  	abstract = {This paper addresses the challenge of learning to play many different video games with little domain-specific knowledge. Specifically, it introduces a neuroevolution approach to general Atari 2600 game playing. Four neuroevolution algorithms were paired with three different state representations and evaluated on a set of 61 Atari games. The neuroevolution agents represent different points along the spectrum of algorithmic sophistication - including weight evolution on topologically fixed neural networks (conventional neuroevolution), covariance matrix adaptation evolution strategy ({CMA}-{ES}), neuroevolution of augmenting topologies ({NEAT}), and indirect network encoding ({HyperNEAT}). State representations include an object representation of the game screen, the raw pixels of the game screen, and seeded noise (a comparative baseline). Results indicate that direct-encoding methods work best on compact state representations while indirect-encoding methods (i.e., {HyperNEAT}) allow scaling to higher dimensional representations (i.e., the raw game screen). Previous approaches based on temporal-difference ({TD}) learning had trouble dealing with the large state spaces and sparse reward gradients often found in Atari games. Neuroevolution ameliorates these problems and evolved policies achieve state-of-the-art results, even surpassing human high scores on three games. These results suggest that neuroevolution is a promising approach to general video game playing ({GVGP}).},
  	pages = {355--366},
  	number = {4},
  	journaltitle = {{IEEE} Transactions on Computational Intelligence and {AI} in Games},
  	author = {Hausknecht, Matthew and Lehman, Joel and Miikkulainen, Risto and Stone, Peter},
  	date = {2014},
  	keywords = {Algorithms, artificial neural networks, evolutionary computation, genetic algorithms, neural networks},
  	file = {Hausknecht et al_2014_A neuroevolution approach to general atari game playing.pdf:/Users/jaime/Zotero/storage/GPGHJKZE/Hausknecht et al_2014_A neuroevolution approach to general atari game playing.pdf:application/pdf},
  }

  @inproceedings{Schmidhuber2013,
  	title = {Evolving large-scale neural networks for vision-based reinforcement learning},
  	url = {http://people.idsia.ch/~juergen/compressednetworksearch.html},
  	pages = {1061--1068},
  	booktitle = {Proceedings of the 15th annual conference on Genetic and evolutionary computation},
  	author = {Schmidhuber, Jürgen and Koutník, Jan and Cuccu, Giuseppe and Gomez, Faustino},
  	date = {2013},
  }

  @article{Stanley2009,
  	title = {A hypercube-based encoding for evolving large-scale neural networks},
  	volume = {2},
  	url = {http://eplex.cs.ucf.edu/hyperNEATpage/},
  	pages = {182--212},
  	number = {15},
  	journaltitle = {Artificial Life},
  	author = {Stanley, Kenneth O. and D'Ambrosio, David B. and Gauci, Jason},
  	date = {2009},
  }

  @article{Stanley2007,
  	title = {Compositional pattern producing networks: A novel abstraction of development},
  	volume = {2},
  	pages = {131--162},
  	number = {8},
  	journaltitle = {Genetic programming and evolvable machines},
  	author = {Stanley, Kenneth O.},
  	date = {2007},
  }

  @article{Li2019,
  	title = {Graph Attention Memory for Visual Navigation},
  	url = {http://arxiv.org/abs/1905.13315},
  	abstract = {Visual navigation in complex environments is inefficient with traditional reactive policy or general-purposed recurrent policy. To address the long-term memory issue, this paper proposes a graph attention memory ({GAM}) architecture consisting of memory construction module, graph attention module and control module. The memory construction module builds the topological graph based on supervised learning by taking the exploration prior. Then, guided attention features are extracted with the graph attention module. Finally, the deep reinforcement learning based control module makes decisions based on visual observations and guided attention features. Detailed convergence analysis of {GAM} is presented in this paper. We evaluate {GAM}-based navigation system in two complex 3D environments. Experimental results show that the {GAM}-based navigation system significantly improves learning efficiency and outperforms all baselines in average success rate.},
  	author = {Li, Dong and Zhang, Qichao and Zhao, Dongbin and Zhuang, Yuzheng and Wang, Bin and Liu, Wulong and Tutunov, Rasul and Wang, Jun},
  	date = {2019},
  	file = {Li et al_2019_Graph Attention Memory for Visual Navigation.pdf:/Users/jaime/Zotero/storage/U6D6PJ32/Li et al_2019_Graph Attention Memory for Visual Navigation.pdf:application/pdf},
  }

  @article{Brunner2018,
  	title = {Teaching a machine to read maps with deep reinforcement learning},
  	abstract = {The ability to use a 2D map to navigate a complex 3D environment is quite remarkable, and even difficult for many humans. Localization and navigation is also an important problem in domains such as robotics, and has recently become a focus of the deep reinforcement learning community. In this paper we teach a reinforcement learning agent to read a map in order to find the shortest way out of a random maze it has never seen before. Our system combines several state-of-the-art methods such as A3C and incorporates novel elements such as a recurrent localization cell. Our agent learns to localize itself based on 3D first person images and an approximate orientation angle. The agent generalizes well to bigger mazes, showing that it learned useful localization and navigation capabilities.},
  	pages = {2763--2770},
  	issue = {Minsky},
  	journaltitle = {32nd {AAAI} Conference on Artificial Intelligence, {AAAI} 2018},
  	author = {Brunner, Gino and Richter, Oliver and Wang, Yuyi and Wattenhofer, Roger},
  	date = {2018},
  	note = {{ISBN}: 9781577358008},
  	file = {Brunner et al_2018_Teaching a machine to read maps with deep reinforcement learning.pdf:/Users/jaime/Zotero/storage/54HVS54M/Brunner et al_2018_Teaching a machine to read maps with deep reinforcement learning.pdf:application/pdf},
  }

  @article{Dhiman2018,
  	title = {A critical investigation of deep reinforcement learning for navigation},
  	abstract = {The navigation problem is classically approached in two steps: an exploration step, where map-information about the environment is gathered; and an exploitation step, where this information is used to navigate efficiently. Deep reinforcement learning ({DRL}) algorithms, alternatively, approach the problem of navigation in an end-to-end fashion. Inspired by the classical approach, we ask whether {DRL} algorithms are able to inherently explore, gather and exploit map-information over the course of navigation. We build upon Mirowski et al. [2017]’s work and introduce a systematic suite of experiments that vary three parameters: the agent’s starting location, the agent’s target location, and the maze structure. We choose evaluation metrics that explicitly measure the algorithm’s ability to gather and exploit map-information. Our experiments show that when trained and tested on the same maps, the algorithm successfully gathers and exploits map-information. However, when trained and tested on different sets of maps, the algorithm fails to transfer the ability to gather and exploit map-information to unseen maps. Furthermore, we find that when the goal location is randomized and the map is kept static, the algorithm is able to gather and exploit map-information but the exploitation is far from optimal. We open-source our experimental suite in the hopes that it serves as a framework for the comparison of future algorithms and leads to the discovery of robust alternatives to classical navigation methods.},
  	journaltitle = {{arXiv}},
  	author = {Dhiman, Vikas and Banerjee, Shurjo and Griffin, Brent and Siskind, Jeffrey M. and Corso, Jason J.},
  	date = {2018},
  	file = {Dhiman et al_2018_A critical investigation of deep reinforcement learning for navigation.pdf:/Users/jaime/Zotero/storage/WPBX2HXX/Dhiman et al_2018_A critical investigation of deep reinforcement learning for navigation.pdf:application/pdf},
  }

  @article{Chaplot2016,
  	title = {Transfer Deep Reinforcement Learning in 3D Environments: An Empirical Study},
  	abstract = {The ability to transfer knowledge from previous experiences is critical for an agent to rapidly adapt to different environments and effectively learn new tasks. In this paper we conduct an empirical study of Deep Q-Networks ({DQNs}) where the agent is evaluated on previously unseen environments. We show that we can train a robust network for navigation in 3D environments and demonstrate its effectiveness in generalizing to unknown maps with unknown background textures. We further investigate the effectiveness of pretraining and finetuning for transferring knowledge between various scenarios in 3D environments. In particular, we show that the features learnt by the navigation network can be effectively utilized to transfer knowledge between a diverse set of tasks, such as object collection, deathmatch, and self-localization.},
  	issue = {Nips},
  	author = {Chaplot, D. S. and Lample, G. and Sathyendra, K. M. and Salakhutdinov, R.},
  	date = {2016},
  	file = {Chaplot et al_2016_Transfer Deep Reinforcement Learning in 3D Environments.pdf:/Users/jaime/Zotero/storage/AL9TEQ5L/Chaplot et al_2016_Transfer Deep Reinforcement Learning in 3D Environments.pdf:application/pdf},
  }

  @article{Disu2020,
  	title = {Short-range robotic navigation and exploration tasks via deep q-networks for biomedical applications},
  	doi = {10.1109/IAICT50021.2020.9172019},
  	abstract = {This research is focused on the performance of a Deep Reinforcement Learning method on an agent (mobile robot) in a simulated virtual environment (Operating Room) for medical applications. The purpose of this research is to compare suitable decisive actions taken by the agent to achieve its goal target. Executing this goal requires the implementation of a reward-penalty system for observation and analysis. The agent's accumulated reward is based on the best-navigated decision to avoid collisions; solely generating an intelligent agent system. We reviewed previous works on the impact of Deep Reinforcement Learning algorithms on an agent in areas of navigation and exploration. Adopting a Deep Reinforcement Learning method and a physical simulator, we trained and tested the agent using existing environments and our modeled operating room, respectively. Measuring the positive reward output of the experiment with different parameters of the algorithm such as the learning rate, maximum Q-value and the average time to attain its goal position, we presented our work with plots of the experiment and compared it with a widely known traditional method. Our experimental results indicated that the agent achieved a high positive reward of 3800 in our operating room environment with a learning rate of 0.5. Our research aimed at training an agent to make intelligent decisions in achieving its goal destination without prior experience and input data. Reinforcement Learning provides a structure for robotics to function effectively; utilizing and engaging a robot to navigate and explore in any given environment.},
  	pages = {35--41},
  	journaltitle = {Proceedings - 2020 {IEEE} International Conference on Industry 4.0, Artificial Intelligence, and Communications Technology, {IAICT} 2020},
  	author = {Disu, Joel Dzidzorvi Kwame and Gandana, Clinton Elian and Xie, Hongzhi and Gu, Lixu},
  	date = {2020},
  	note = {{ISBN}: 9781728193366},
  	keywords = {Navigation, Reinforcement Learning, Biomedical application, Deep Q-Networks, Exploration, Operating room, {SLAM}},
  	file = {Disu et al_2020_Short-range robotic navigation and exploration tasks via deep q-networks for.pdf:/Users/jaime/Zotero/storage/M5JVZGTH/Disu et al_2020_Short-range robotic navigation and exploration tasks via deep q-networks for.pdf:application/pdf},
  }

  @article{Wang2018,
  	title = {Deep reinforcement learning by parallelizing reward and punishment using the maxpain architecture},
  	doi = {10.1109/DEVLRN.2018.8761044},
  	abstract = {Traditionally, reinforcement learning treats punishments as negative rewards. However, in biological decision systems, some evidence shows that animals have separate systems for rewards and punishments. The {MaxPain} architecture parallelizes the predictions of rewards and punishments and scales them into dual-attribute policies, and has been shown to both improve the learning speed and the learning of safer behaviors. This paper extends the {MaxPain} architecture into a deep reinforcement learning framework using convolutional neural networks to approximate two action-value functions. To derive the behavioral policy, we consider the mixture distributions of the policies computed from the two action-value functions. For evaluation, we compare the {MaxPain} architecture with count-based exploration and a reward-decomposing structure called Hybrid Reward Architecture ({HRA}) in grid-world navigation and vision-based navigation in a U-shape maze in the Gazebo robot simulation environment. The simulation results show the superiority of the {MaxPain} approach over the count-based method because the {MaxPain} agents efficiently avoid dead-end states by predicting future punishments. In addition, the {MaxPain} agents learn safe behaviors, while the {HRA} agents learn similar behaviors, as in the case of no punishments.},
  	pages = {175--180},
  	journaltitle = {2018 Joint {IEEE} 8th International Conference on Development and Learning and Epigenetic Robotics, {ICDL}-{EpiRob} 2018},
  	author = {Wang, Jiexin and Elfwing, Stefan and Uchibe, Eiji},
  	date = {2018},
  	note = {Publisher: {IEEE}
  {ISBN}: 9781538661109},
  	file = {Wang et al_2018_Deep reinforcement learning by parallelizing reward and punishment using the.pdf:/Users/jaime/Zotero/storage/3KNKURTB/Wang et al_2018_Deep reinforcement learning by parallelizing reward and punishment using the.pdf:application/pdf},
  }

  @book{Graesser2018,
  	title = {Intelligent tutoring systems},
  	isbn = {978-1-317-20836-5},
  	abstract = {Intelligent tutoring systems ({ITS}) are computer learning environments that help students master knowledge and skills by implementing intelligent algorithms that adapt to students at a fine-grained level and that instantiate complex principles of learning. An {ITS} normally works with one student at a time because students differ on many dimensions and the goal is to be sensitive to the idiosyncrasies of individual learners. {ITS} have been developed for mathematics and other computationally well-formed topics as well as knowledge domains that have a verbal foundation. Reviews and quantitative meta-analyses confirm that {ITS} technologies frequently improve learning over reading text and traditional teacher-directed classroom teaching. This chapter describes affordances that are frequently incorporated in most applications. Some affordances are routinely incorporate in {ITS} (active student learning, interactivity, adaptivity, and feedback) whereas others are frequently but not always included (choice, non-linear access to topics, linked representations, and open-ended learner input). The Generalized Intelligent Framework for Tutoring ({GIFT}) is a framework that articulates the frequent practices, pedagogical and technical standards, and computational architectures for developing {ITS}; the goal of the {GIFT} initiative is to scale up {ITS} development for schools, the military, industry, and the public. The chapter also identifies major challenges in building {ITS} and some of their limitations.},
  	pagetotal = {246-255},
  	author = {Graesser, Arthur C. and Hu, Xiangen and Sottilare, Robert},
  	date = {2018},
  	doi = {10.4324/9781315617572},
  	note = {Publication Title: International Handbook of the Learning Sciences},
  }

  @article{Kamran2020,
  	title = {Risk-Aware High-level Decisions for Automated Driving at Occluded Intersections with Reinforcement Learning},
  	abstract = {Reinforcement learning is nowadays a popular framework for solving different decision making problems in automated driving. However, there are still some remaining crucial challenges that need to be addressed for providing more reliable policies. In this paper, we propose a generic risk-aware {DQN} approach in order to learn high level actions for driving through unsignalized occluded intersections. The proposed state representation provides lane based information which allows to be used for multi-lane scenarios. Moreover, we propose a risk based reward function which punishes risky situations instead of only collision failures. Such rewarding approach helps to incorporate risk prediction into our deep Q network and learn more reliable policies which are safer in challenging situations. The efficiency of the proposed approach is compared with a {DQN} learned with conventional collision based rewarding scheme and also with a rule-based intersection navigation policy. Evaluation results show that the proposed approach outperforms both of these methods. It provides safer actions than collision-aware {DQN} approach and is less overcautious than the rule-based policy.},
  	journaltitle = {{arXiv}},
  	author = {Kamran, Danial and Lopez, Carlos Fernandez and Lauer, Martin and Stiller, Christoph},
  	date = {2020},
  	file = {Kamran et al_2020_Risk-Aware High-level Decisions for Automated Driving at Occluded Intersections.pdf:/Users/jaime/Zotero/storage/26IW29GK/Kamran et al_2020_Risk-Aware High-level Decisions for Automated Driving at Occluded Intersections.pdf:application/pdf},
  }

  @article{Bouhamed2020,
  	title = {A {DDPG}-based Approach for Energy-aware {UAV} Navigation in Obstacle-constrained Environment},
  	doi = {10.1109/WF-IoT48130.2020.9221115},
  	abstract = {In this paper, we propose a three-dimensional autonomous {UAV} navigation framework using Deep Deterministic Policy Gradient ({DDPG}) learning approach. The objective is to employ a self-trained {UAV} as an airborne Internet of Things ({IoT}) unit to navigate obstacles and reach a destination point, where it can communicate with a ground sensor node with sufficiently high data rate. We develop a customized reward function which aims to minimize the distance separating the {UAV} and its destination while penalizing collisions. A dynamic energy threshold is also set to redirect the {UAV} towards the charging station in case of battery depletion. We numerically simulate the behavior of the {UAV} when learning the environmental obstacles, and autonomously selecting trajectories for selected scenarios. Finally, we show that our learning approach achieves close performance to the one of the graph-based Dijkstra's algorithm.},
  	journaltitle = {{IEEE} World Forum on Internet of Things, {WF}-{IoT} 2020 - Symposium Proceedings},
  	author = {Bouhamed, Omar and Wan, Xiangpeng and Ghazzai, Hakim and Massoud, Yehia},
  	date = {2020},
  	note = {{ISBN}: 9781728155036},
  	keywords = {Autonomous navigation, deep reinforcement learning, internet of things, obstacle avoidance, unmanned aerial vehicle},
  	file = {Bouhamed et al_2020_A DDPG-based Approach for Energy-aware UAV Navigation in Obstacle-constrained.pdf:/Users/jaime/Zotero/storage/6T3Q7M8U/Bouhamed et al_2020_A DDPG-based Approach for Energy-aware UAV Navigation in Obstacle-constrained.pdf:application/pdf},
  }

  @article{Park2020,
  	title = {Reinforcement learning with distance-based incentive/penalty ({DIP}) updates for highly constrained industrial control systems},
  	url = {http://arxiv.org/abs/2011.10897},
  	abstract = {Typical reinforcement learning ({RL}) methods show limited applicability for real-world industrial control problems because industrial systems involve various constraints and simultaneously require continuous and discrete control. To overcome these challenges, we devise a novel {RL} algorithm that enables an agent to handle a highly constrained action space. This algorithm has two main features. First, we devise two distance-based Q-value update schemes, incentive update and penalty update, in a distance-based incentive/penalty update technique to enable the agent to decide discrete and continuous actions in the feasible region and to update the value of these types of actions. Second, we propose a method for defining the penalty cost as a shadow price-weighted penalty. This approach affords two advantages compared to previous methods to efficiently induce the agent to not select an infeasible action. We apply our algorithm to an industrial control problem, microgrid system operation, and the experimental results demonstrate its superiority.},
  	pages = {1--8},
  	author = {Park, Hyungjun and Min, Daiki and Ryu, Jong-hyun and Choi, Dong Gu},
  	date = {2020},
  	file = {Park et al_2020_Reinforcement learning with distance-based incentive-penalty (DIP) updates for.pdf:/Users/jaime/Zotero/storage/V2GABV7Z/Park et al_2020_Reinforcement learning with distance-based incentive-penalty (DIP) updates for.pdf:application/pdf},
  }

  @article{Zhu2020,
  	title = {Transfer Learning in Deep Reinforcement Learning: A Survey},
  	url = {http://arxiv.org/abs/2009.07888},
  	abstract = {This paper surveys the field of transfer learning in the problem setting of Reinforcement Learning ({RL}). {RL} has been a key solution to sequential decision-making problems. Along with the fast advances of {RL} in various domains, such as robotics and game-playing, transfer learning arises as an important technique to assist {RL} by leveraging and transferring external expertise to boost the learning process of {RL}. In this survey, we review the central issues of transfer learning in the {RL} domain, providing a systematic categorization of its state-of-the-art techniques. We analyze their goals, methodologies, applications, and the {RL} frameworks under which the transfer learning techniques are approachable. We discuss the relationship between transfer learning and other relevant topics from the {RL} perspective and also explore the potential challenges as well as future development directions for transfer learning in {RL}.},
  	pages = {1--22},
  	author = {Zhu, Zhuangdi and Lin, Kaixiang and Zhou, Jiayu},
  	date = {2020},
  	file = {Zhu et al_2020_Transfer Learning in Deep Reinforcement Learning.pdf:/Users/jaime/Zotero/storage/JZFMH2IA/Zhu et al_2020_Transfer Learning in Deep Reinforcement Learning.pdf:application/pdf},
  }

  @article{Cetin2019,
  	title = {Drone Navigation and Avoidance of Obstacles Through Deep Reinforcement Learning},
  	volume = {2019-Septe},
  	issn = {21557209},
  	doi = {10.1109/DASC43569.2019.9081749},
  	abstract = {Unmanned aerial vehicles ({UAV}) specifically drones have been used for surveillance, shipping and delivery, wildlife monitoring, disaster management etc. The increase on the number of drones in the airspace worldwide will lead necessarily to full autonomous drones. Given the expected huge number of drones, if they were operated by human pilots, the possibility to collide with each other could be too high. In this paper, deep reinforcement learning ({DRL}) architecture is proposed to make drones behave autonomously inside a suburb neighborhood environment. The environment in the simulator has plenty of obstacles such as trees, cables, parked cars and houses. In addition, there are also another drones, acting as moving obstacles, inside the environment while the learner drone has a goal to achieve. In this way the drone can be trained to detect stationary and moving obstacles inside the neighborhood and so the drones can be used safely in a public area in the future. The drone has a front camera and it can capture continuously depth images. Every depth image is part of the state used in {DRL} architecture. Also, another part of the state is the distance to the geo-fence (a virtual barrier on the environment) which is added as a scalar value. The agent will be rewarded negatively when it tries to overpass the geo-fence limits. In addition, angle to goal and elevation angle between the goal and the drone will be used as information to be added to the state. It is considered that these scalar values will improve the {DRL} performance and also the reward obtained. The drone is trained using Q-Network and its convergence and final reward are evaluated. The states containing image and several scalars are processed by a neural network that joints the two state parts into a unique flow. This neural network is named as Joint Neural Network ({JNN}) [1]. The training and test results show that the agent can successfully learn to avoid any obstacle in the environment. The results for three scenarios are very promising and the learner drone reaches the destination with a success rate 100\% in first two tests and with a success rate 98\% in the last test, this one with a total of three drones.},
  	journaltitle = {{AIAA}/{IEEE} Digital Avionics Systems Conference - Proceedings},
  	author = {Cetin, Ender and Barrado, Cristina and Munoz, Guillem and {MacIas}, Miquel and Pastor, Enric},
  	date = {2019},
  	note = {{ISBN}: 9781728106496},
  	keywords = {{DDQN}, Deep Reinforcement Learning, Drones, {JNN}, Q-Network, {UAV}},
  	file = {Cetin et al_2019_Drone Navigation and Avoidance of Obstacles Through Deep Reinforcement Learning.pdf:/Users/jaime/Zotero/storage/54BKJVII/Cetin et al_2019_Drone Navigation and Avoidance of Obstacles Through Deep Reinforcement Learning.pdf:application/pdf},
  }

  @article{Jin2020,
  	title = {Learning Principle of Least Action with Reinforcement Learning},
  	url = {http://arxiv.org/abs/2011.11891},
  	abstract = {Nature provides a way to understand physics with reinforcement learning since nature favors the economical way for an object to propagate. In the case of classical mechanics, nature favors the object to move along the path according to the integral of the Lagrangian, called the action \${\textbackslash}mathcal\{S\}\$. We consider setting the reward/penalty as a function of \${\textbackslash}mathcal\{S\}\$, so the agent could learn the physical trajectory of particles in various kinds of environments with reinforcement learning. In this work, we verified the idea by using a Q-Learning based algorithm on learning how light propagates in materials with different refraction indices, and show that the agent could recover the minimal-time path equivalent to the solution obtained by Snell's law or Fermat's Principle. We also discuss the similarity of our reinforcement learning approach to the path integral formalism.},
  	pages = {1--5},
  	author = {Jin, Zehao and Lin, Joshua Yao-Yu and Li, Siao-Fong},
  	date = {2020},
  	file = {Jin et al_2020_Learning Principle of Least Action with Reinforcement Learning.pdf:/Users/jaime/Zotero/storage/W83GM67I/Jin et al_2020_Learning Principle of Least Action with Reinforcement Learning.pdf:application/pdf},
  }

  @article{Chen2019,
  	title = {A knowledge-free path planning approach for smart ships based on reinforcement learning},
  	volume = {189},
  	issn = {00298018},
  	url = {https://doi.org/10.1016/j.oceaneng.2019.106299},
  	doi = {10.1016/j.oceaneng.2019.106299},
  	abstract = {The autonomous navigation of smart ships needs to meet their huge inertia and obey existing complex rules. A smart ship has to realise autonomous driving instead of manual operation, which consists of path planning and controlling. Toward to this goal, this research proposes a path planning and manipulating approach based on Q-learning, which can drive a cargo ship by itself without requiring any input from human experiences. At the very beginning, a ship is modelled with the Nomoto model in a simulation waterway. Then, distances, obstacles and prohibited areas are regularized as rewards or punishments, which are used to judge the performance, or manipulation decisions of the ship. Subsequently, Q-learning is introduced to learn the action–reward model and the learning outcome is used to manipulate the ship's movement. By chasing higher reward values, the ship can find an appropriate path or navigation strategies by itself. After a sufficient number of rounds of training, a convincing path and manipulating strategies will likely be produced. By comparing the proposed approach with the existing methods, it is shown that this approach is more effective in self-learning and continuous optimisation, and therefore closer to human manoeuvring.},
  	pages = {106299},
  	issue = {June},
  	journaltitle = {Ocean Engineering},
  	author = {Chen, Chen and Chen, Xian Qiao and Ma, Feng and Zeng, Xiao Jun and Wang, Jin},
  	date = {2019},
  	note = {Publisher: Elsevier Ltd},
  	keywords = {Path planning, Q-learning, Rewards, Smart ships, Value function},
  	file = {Chen et al_2019_A knowledge-free path planning approach for smart ships based on reinforcement.pdf:/Users/jaime/Zotero/storage/K23VMSL9/Chen et al_2019_A knowledge-free path planning approach for smart ships based on reinforcement.pdf:application/pdf},
  }

  @article{DaSilva2020,
  	title = {Q-Learning Applied to Soft-Kill Countermeasures for Unmanned Aerial Vehicles ({UAVs})},
  	doi = {10.1109/PLANS46316.2020.9110222},
  	abstract = {This work presents a three-dimensional control algorithm using reinforcement learning to guide an attacking hunter drone capable of performing a global navigation satellite systems ({GNSS}) repeater attack on the {GNSS} receiver of a target invader drone. Considering the mission and movement requirements of the hunter drone, a Q-learning algorithm was developed, for which the table with the possible transitions of the states and actions is obtained by the actions that the vehicle can take considering directions and the respective consequences of each action. The learning capability of the proposed algorithm arises from the trial and error by an agent. The penalty calculation is based on the error of the invader position to the hunter's desired position of the attacked drone. The developed algorithm is tested using a software-in-the-loop ({SITL}) implementation, which is based on the Ardupilot platform. {SITL} simulations are performed in a developed testbed to emulate operational scenarios, where an unmanned aerial vehicle ({UAV}) is hijacked and then controlled by an attacking {UAV} until it reaches the final position desired by the hunter, usually a secure area where the vehicle can be captured without being destroyed. Results, including error metrics and action time, are discussed for different mission scenarios.},
  	pages = {91--99},
  	journaltitle = {2020 {IEEE}/{ION} Position, Location and Navigation Symposium, {PLANS} 2020},
  	author = {Da Silva, Douglas L. and Antreich, Felix and Coutinho, Olympio L. and {MacHado}, Renato},
  	date = {2020},
  	note = {{ISBN}: 9781728102443},
  	keywords = {Ardupilot, Control Algorithm, Q-Learning, Reinforcement Learning, Software-in-the-loop},
  	file = {Da Silva et al_2020_Q-Learning Applied to Soft-Kill Countermeasures for Unmanned Aerial Vehicles.pdf:/Users/jaime/Zotero/storage/N55YJKV5/Da Silva et al_2020_Q-Learning Applied to Soft-Kill Countermeasures for Unmanned Aerial Vehicles.pdf:application/pdf},
  }

  @article{Stooke2020,
  	title = {Decoupling Representation Learning from Reinforcement Learning},
  	url = {http://arxiv.org/abs/2009.08319},
  	abstract = {In an effort to overcome limitations of reward-driven feature learning in deep reinforcement learning ({RL}) from images, we propose decoupling representation learning from policy learning. To this end, we introduce a new unsupervised learning ({UL}) task, called Augmented Temporal Contrast ({ATC}), which trains a convolutional encoder to associate pairs of observations separated by a short time difference, under image augmentations and using a contrastive loss. In online {RL} experiments, we show that training the encoder exclusively using {ATC} matches or outperforms end-to-end {RL} in most environments. Additionally, we benchmark several leading {UL} algorithms by pre-training encoders on expert demonstrations and using them, with weights frozen, in {RL} agents; we find that agents using {ATC}-trained encoders outperform all others. We also train multi-task encoders on data from multiple environments and show generalization to different downstream {RL} tasks. Finally, we ablate components of {ATC}, and introduce a new data augmentation to enable replay of (compressed) latent images from pre-trained encoders when {RL} requires augmentation. Our experiments span visually diverse {RL} benchmarks in {DeepMind} Control, {DeepMind} Lab, and Atari, and our complete code is available at https://github.com/astooke/rlpyt/tree/master/rlpyt/ul.},
  	author = {Stooke, Adam and Lee, Kimin and Abbeel, Pieter and Laskin, Michael},
  	date = {2020},
  	file = {Stooke et al_2020_Decoupling Representation Learning from Reinforcement Learning.pdf:/Users/jaime/Zotero/storage/FJAZRBLY/Stooke et al_2020_Decoupling Representation Learning from Reinforcement Learning.pdf:application/pdf},
  }

  @article{Tobin2017,
  	title = {Domain randomization for transferring deep neural networks from simulation to the real world},
  	volume = {2017-Septe},
  	issn = {21530866},
  	doi = {10.1109/IROS.2017.8202133},
  	abstract = {Bridging the 'reality gap' that separates simulated robotics from experiments on hardware could accelerate robotic research through improved data availability. This paper explores domain randomization, a simple technique for training models on simulated images that transfer to real images by randomizing rendering in the simulator. With enough variability in the simulator, the real world may appear to the model as just another variation. We focus on the task of object localization, which is a stepping stone to general robotic manipulation skills. We find that it is possible to train a real-world object detector that is accurate to 1.5 cm and robust to distractors and partial occlusions using only data from a simulator with non-realistic random textures. To demonstrate the capabilities of our detectors, we show they can be used to perform grasping in a cluttered environment. To our knowledge, this is the first successful transfer of a deep neural network trained only on simulated {RGB} images (without pre-training on real images) to the real world for the purpose of robotic control.},
  	pages = {23--30},
  	journaltitle = {{IEEE} International Conference on Intelligent Robots and Systems},
  	author = {Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter},
  	date = {2017},
  	note = {{ISBN}: 9781538626825},
  }

  @article{Peng2018,
  	title = {Sim-to-Real Transfer of Robotic Control with Dynamics Randomization},
  	issn = {10504729},
  	doi = {10.1109/ICRA.2018.8460528},
  	abstract = {Simulations are attractive environments for training agents as they provide an abundant source of data and alleviate certain safety concerns during the training process. But the behaviours developed by agents in simulation are often specific to the characteristics of the simulator. Due to modeling error, strategies that are successful in simulation may not transfer to their real world counterparts. In this paper, we demonstrate a simple method to bridge this 'reality gap'. By randomizing the dynamics of the simulator during training, we are able to develop policies that are capable of adapting to very different dynamics, including ones that differ significantly from the dynamics on which the policies were trained. This adaptivity enables the policies to generalize to the dynamics of the real world without any training on the physical system. Our approach is demonstrated on an object pushing task using a robotic arm. Despite being trained exclusively in simulation, our policies are able to maintain a similar level of performance when deployed on a real robot, reliably moving an object to a desired location from random initial configurations. We explore the impact of various design decisions and show that the resulting policies are robust to significant calibration error.},
  	pages = {3803--3810},
  	journaltitle = {Proceedings - {IEEE} International Conference on Robotics and Automation},
  	author = {Peng, Xue Bin and Andrychowicz, Marcin and Zaremba, Wojciech and Abbeel, Pieter},
  	date = {2018},
  	note = {{ISBN}: 9781538630815},
  }

  @article{Czarnecki2018,
  	title = {Mix \& match - Agent curricula for reinforcement learning},
  	volume = {3},
  	abstract = {We introduce Mix \& Match (M\&M) - A training framework designed to facilitate rapid and effective learning in {RL} agents, especially those that would be too slow or too challenging to train otherwise. The key innovation is a procedure that allows us to automatically form a curriculum over agents. Through such a curriculum we can progressively train more complex agents by, effectively, bootstrapping from solutions found by simpler agents. In contradistinction to typical curriculum learning approaches, we do not gradually modify the tasks or environments presented, but instead use a process to gradually alter how the policy is represented internally. We show the broad applicability of our method by demonstrating significant performance gains in three different experimental setups: (I) We train an agent able to control more than 700 actions in a challenging 3D first-person task; using our method to progress through an action-space cur-riculum wc achieve both faster training and better final performance than one obtains using tradi-tional methods. (2) We further show that M\&M can be used successfully to progress through a curriculum of architectural variants defining an agents internal state. (3) Finally, we illustrate how a variant of our method can be used to improve agent performance in a multitask setting.},
  	pages = {1761--1773},
  	journaltitle = {35th International Conference on Machine Learning, {ICML} 2018},
  	author = {Czarnecki, Wojciech Marian and Jayakumar, Siddhant M. and Jadcrbcrg, Max and Hasenclever, Leonard and Tch, Yec Whye and Osindero, Simon and Heess, Nicolas and Pascanu, Razvan},
  	date = {2018},
  	note = {{ISBN}: 9781510867963},
  }

  @article{Dong2020,
  	title = {Learning Task-aware Local Representations for Few-shot Learning},
  	issn = {10450823},
  	doi = {10.24963/ijcai.2020/100},
  	abstract = {Few-shot learning for visual recognition aims to adapt to novel unseen classes with only a few images. Recent work, especially the work based on low-level information, has achieved great progress. In these work, local representations ({LRs}) are typically employed, because {LRs} are more consistent among the seen and unseen classes. However, most of them are limited to an individual image-to-image or image-to-class measure manner, which cannot fully exploit the capabilities of {LRs}, especially in the context of a certain task. This paper proposes an Adaptive Task-aware Local Representations Network ({ATL}-Net) to address this limitation by introducing episodic attention, which can adaptively select the important local patches among the entire task, as the process of human recognition. We achieve much superior results on multiple benchmarks. On the {miniImagenet}, {ATL}-Net gains 0.93\% and 0.88\% improvements over the compared methods under the 5-way 1-shot and 5-shot settings. Moreover, {ATL}-Net can naturally tackle the problem that how to adaptively identify and weight the importance of different key local parts, which is the major concern of fine-grained recognition. Specifically, on the fine-grained dataset Stanford Dogs, {ATL}-Net outperforms the second best method with 5.39\% and 9.69\% gains under the 5-way 1-shot and 5-shot settings.},
  	pages = {716--722},
  	author = {Dong, Chuanqi and Li, Wenbin and Huo, Jing and Gu, Zheng and Gao, Yang},
  	date = {2020},
  	note = {{ISBN}: 9780999241165},
  	keywords = {Computer Vision: Recognition: Detection \& Categori, ★},
  	file = {Dong et al_2020_Learning Task-aware Local Representations for Few-shot Learning.pdf:/Users/jaime/Zotero/storage/6T7BDJMR/Dong et al_2020_Learning Task-aware Local Representations for Few-shot Learning.pdf:application/pdf},
  }

  @article{Svetlik2017,
  	title = {Automatic curriculum graph generation for reinforcement learning agents},
  	abstract = {In recent years, research has shown that transfer learning methods can be leveraged to construct curricula that sequence a series of simpler tasks such that performance on a final target task is improved. A major limitation of existing approaches is that such curricula are handcrafted by humans that are typically domain experts. To address this limitation, we introduce a method to generate a curriculum based on task descriptors and a novel metric of transfer potential. Our method automatically generates a curriculum as a directed acyclic graph (as opposed to a linear sequence as done in existing work). Experiments in both discrete and continuous domains show that our method produces curricula that improve the agent's learning performance when compared to the baseline condition of learning on the target task from scratch.},
  	pages = {2590--2596},
  	number = {1},
  	journaltitle = {31st {AAAI} Conference on Artificial Intelligence, {AAAI} 2017},
  	author = {Svetlik, Maxwell and Leonetti, Matteo and Sinapov, Jivko and Shah, Rishi and Walker, Nick and Stone, Peter},
  	date = {2017},
  }

  @article{Molchanov2018,
  	title = {Region growing curriculum generation for reinforcement learning},
  	abstract = {Learning a policy capable of moving an agent between any two states in the environment is important for many robotics problems involving navigation and manipulation. Due to the sparsity of rewards in such tasks, applying reinforcement learning in these scenarios can be challenging. Common approaches for tackling this problem include reward engineering with auxiliary rewards, requiring domain-specific knowledge or changing the objective. In this work, we introduce a method based on region-growing that allows learning in an environment with any pair of initial and goal states. Our algorithm first learns how to move between nearby states and then increases the difficulty of the start-goal transitions as the agent's performance improves. This approach creates an efficient curriculum for learning the objective behavior of reaching any goal from any initial state. In addition, we describe a method to adaptively adjust expansion of the growing region that allows automatic adjustment of the key exploration hyperparameter to environments with different requirements. We evaluate our approach on a set of simulated navigation and manipulation tasks, where we demonstrate that our algorithm can efficiently learn a policy in the presence of sparse rewards.},
  	pages = {1026--1034},
  	journaltitle = {{arXiv}},
  	author = {Molchanov, Artem and Hausman, Karol and Birchfield, Stan and Sukhatme, Gaurav},
  	date = {2018},
  	keywords = {Reinforcement Learning, transfer learning, 2018, acm reference format, and anna helena reali, costa, curriculum learning, Curriculum Learning, felipe leno da silva, object-oriented, reinforcement learning, Transfer Learning},
  }

  @article{Narvekar2019,
  	title = {Learning curriculum policies for reinforcement learning},
  	volume = {1},
  	issn = {15582914},
  	abstract = {Curriculum learning in reinforcement learning is a training methodology that seeks to speed up learning of a difficult target task, by first training on a series of simpler tasks and transferring the knowledge acquired to the target task. Automatically choosing a sequence of such tasks (i.e., a curriculum) is an open problem that has been the subject of much recent work in this area. In this paper, we build upon a recent method for curriculum design, which formulates the curriculum sequencing problem as a Markov Decision Process. We extend this model to handle multiple transfer learning algorithms, and show for the first time that a curriculum policy over this {MDP} can be learned from experience. We explore various representations that make this possible, and evaluate our approach by learning curriculum policies for multiple agents in two different domains. The results show that our method produces curricula that can train agents to perform on a target task as fast or faster than existing methods.},
  	pages = {25--33},
  	journaltitle = {Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, {AAMAS}},
  	author = {Narvekar, Sanmit and Stone, Peter},
  	date = {2019},
  	note = {{ISBN}: 9781510892002},
  	keywords = {Curriculum learning, Reinforcement learning, Transfer learning},
  	file = {Narvekar_Stone_2019_Learning curriculum policies for reinforcement learning.pdf:/Users/jaime/Zotero/storage/GQLJTG8M/Narvekar_Stone_2019_Learning curriculum policies for reinforcement learning.pdf:application/pdf},
  }

  @article{Florensa2017,
  	title = {Reverse curriculum generation for reinforcement learning},
  	abstract = {Many relevant tasks require an agent to reach a certain state or to manipulate objects into a desired configuration. For example, we might want a robot to align and assemble a gear onto an axle or insert and turn a key in a lock. These goal-oriented tasks present a considerable challenge for reinforcement learning, since their natural reward function is sparse and prohibitive amounts of exploration are required to reach the goal and receive some learning signal. Past approaches tackle these problems by exploiting expert demonstrations or by manually designing a task-specific reward shaping function to guide the learning agent. Instead, we propose a method to learn these tasks without requiring any prior knowledge other than obtaining a single state in which the task is achieved. The robot is trained in “reverse,” gradually learning to reach the goal from a set of start states increasingly far from the goal. Our method automatically generates a curriculum of start states that adapts to the agent's performance, leading to efficient training on goal-oriented tasks. We demonstrate our approach on difficult simulated navigation and fine-grained manipulation problems, not solvable by state-of-the-art reinforcement learning methods.},
  	pages = {1--14},
  	issue = {{CoRL}},
  	journaltitle = {{arXiv}},
  	author = {Florensa, Carlos and Held, David and Wulfmeier, Markus and Zhang, Michael R. and Abbeel, Pieter},
  	date = {2017},
  	keywords = {Reinforcement Learning, Automatic Curriculum Generation, Robotic Manipulation},
  }

  @article{Qiu2020,
  	title = {Towards crossing the reality gap with evolved plastic neurocontrollers},
  	doi = {10.1145/3377930.3389843},
  	abstract = {A critical issue in evolutionary robotics is the transfer of controllers learned in simulation to reality. This is especially the case for small Unmanned Aerial Vehicles ({UAVs}), as the platforms are highly dynamic and susceptible to breakage. Previous approaches often require simulation models with a high level of accuracy, otherwise significant errors may arise when the well-designed controller is being deployed onto the targeted platform. Here we try to overcome the transfer problem from a different perspective, by designing a spiking neurocontroller which uses synaptic plasticity to cross the reality gap via online adaptation. Through a set of experiments we show that the evolved plastic spiking controller can maintain its functionality by self-adapting to model changes that take place after evolutionary training, and consequently exhibit better performance than its non-plastic counterpart.},
  	pages = {130--138},
  	journaltitle = {{GECCO} 2020 - Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
  	author = {Qiu, Huanneng and Garratt, Matthew and Howard, David and Anavatti, Sreenatha},
  	date = {2020},
  	note = {{ISBN}: 9781450371285},
  	keywords = {Neuroevolution, Evolutionary robotics, Hebbian plasticity, Spiking neural networks, {UAV} control, ★},
  	file = {Qiu et al_2020_Towards crossing the reality gap with evolved plastic neurocontrollers.pdf:/Users/jaime/Zotero/storage/GDLX69MZ/Qiu et al_2020_Towards crossing the reality gap with evolved plastic neurocontrollers.pdf:application/pdf},
  }

  @article{Titchener2020,
  	title = {Oculomotor Responses to Dynamic Stimuli in a 44-Channel Suprachoroidal Retinal Prosthesis},
  	pages = {1--13},
  	author = {Titchener, Samuel A and Kvansakul, Jessica and Shivdasani, Mohit N and Fallon, James B and Nayagam, D A X and Epp, Stephanie B and Williams, Chris E and Barnes, Nick and Kentler, William G and Kolic, Maria and Baglin, Elizabeth K and Ayton, Lauren N and Abbott, Carla J and Luu, Chi D and Allen, Penelope J and Petoe, Matthew A},
  	date = {2020},
  	keywords = {★, citation, eye, head movements, kvansakul j, motion perception, movements, prosthesis, retinal, titchener sa, visual prosthesis},
  	file = {Titchener et al_2020_Oculomotor Responses to Dynamic Stimuli in a 44-Channel Suprachoroidal Retinal.pdf:/Users/jaime/Zotero/storage/LWBAQIBJ/Titchener et al_2020_Oculomotor Responses to Dynamic Stimuli in a 44-Channel Suprachoroidal Retinal.pdf:application/pdf},
  }

  @article{Jakobi1998,
  	title = {Running across the reality gap: Octopod locomotion evolved in a minimal simulation},
  	volume = {1468},
  	issn = {16113349},
  	doi = {10.1007/3-540-64957-3_63},
  	abstract = {This paper describes experiments in which neural network control architectures were evolved in minimal simulation for an octopod robot. The robot is around 30cm long and has 4 infra red sensors that point ahead and to the side, various bumpers and whiskers, and ten ambient light sensors positioned strategically around the body. Each of the robot's eight legs is controlled by two servo motors, one for movement in the horizontal plane, and one for movement in the vertical plane, which means that the robots motors have a total of sixteen degrees of freedom. The aim of the experiments was to evolve neural network control architectures that would allow the robot to wander around its environment avoiding objects using its infra-red sensors and backing away from objects that it hits with its bumpers. This is a hard behaviour to evolve when one considers that in order to achieve any sort of coherent movement the controller has to control not just one or two motors in a coordinated fashion but sixteen. Moreover it is an extremely difficult set-up to simulate using traditional techniques since the physical outcome of sixteen motor movements is rarely predictable in all but the simplest cases. The evolution of this behaviour in a minimal simulation, with perfect transference to reality, therefore, provides essential evidence that complex motor behaviours can be evolved in simulations built according to the theory and methodology of minimal simulations.},
  	pages = {39--58},
  	journaltitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  	author = {Jakobi, Nick},
  	date = {1998},
  	note = {{ISBN}: 3540649573},
  	file = {Jakobi_1998_Running across the reality gap.pdf:/Users/jaime/Zotero/storage/LAW3XZPW/Jakobi_1998_Running across the reality gap.pdf:application/pdf},
  }

  @article{Koos2013,
  	title = {The transferability approach: Crossing the reality gap in evolutionary robotics},
  	volume = {17},
  	issn = {1089778X},
  	doi = {10.1109/TEVC.2012.2185849},
  	abstract = {The reality gap, which often makes controllers evolved in simulation inefficient once transferred onto the physical robot, remains a critical issue in evolutionary robotics ({ER}). We hypothesize that this gap highlights a conflict between the efficiency of the solutions in simulation and their transferability from simulation to reality: the most efficient solutions in simulation often exploit badly modeled phenomena to achieve high fitness values with unrealistic behaviors. This hypothesis leads to the transferability approach, a multiobjective formulation of {ER} in which two main objectives are optimized via a Pareto-based multiobjective evolutionary algorithm: 1) the fitness; and 2) the transferability, estimated by a simulation-to-reality ({STR}) disparity measure. To evaluate this second objective, a surrogate model of the exact {STR} disparity is built during the optimization. This transferability approach has been compared to two reality-based optimization methods, a noise-based approach inspired from Jakobi's minimal simulation methodology and a local search approach. It has been validated on two robotic applications: 1) a navigation task with an e-puck robot; and 2) a walking task with a 8-{DOF} quadrupedal robot. For both experimental setups, our approach successfully finds efficient and well-transferable controllers only with about ten experiments on the physical robot. © 1997-2012 {IEEE}.},
  	pages = {122--145},
  	number = {1},
  	journaltitle = {{IEEE} Transactions on Evolutionary Computation},
  	author = {Koos, Sylvain and Mouret, Jean Baptiste and Doncieux, Stéphane},
  	date = {2013},
  	keywords = {Evolutionary robotics, reality gap, transfer-ability approach},
  	file = {Koos et al_2013_The transferability approach.pdf:/Users/jaime/Zotero/storage/XRQZ9GST/Koos et al_2013_The transferability approach.pdf:application/pdf},
  }

  @article{Guanella2006,
  	title = {Flying over the reality gap : From simulated to real indoor airships},
  	doi = {10.1007/s10514-006-9718-8},
  	pages = {243--254},
  	author = {Guanella, Jean-christophe Zufferey Alexis and Beyeler, Antoine and Floreano, Dario},
  	date = {2006},
  	file = {Guanella et al_2006_Flying over the reality gap.pdf:/Users/jaime/Zotero/storage/93CAKCPE/Guanella et al_2006_Flying over the reality gap.pdf:application/pdf},
  }

  @article{Matthias2019,
  	title = {Sim-to-Real Transfer for Autonomous Navigation Dissertation by},
  	author = {Matthias, M},
  	date = {2019},
  }

  @article{Chapman-Rounds2019,
  	title = {Inattentional Blindness in Visual Search},
  	doi = {10.31234/osf.io/yts23},
  	abstract = {Models of visual saliency normally belong to one of two camps: models such as Experience Guided Search (E-{GS}), which emphasize top-down guidance based on task features, and models such as Attention as Information Maximisation ({AIM}), which emphasize the role of bottom-up saliency. In this paper, we show that E-{GS} and {AIM} are structurally similar and can be unified to create a general model of visual search which includes a generic prior over potential non-task related objects. We demonstrate that this model displays inattentional blindness, and that blindness can be modulated by adjusting the relative precisions of several terms within the model. At the same time, our model correctly accounts for a series of classical visual search results.},
  	author = {Chapman-Rounds, Matt and Lucas, Christopher and Keller, Frank},
  	date = {2019},
  	keywords = {Bayesian Brain, Inattentional Blindness, Visual Search},
  	file = {Chapman-Rounds et al_2019_Inattentional Blindness in Visual Search.pdf:/Users/jaime/Zotero/storage/GRPXIUVY/Chapman-Rounds et al_2019_Inattentional Blindness in Visual Search.pdf:application/pdf},
  }

  @misc{Tremblay2018,
  	title = {Training deep networks with synthetic data: bridging the reality gap by domain randomization},
  	abstract = {We present a system for training deep neural networks for object detection using synthetic images. To handle the variability in real-world data, the system relies upon the technique of domain randomization, in which the parameters of the simulator—such as lighting, pose, object textures, etc.—are randomized in non-realistic ways to force the neural network to learn the essential features of the object of interest. We explore the importance of these parameters, showing that it is possible to produce a network with compelling performance using only non-artistically-generated synthetic data. With additional fine-tuning on real data, the network yields better performance than using real data alone. This result opens up the possibility of using inexpensive synthetic data for training neural networks while avoiding the need to collect large amounts of hand-annotated real-world data or to generate high-fidelity synthetic worlds—both of which remain bottlenecks for many applications. The approach is evaluated on bounding box detection of cars on the {KITTI} dataset.},
  	author = {Tremblay, Jonathan and Prakash, Aayush and Acuna, David and Brophy, Mark and Jampani, Varun and Anil, Cem and To, Thang and Cameracci, Eric and Boochoon, Shaad and Birchfield, Stan},
  	date = {2018},
  	note = {Publication Title: {arXiv}},
  	file = {Tremblay et al_2018_Training deep networks with synthetic data.pdf:/Users/jaime/Zotero/storage/XRQ8QSEB/Tremblay et al_2018_Training deep networks with synthetic data.pdf:application/pdf},
  }

  @article{Tecnico2004,
  	title = {{IAV}2004 - {PREPRINTS} 5th {IFAC} / {EURON} Symposium on Intelligent Autonomous Vehicles},
  	volume = {37},
  	issn = {1474-6670},
  	url = {http://dx.doi.org/10.1016/S1474-6670(17)32084-0},
  	doi = {10.1016/S1474-6670(17)32084-0},
  	pages = {834--839},
  	number = {8},
  	journaltitle = {{IFAC} Proceedings Volumes},
  	author = {Técnico, Instituto Superior},
  	date = {2004},
  	note = {Publisher: Elsevier},
  	keywords = {autonomous systems, evolutionary robotics, Evolutionary Robotics,Autonomous Systems,On-line L, on-line learning},
  	file = {Técnico_2004_IAV2004 - PREPRINTS 5th IFAC - EURON Symposium on Intelligent Autonomous.pdf:/Users/jaime/Zotero/storage/7V5B6RJT/Técnico_2004_IAV2004 - PREPRINTS 5th IFAC - EURON Symposium on Intelligent Autonomous.pdf:application/pdf},
  }

  @article{GarridoMerchan2020,
  	title = {A machine consciousness architecture based on deep learning and gaussian processes},
  	doi = {10.1007/978-3-030-61705-9_29},
  	abstract = {Recent developments in machine learning have pushed the tasks that machines can do outside the boundaries of what was thought to be possible years ago. Methodologies such as deep learning or generative models have achieved complex tasks such as generating art pictures or literature automatically. Machine Consciousness is a field that has been deeply studied and several theories based in the functionalism philosophical theory like the global workspace theory have been proposed. In this work, we propose an architecture that may arise consciousness in a machine based in the global workspace theory and in the assumption that consciousness appear in machines that have cognitive processes and exhibit conscious behaviour. This architecture is based in processes that use the recent Deep Learning and generative process models. For every module of this architecture, we provide detailed explanations of the models involved and how they communicate with each other to create the cognitive architecture. We illustrate how we can optimize the architecture to generate social interactions between robots and genuine pieces of art, both features correlated with machine consciousness. As far as we know, this is the first machine consciousness architecture that use generative models and deep learning to exhibit conscious social behaviour and to retrieve pictures and other subjective content made by robots.},
  	pages = {1--12},
  	journaltitle = {{arXiv}},
  	author = {Garrido Merchán, Eduardo C. and Molina, Martin},
  	date = {2020},
  	keywords = {Artificial Intelligence, Deep Learning Gaussian Processes, Machine Consciousness, Machine Learning},
  	file = {Garrido Merchán_Molina_2020_A machine consciousness architecture based on deep learning and gaussian.pdf:/Users/jaime/Zotero/storage/U8VXJH7E/Garrido Merchán_Molina_2020_A machine consciousness architecture based on deep learning and gaussian.pdf:application/pdf},
  }

  @article{Rusu2017,
  	title = {Sim-to-Real Robot Learning from Pixels with Progressive Nets},
  	pages = {1--9},
  	issue = {{CoRL}},
  	author = {Rusu, Andrei A and Heess, Nicolas and Rothörl, Thomas},
  	date = {2017},
  	keywords = {corl, progressive networks, robot learning, sim-to-real, transfer},
  	file = {Rusu et al_2016_Sim-to-Real Robot Learning from Pixels with Progressive Nets.pdf:/Users/jaime/Zotero/storage/ZQ6R7KPG/Rusu et al_2016_Sim-to-Real Robot Learning from Pixels with Progressive Nets.pdf:application/pdf;Rusu et al_2017_Sim-to-Real Robot Learning from Pixels with Progressive Nets.pdf:/Users/jaime/Zotero/storage/PL38WABZ/Rusu et al_2017_Sim-to-Real Robot Learning from Pixels with Progressive Nets.pdf:application/pdf},
  }

  @inproceedings{Jakobi1995,
  	title = {Noise and the reality gap: The use of simulation in evolutionary robotics},
  	volume = {929},
  	isbn = {3-540-59496-5},
  	doi = {10.1007/3-540-59496-5_337},
  	abstract = {The pitfalls of naive robot simulations have been recognised for areas such as evolutionary robotics. It has been suggested that carefully validated simulations with a proper treatment of noise may overcome these problems. This paper reports the results of experiments intended to test some of these claims. A simulation was constructed of a two-wheeled Kheper{\textasciitilde} robot with {IR} and ambient light sensors. This included detailed mathematical models of the robot-environment interaction dynamics with empirically determined parameters. Artificial evolution was used to develop recurrent dynamical network controllers for the simulated robot, for obstacle-avoidance and light-seeking tasks, using different levels of noise in the simulation. The evolved controllers were down-loaded onto the real robot a=hd the correspondence between behaviour in si mulation and in reality was tosted. The level of correspondence varied according to how much noise was used in the simulation, with very good results achieved when realistic quantities were applied. It has been demonstrated that it is possible to develop successful robot controllers in simulation that generate almost identical behaviours in reality, at least for a particular class of robot-environment interaction dynamics.},
  	pages = {704--720},
  	booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  	author = {Jakobi, Nick and Husbands, Phil and Harvey, Inman},
  	date = {1995},
  	note = {{ISSN}: 16113349},
  	keywords = {Evolutionary robotics, Artificial evolution, High fidelity simulations, Noise},
  	file = {Jakobi et al_1995_Noise and the reality gap.pdf:/Users/jaime/Zotero/storage/2WZ4S5M3/Jakobi et al_1995_Noise and the reality gap.pdf:application/pdf},
  }

  @book{Balaji2019,
  	title = {{DeepRacer}: Educational autonomous racing platform for experimentation with sim2real reinforcement learning},
  	isbn = {978-1-72817-395-5},
  	abstract = {{DeepRacer} is a platform for end-to-end experimentation with {RL} and can be used to systematically investigate the key challenges in developing intelligent control systems. Using the platform, we demonstrate how a 1/18th scale car can learn to drive autonomously using {RL} with a monocular camera. It is trained in simulation with no additional tuning in physical world and demonstrates: 1) formulation and solution of a robust reinforcement learning algorithm, 2) narrowing the reality gap through joint perception and dynamics, 3) distributed on-demand compute architecture for training optimal policies, and 4) a robust evaluation method to identify when to stop training. It is the first successful large-scale deployment of deep reinforcement learning on a robotic control agent that uses only raw camera images as observations and a model-free learning method to perform robust path planning. We open source our code and video demo on {GitHub}2.},
  	pagetotal = {2746-2754},
  	author = {Balaji, Bharathan and Mallya, Sunil and Genc, Sahika and Gupta, Saurabh and Dirac, Leo and Khare, Vineet and Roy, Gourav and Sun, Tao and Tao, Yunzhe and Townsend, Brian and Calleja, Eddie and Muralidhara, Sunil and Karuppasamy, Dhanasekar},
  	date = {2019},
  	note = {Publication Title: {arXiv}},
  	file = {Balaji et al_2019_DeepRacer.pdf:/Users/jaime/Zotero/storage/6P4RPXEH/Balaji et al_2019_DeepRacer.pdf:application/pdf},
  }

  @inproceedings{Stein2018,
  	title = {{GeneSIS}-Rt: Generating Synthetic Images for Training Secondary Real-World Tasks},
  	isbn = {978-1-5386-3081-5},
  	doi = {10.1109/ICRA.2018.8462971},
  	abstract = {We propose a novel approach for generating high-quality, synthetic data for domain-specific learning tasks, for which training data may not be readily available. We leverage recent progress in image-to-image translation to bridge the gap between simulated and real images, allowing us to generate realistic training data for real-world tasks using only unlabeled real-world images and a simulation. {GeneSIS}-Rtameliorates the burden of having to collect labeled real-world images and is a promising candidate for generating high-quality, domain-specific, synthetic data. To show the effectiveness of using {GeneSIS}-Rtto create training data, we study two tasks: semantic segmentation and reactive obstacle avoidance. We demonstrate that learning algorithms trained using data generated by {GeneSIS}-{RT} make high-accuracy predictions and outperform systems trained on raw simulated data alone, and as well or better than those trained on real data. Finally, we use our data to train a quadcopter to fly 60 meters at speeds up to 3.4 m/s through a cluttered environment, demonstrating that our {GeneSIS}-{RT} images can be used to learn to perform mission-critical tasks.},
  	pages = {7151--7158},
  	booktitle = {Proceedings - {IEEE} International Conference on Robotics and Automation},
  	author = {Stein, Gregory J and Roy, Nicholas},
  	date = {2018},
  	note = {{ISSN}: 10504729},
  	file = {Stein_Roy_2018_GeneSIS-Rt.pdf:/Users/jaime/Zotero/storage/KLYGKJ5T/Stein_Roy_2018_GeneSIS-Rt.pdf:application/pdf},
  }

  @inproceedings{Bousmalis2018,
  	title = {Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping},
  	isbn = {978-1-5386-3081-5},
  	doi = {10.1109/ICRA.2018.8460875},
  	abstract = {Instrumenting and collecting annotated visual grasping datasets to train modern machine learning algorithms can be extremely time-consuming and expensive. An appealing alternative is to use off-the-shelf simulators to render synthetic data for which ground-truth annotations are generated automatically. Unfortunately, models trained purely on simulated data often fail to generalize to the real world. We study how randomized simulated environments and domain adaptation methods can be extended to train a grasping system to grasp novel objects from raw monocular {RGB} images. We extensively evaluate our approaches with a total of more than 25,000 physical test grasps, studying a range of simulation conditions and domain adaptation methods, including a novel extension of pixel-level domain adaptation that we term the {GraspGAN}. We show that, by using synthetic data and domain adaptation, we are able to reduce the number of real-world samples needed to achieve a given level of performance by up to 50 times, using only randomly generated simulated objects. We also show that by using only unlabeled real-world data and our {GraspGAN} methodology, we obtain real-world grasping performance without any real-world labels that is similar to that achieved with 939,777 labeled real-world samples.},
  	pages = {4243--4250},
  	booktitle = {Proceedings - {IEEE} International Conference on Robotics and Automation},
  	author = {Bousmalis, Konstantinos and Irpan, Alex and Wohlhart, Paul and Bai, Yunfei and Kelcey, Matthew and Kalakrishnan, Mrinal and Downs, Laura and Ibarz, Julian and Pastor, Peter and Konolige, Kurt and Levine, Sergey and Vanhoucke, Vincent},
  	date = {2018},
  	note = {{ISSN}: 10504729},
  	file = {Bousmalis et al_2018_Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic.pdf:/Users/jaime/Zotero/storage/SVKDMZ4N/Bousmalis et al_2018_Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic.pdf:application/pdf},
  }

  @article{Zhao,
  	title = {Towards Closing the Sim-to-Real Gap in Collaborative Multi-Robot Deep Reinforcement Learning},
  	pages = {1--7},
  	author = {Zhao, Wenshuai and Pe, Jorge},
  	file = {Zhao_Pe_Towards Closing the Sim-to-Real Gap in Collaborative Multi-Robot Deep.pdf:/Users/jaime/Zotero/storage/V2KTEGJD/Zhao_Pe_Towards Closing the Sim-to-Real Gap in Collaborative Multi-Robot Deep.pdf:application/pdf},
  }

  @article{Ji2018,
  	title = {Saliency detection via conditional adversarial image-to-image network},
  	volume = {316},
  	issn = {18728286},
  	url = {https://doi.org/10.1016/j.neucom.2018.08.013},
  	doi = {10.1016/j.neucom.2018.08.013},
  	abstract = {Recent research has shown that conditional adversarial network ({cGAN}) can be adopted to perform image-to-image translation task effectively. Saliency detection is another challenging computer vision task to model human vision attention mechanism. By reformulating the saliency detection task, in this work, we propose to conduct saliency detection by exploiting conditional adversarial network under the {cGAN} framework, in which saliency map prediction is transformed as a saliency segmentation task by using pair-wised image-to-ground-truth saliency. To further investigate the potential of {cGAN} for saliency detection, we train the {cGAN} model to capture saliency-to-context information by translating saliency mask to real image. Experimental results confirm that the trained generator can achieve comparable state-of-the-art performance on saliency segmentation, and can generate reasonable results for saliency-to-image translation.},
  	pages = {357--368},
  	journaltitle = {Neurocomputing},
  	author = {Ji, Yuzhu and Zhang, Haijun and Jonathan Wu, Q. M.},
  	date = {2018},
  	note = {Publisher: Elsevier B.V.},
  	keywords = {Conditional adversarial network, Image to image translation, Saliency segmentation},
  	file = {Ji et al_2018_Saliency detection via conditional adversarial image-to-image network.pdf:/Users/jaime/Zotero/storage/FVXRZ45H/Ji et al_2018_Saliency detection via conditional adversarial image-to-image network.pdf:application/pdf},
  }

  @inproceedings{Sadeghi2017,
  	title = {{CAD}2RL: Real single-image flight without a single real image},
  	volume = {13},
  	isbn = {978-0-9923747-3-0},
  	doi = {10.15607/rss.2017.xiii.034},
  	abstract = {Deep reinforcement learning has emerged as a promising and powerful technique for automatically acquiring control policies that can process raw sensory inputs, such as images, and perform complex behaviors. However, extending deep {RL} to real-world robotic tasks has proven challenging, particularly in safety-critical domains such as autonomous flight, where a trial-and-error learning process is often impractical. In this paper, we explore the following question: can we train visionbased navigation policies entirely in simulation, and then transfer them into the real world to achieve real-world flight without a single real training image? We propose a learning method that we call {CAD}2RL, which can be used to perform collision-free indoor flight in the real world while being trained entirely on 3D {CAD} models. Our method uses single {RGB} images from a monocular camera, without needing to explicitly reconstruct the 3D geometry of the environment or perform explicit motion planning. Our learned collision avoidance policy is represented by a deep convolutional neural network that directly processes raw monocular images and outputs velocity commands. This policy is trained entirely on simulated images, with a Monte Carlo policy evaluation algorithm that directly optimizes the network's ability to produce collision-free flight. By highly randomizing the rendering settings for our simulated training set, we show that we can train a policy that generalizes to the real world, without requiring the simulator to be particularly realistic or high-fidelity. We evaluate our method by flying a real quadrotor through indoor environments, and further evaluate the design choices in our simulator through a series of ablation studies on depth prediction.},
  	booktitle = {Robotics: Science and Systems},
  	author = {Sadeghi, Fereshteh and Levine, Sergey},
  	date = {2017},
  	note = {{ISSN}: 2330765X},
  	file = {Sadeghi_Levine_2017_CAD2RL.pdf:/Users/jaime/Zotero/storage/PJMBK7NU/Sadeghi_Levine_2017_CAD2RL.pdf:application/pdf},
  }

  @article{Zhao2019,
  	title = {Pyramid feature attention network for saliency detection},
  	volume = {2019-June},
  	issn = {10636919},
  	doi = {10.1109/CVPR.2019.00320},
  	abstract = {Saliency detection is one of the basic challenges in computer vision. Recently, {CNNs} are the most widely used and powerful techniques for saliency detection, in which feature maps from different layers are always integrated without distinction. However, instinctively, the different feature maps of {CNNs} and the different features in the same maps should play different roles in saliency detection. To address this problem, a novel {CNN} named pyramid feature attention network ({PFAN}) is proposed to enhance the high-level context features and the low-level spatial structural features. In the proposed {PFAN}, a context-aware pyramid feature extraction ({CPFE}) module is designed for multi-scale high-level feature maps to capture the rich context features. A channel-wise attention ({CA}) model and a spatial attention ({SA}) model are respectively applied to the {CPFE} feature maps and the low-level feature maps, and then fused to detect salient regions. Finally, an edge preservation loss is proposed to get the accurate boundaries of salient regions. The proposed {PFAN} is extensively evaluated on five benchmark datasets and the experimental results demonstrate that the proposed network outperforms the state-of-the-art approaches under different evaluation metrics.},
  	pages = {3080--3089},
  	journaltitle = {Proceedings of the {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition},
  	author = {Zhao, Ting and Wu, Xiangqian},
  	date = {2019},
  	note = {{ISBN}: 9781728132938},
  	keywords = {Categorization, Deep Learning, Grouping and Shape, Recognition: Detection, Retrieval, Segmentation},
  	file = {Zhao_Wu_2019_Pyramid feature attention network for saliency detection.pdf:/Users/jaime/Zotero/storage/XHVN3JTJ/Zhao_Wu_2019_Pyramid feature attention network for saliency detection.pdf:application/pdf},
  }

  @article{Pan2017,
  	title = {{SalGAN}: Visual saliency prediction with adversarial networks},
  	abstract = {Recent approaches for saliency prediction are generally trained with a loss function based on a single saliency metric. This could lead to low performance when evaluating with other saliency metrics. In this paper, we propose a novel data-driven metric based saliency prediction method, named {SalGAN} (Saliency {GAN}), trained with adversarial loss function. {SalGAN} consists of two networks: one predicts saliency maps from raw pixels of an input image; the other one takes the output of the first one to discriminate whether a saliency map is a predicted one or ground truth. By trying to make the predicted saliency map indistinguishable with the ground truth, {SalGAN} is expected to generate saliency maps that resembles the ground truth. Our experiments show that the adversarial training allows our model to obtain state-of-the-art performances across various saliency metrics.},
  	pages = {1--2},
  	journaltitle = {{arXiv}},
  	author = {Pan, Junting and Canton-Ferrer, Cristian and {McGuinness}, Kevin and O'Connor, Noel E. and Torres, Jordi and Sayrol, Elisa and Giro-I-Nieto, Xavier},
  	date = {2017},
  	file = {Pan et al_2017_SalGAN.pdf:/Users/jaime/Zotero/storage/UMM2J462/Pan et al_2017_SalGAN.pdf:application/pdf},
  }

  @article{Piao2019,
  	title = {Depth-induced multi-scale recurrent attention network for saliency detection},
  	volume = {2019-Octob},
  	issn = {15505499},
  	doi = {10.1109/ICCV.2019.00735},
  	abstract = {In this work, we propose a novel depth-induced multi-scale recurrent attention network for saliency detection. It achieves dramatic performance especially in complex scenarios. There are three main contributions of our network that are experimentally demonstrated to have significant practical merits. First, we design an effective depth refinement block using residual connections to fully extract and fuse multi-level paired complementary cues from {RGB} and depth streams. Second, depth cues with abundant spatial information are innovatively combined with multi-scale context features for accurately locating salient objects. Third, we boost our model's performance by a novel recurrent attention module inspired by Internal Generative Mechanism of human brain. This module can generate more accurate saliency results via comprehensively learning the internal semantic relation of the fused feature and progressively optimizing local details with memory-oriented scene understanding. In addition, we create a large scale {RGB}-D dataset containing more complex scenarios, which can contribute to comprehensively evaluating saliency models. Extensive experiments on six public datasets and ours demonstrate that our method can accurately identify salient objects and achieve consistently superior performance over 16 state-of-the-art {RGB} and {RGB}-D approaches.},
  	pages = {7253--7262},
  	journaltitle = {Proceedings of the {IEEE} International Conference on Computer Vision},
  	author = {Piao, Yongri and Ji, Wei and Li, Jingjing and Zhang, Miao and Lu, Huchuan},
  	date = {2019},
  	note = {{ISBN}: 9781728148038},
  	file = {Piao et al_2019_Depth-induced multi-scale recurrent attention network for saliency detection.pdf:/Users/jaime/Zotero/storage/9RY29QR6/Piao et al_2019_Depth-induced multi-scale recurrent attention network for saliency detection.pdf:application/pdf},
  }

  @article{Ma2019,
  	title = {Using rgb image as visual input for mapless robot navigation},
  	abstract = {Robot navigation in mapless environment is one of the essential problems and challenges in mobile robots. Deep reinforcement learning is a promising technique to tackle the task of mapless navigation. Since reinforcement learning requires a lot of explorations, it is usually necessary to train the agent in the simulator and then migrate to the real environment. The big reality gap makes {RGB} image, the most common visual format, rarely used. In this paper we present a learning-based mapless motion planner taking {RGB} image as visual input. In the end-to-end navigation network many of the parameters are used to extract visual features. The proposed motion palnner decoupled visual features extracted module from the reinforcement learning network to improve the sample efficiency. Variational Autoencoder ({VAE}) is used to encode the image, and the obtained latent vector is input as low-dimensional visual features into the network together with the target and motion information. We built and released a set of simulation environments for algorithm comparison. In the test environment, the proposed method was compared with the end-to-end network, which proved its effectiveness and efficiency. The source code is available: Https://github.com/marooncn/navbot.},
  	number = {2},
  	journaltitle = {{arXiv}},
  	author = {Ma, Liulong and Liu, Yanjie and Chen, Jiao},
  	date = {2019},
  	file = {Ma et al_2019_Using rgb image as visual input for mapless robot navigation.pdf:/Users/jaime/Zotero/storage/9JU6YGHC/Ma et al_2019_Using rgb image as visual input for mapless robot navigation.pdf:application/pdf},
  }

  @article{Bharadhwaj2018,
  	title = {A data-efficient framework for training and sim-to-real transfer of navigation policies},
  	abstract = {Learning effective visuomotor policies for robots purely from data is challenging, but also appealing since a learning-based system should not require manual tuning or calibration. In the case of a robot operating in a real environment the training process can be costly, time-consuming, and even dangerous since failures are common at the start of training. For this reason, it is desirable to be able to leverage simulation and off-policy data to the extent possible to train the robot. In this work, we introduce a robust framework that plans in simulation and transfers well to the real environment. Our model incorporates a gradient-descent based planning module, which, given the initial image and goal image, encodes the images to a lower dimensional latent state and plans a trajectory to reach the goal. The model, consisting of the encoder and planner modules, is trained through a meta-learning strategy in simulation first. We subsequently perform adversarial domain transfer on the encoder by using a bank of unlabelled but random images from the simulation and real environments to enable the encoder to map images from the real and simulated environments to a similarly distributed latent representation. By fine tuning the entire model (encoder + planner) with far fewer real world expert demonstrations, we show successful planning performances in different navigation tasks.},
  	journaltitle = {{arXiv}},
  	author = {Bharadhwaj, Homanga and Wang, Zihan and Bengio, Yoshua and Paull, Liam},
  	date = {2018},
  	note = {{ISBN}: 9781538660270},
  	keywords = {Learning from Demonstration,Model Learning for Con},
  	file = {Bharadhwaj et al_2018_A data-efficient framework for training and sim-to-real transfer of navigation.pdf:/Users/jaime/Zotero/storage/GC27YJAE/Bharadhwaj et al_2018_A data-efficient framework for training and sim-to-real transfer of navigation.pdf:application/pdf},
  }

  @article{Martinez-Gonzalez2020,
  	title = {{UnrealROX}: an extremely photorealistic virtual reality environment for robotics simulations and synthetic data generation},
  	volume = {24},
  	issn = {14349957},
  	doi = {10.1007/s10055-019-00399-5},
  	abstract = {Data-driven algorithms have surpassed traditional techniques in almost every aspect in robotic vision problems. Such algorithms need vast amounts of quality data to be able to work properly after their training process. Gathering and annotating that sheer amount of data in the real world is a time-consuming and error-prone task. These problems limit scale and quality. Synthetic data generation has become increasingly popular since it is faster to generate and automatic to annotate. However, most of the current datasets and environments lack realism, interactions, and details from the real world. {UnrealROX} is an environment built over Unreal Engine 4 which aims to reduce that reality gap by leveraging hyperrealistic indoor scenes that are explored by robot agents which also interact with objects in a visually realistic manner in that simulated world. Photorealistic scenes and robots are rendered by Unreal Engine into a virtual reality headset which captures gaze so that a human operator can move the robot and use controllers for the robotic hands; scene information is dumped on a per-frame basis so that it can be reproduced offline to generate raw data and ground truth annotations. This virtual reality environment enables robotic vision researchers to generate realistic and visually plausible data with full ground truth for a wide variety of problems such as class and instance semantic segmentation, object detection, depth estimation, visual grasping, and navigation.},
  	pages = {271--288},
  	number = {2},
  	journaltitle = {Virtual Reality},
  	author = {Martinez-Gonzalez, Pablo and Oprea, Sergiu and Garcia-Garcia, Alberto and Jover-Alvarez, Alvaro and Orts-Escolano, Sergio and Garcia-Rodriguez, Jose},
  	date = {2020},
  	keywords = {Robotics, Grasping, Synthetic data},
  	file = {Martinez-Gonzalez et al_2020_UnrealROX.pdf:/Users/jaime/Zotero/storage/BX8M4ZU6/Martinez-Gonzalez et al_2020_UnrealROX.pdf:application/pdf},
  }

  @article{Scheper2017,
  	title = {Abstraction, Sensory-Motor Coordination, and the Reality Gap in Evolutionary Robotics},
  	volume = {23},
  	issn = {1064-5462},
  	url = {https://www.mitpressjournals.org/doi/abs/10.1162/ARTL_a_00227},
  	doi = {10.1162/ARTL_a_00227},
  	abstract = {One of the major challenges of evolutionary robotics is to transfer robot controllers evolved in simulation to robots in the real world. In this article, we investigate abstraction of the sensory inputs and motor actions as a tool to tackle this problem. Abstraction in robots is simply the use of preprocessed sensory inputs and low-level closed-loop control systems that execute higher-level motor commands. To demonstrate the impact abstraction could have, we evolved two controllers with different levels of abstraction to solve a task of forming an asymmetric triangle with a homogeneous swarm of micro air vehicles. The results show that although both controllers can effectively complete the task in simulation, the controller with the lower level of abstraction is not effective on the real vehicle, due to the reality gap. The controller with the higher level of abstraction is, however, effective both in simulation and in reality, suggesting that abstraction can be a useful tool in making evolved behavior robust to the reality gap. Additionally, abstraction aided in reducing the computational complexity of the simulation environment, speeding up the optimization process. Preeminently, we show that the optimized behavior exploits the environment (in this case the identical behavior of the other robots) and performs input shaping to allow the vehicles to fly into and maintain the required formation, demonstrating clear sensory-motor coordination. This shows that the power of the genetic optimization to find complex correlations is not necessarily lost through abstraction as some have suggested.},
  	pages = {124--141},
  	number = {2},
  	journaltitle = {Artificial Life},
  	author = {Scheper, Kirk Y. W. and de Croon, Guido C. H. E.},
  	date = {2017-05},
  	pmid = {23373976},
  	note = {{ISBN}: 1064-5462{\textbackslash}n1530-9185},
  	file = {Scheper_de Croon_2017_Abstraction, Sensory-Motor Coordination, and the Reality Gap in Evolutionary.pdf:/Users/jaime/Zotero/storage/KDDTRNB2/Scheper_de Croon_2017_Abstraction, Sensory-Motor Coordination, and the Reality Gap in Evolutionary.pdf:application/pdf},
  }

  @article{Li2020,
  	title = {Deep reinforcement learning based automatic exploration for navigation in unknown environment},
  	abstract = {This paper investigates the automatic exploration problem under the unknown environment, which is the key point of applying the robotic system to some social tasks. The solution to this problem via stacking decision rules is impossible to cover various environments and sensor properties. Learning based control methods are adaptive for these scenarios. However, these methods are damaged by low learning efficiency and awkward transferability from simulation to reality. In this paper, we construct a general exploration framework via decomposing the exploration process into the decision, planning, and mapping modules, which increases the modularity of the robotic system. Based on this framework, we propose a deep reinforcement learning based decision algorithm which uses a deep neural network to learning exploration strategy from the partial map. The results show that this proposed algorithm has better learning efficiency and adaptability for unknown environments. In addition, we conduct the experiments on the physical robot, and the results suggest that the learned policy can be well transfered from simulation to the real robot.},
  	pages = {1--13},
  	journaltitle = {{arXiv}},
  	author = {Li, Haoran and Zhang, Qichao and Zhao, Dongbin},
  	date = {2020},
  	keywords = {Automatic exploration, Deep reinforcement learning, Optimal decision, Partial observation},
  	file = {Li et al_2020_Deep reinforcement learning based automatic exploration for navigation in.pdf:/Users/jaime/Zotero/storage/R79ZIY8C/Li et al_2020_Deep reinforcement learning based automatic exploration for navigation in.pdf:application/pdf},
  }

  @article{Lobos-Tsunekawa2018,
  	title = {Visual navigation for biped humanoid robots using deep reinforcement learning},
  	volume = {3},
  	issn = {23773766},
  	doi = {10.1109/LRA.2018.2851148},
  	abstract = {In this letter, we propose a map-less visual navigation system for biped humanoid robots, which extracts information from color images to derive motion commands using deep reinforcement learning ({DRL}). The map-less visual navigation policy is trained using the Deep Deterministic Policy Gradients ({DDPG}) algorithm, which corresponds to an actor-critic {DRL} algorithm. The algorithm is implemented using two separate networks, one for the actor and one for the critic, but with similar structures. In addition to convolutional and fully connected layers, Long Short-Term Memory ({LSTM}) layers are included to address the limited observability present in the problem. As a proof of concept, we consider the case of robotic soccer using humanoid {NAO} V5 robots, which have reduced computational capabilities, and low-cost Red - Green - Blue ({RGB}) cameras as main sensors. The use of {DRL} allowed to obtain a complex and high performant policy from scratch, without any prior knowledge of the domain, or the dynamics involved. The visual navigation policy is trained in a robotic simulator and then successfully transferred to a physical robot, where it is able to run in 20 ms, allowing its use in real-time applications.},
  	pages = {3247--3254},
  	number = {4},
  	journaltitle = {{IEEE} Robotics and Automation Letters},
  	author = {Lobos-Tsunekawa, Kenzo and Leiva, Francisco and Ruiz-Del-Solar, Javier},
  	date = {2018},
  	note = {Publisher: {IEEE}},
  	keywords = {deep learning in robotics and automation and human, Visual-based navigation},
  	file = {Lobos-Tsunekawa et al_2018_Visual navigation for biped humanoid robots using deep reinforcement learning.pdf:/Users/jaime/Zotero/storage/H8B8ZQUD/Lobos-Tsunekawa et al_2018_Visual navigation for biped humanoid robots using deep reinforcement learning.pdf:application/pdf},
  }

  @article{Gordon2019,
  	title = {{SplitNet}: Sim2Sim and Task2Task transfer for embodied visual navigation},
  	abstract = {We propose {SplitNet}, a method for decoupling visual perception and policy learning. By incorporating auxiliary tasks and selective learning of portions of the model, we explicitly decompose the learning objectives for visual navigation into perceiving the world and acting on that perception. We show dramatic improvements over baseline models on transferring between simulators, an encouraging step towards Sim2Real. Additionally, {SplitNet} generalizes better to unseen environments from the same simulator and transfers faster and more effectively to novel embodied navigation tasks. Further, given only a small sample from a target domain, {SplitNet} can match the performance of traditional end-to-end pipelines which receive the entire dataset},
  	pages = {1022--1031},
  	journaltitle = {{arXiv}},
  	author = {Gordon, Daniel and Kadian, Abhishek and Parikh, Devi and Hoffman, Judy and Batra, Dhruv},
  	date = {2019},
  	file = {Gordon et al_2019_SplitNet.pdf:/Users/jaime/Zotero/storage/9I22JATU/Gordon et al_2019_SplitNet.pdf:application/pdf},
  }

  @article{Chaffre2019,
  	title = {Sim-to-Real Transfer with Incremental Environment Complexity for Reinforcement Learning of Depth-Based Robot Navigation},
  	author = {Chaffre, Thomas and Moras, Julien and Chan-hon-tong, Adrien and Marzat, Julien},
  	date = {2019},
  	file = {Chaffre et al_2019_Sim-to-Real Transfer with Incremental Environment Complexity for Reinforcement.pdf:/Users/jaime/Zotero/storage/ETSTB7QH/Chaffre et al_2019_Sim-to-Real Transfer with Incremental Environment Complexity for Reinforcement.pdf:application/pdf},
  }

  @article{Truong2020,
  	title = {Bi-directional Domain Adaptation for Sim2Real Transfer of Embodied Navigation Agents},
  	volume = {1},
  	author = {Truong, Joanne and Chernova, Sonia and Batra, Dhruv},
  	date = {2020},
  	file = {Truong et al_2020_Bi-directional Domain Adaptation for Sim2Real Transfer of Embodied Navigation.pdf:/Users/jaime/Zotero/storage/UFZGI5SW/Truong et al_2020_Bi-directional Domain Adaptation for Sim2Real Transfer of Embodied Navigation.pdf:application/pdf},
  }

  @article{Ligot2019,
  	title = {Simulation-only experiments to mimic the effects of the reality gap in the automatic design of robot swarms},
  	issn = {1935-3820},
  	url = {https://doi.org/10.1007/s11721-019-00175-w},
  	doi = {10.1007/s11721-019-00175-w},
  	journaltitle = {Swarm Intelligence},
  	author = {Ligot, Antoine and Birattari, Mauro},
  	date = {2019},
  	note = {Publisher: Springer {US}},
  	keywords = {reality gap, and performed by antoine, and revised by the, automatic design, by mauro birattari, by the two authors, drafted by antoine ligot, ligot, swarm robotics, Swarm robotics,Automatic design,Reality gap, the article was, the experiments were conceived, the research was directed, two authors},
  	file = {Ligot_Birattari_2019_Simulation-only experiments to mimic the effects of the reality gap in the.pdf:/Users/jaime/Zotero/storage/B45KFMP3/Ligot_Birattari_2019_Simulation-only experiments to mimic the effects of the reality gap in the.pdf:application/pdf},
  }

  @article{Marchesini2020,
  	title = {Discrete Deep Reinforcement Learning for Mapless Navigation},
  	pages = {10688--10694},
  	author = {Marchesini, Enrico and Farinelli, Alessandro},
  	date = {2020},
  	note = {{ISBN}: 9781728173955},
  	file = {Marchesini_Farinelli_2020_Discrete Deep Reinforcement Learning for Mapless Navigation.pdf:/Users/jaime/Zotero/storage/C6WYBQBJ/Marchesini_Farinelli_2020_Discrete Deep Reinforcement Learning for Mapless Navigation.pdf:application/pdf},
  }

  @book{Prakash2018,
  	title = {Structured domain randomization: Bridging the reality gap by context-aware synthetic data},
  	isbn = {978-1-5386-6027-0},
  	abstract = {We present structured domain randomization ({SDR}), a variant of domain randomization ({DR}) that takes into account the structure and context of the scene. In contrast to {DR}, which places objects and distractors randomly according to a uniform probability distribution, {SDR} places objects and distractors randomly according to probability distributions that arise from the specific problem at hand. In this manner, {SDR}-generated imagery enables the neural network to take the context around an object into consideration during detection. We demonstrate the power of {SDR} for the problem of 2D bounding box car detection, achieving competitive results on real data after training only on synthetic data. On the {KITTI} easy, moderate, and hard tasks, we show that {SDR} outperforms other approaches to generating synthetic data ({VKITTI}, Sim 200k, or {DR}), as well as real data collected in a different domain ({BDD}100K). Moreover, synthetic {SDR} data combined with real {KITTI} data outperforms real {KITTI} data alone.},
  	pagetotal = {7249-7255},
  	author = {Prakash, Aayush and Cameracci, Eric and Boochoon, Shaad and State, Gavriel and Brophy, Mark and Shapira, Omer and Acuna, David and Birchfield, Stan},
  	date = {2018},
  	note = {Publication Title: {arXiv}},
  	file = {Prakash et al_2018_Structured domain randomization.pdf:/Users/jaime/Zotero/storage/S7LQRAM8/Prakash et al_2018_Structured domain randomization.pdf:application/pdf},
  }

  @article{Languages1998,
  	title = {Bridging the Gap : The Use of Pathfinder Networks in Visual Navigation},
  	pages = {267--286},
  	author = {Languages, Visual and Systems, Information and Ub, Uxbridge},
  	date = {1998},
  	keywords = {virtual reality, analysis, cognitive map, generalised similarity, pathfinder network, user-interface design, visualisation},
  	file = {Languages et al_1998_Bridging the Gap.pdf:/Users/jaime/Zotero/storage/WRZ2ABAA/Languages et al_1998_Bridging the Gap.pdf:application/pdf},
  }

  @article{Benhamou2018,
  	title = {A discrete version of {CMA}-{ES}},
  	issn = {1556-5068},
  	doi = {10.2139/ssrn.3307212},
  	abstract = {Modern machine learning uses more and more advanced optimization techniques to find optimal hyper parameters. Whenever the objective function is non-convex, non continuous and with potentially multiple local minima, standard gradient descent optimization methods fail. A last resource and very different method is to assume that the optimum(s), not necessarily unique, is/are distributed according to a distribution and iteratively to adapt the distribution according to tested points. These strategies originated in the early 1960s, named Evolution Strategy ({ES}) have culminated with the {CMA}-{ES} (Covariance Matrix Adaptation) {ES}. It relies on a multi variate normal distribution and is supposed to be state of the art for general optimization program. However, it is far from being optimal for discrete variables. In this paper, we extend the method to multivariate binomial correlated distributions. For such a distribution, we show that it shares similar features to the multi variate normal: independence and correlation is equivalent and correlation is efficiently modeled by interaction between different variables. We discuss this distribution in the framework of the exponential family. We prove that the model can estimate not only pairwise interactions among the two variables but also is capable of modeling higher order interactions. This allows creating a version of {CMA} {ES} that can accomodate efficiently discrete variables. We provide the corresponding algorithm and conclude.},
  	journaltitle = {{arXiv}},
  	author = {Benhamou, Eric and Atif, Jamal and Laraki, Rida},
  	date = {2018},
  	file = {Benhamou et al_2018_A discrete version of CMA-ES.pdf:/Users/jaime/Zotero/storage/QS54IRJX/Benhamou et al_2018_A discrete version of CMA-ES.pdf:application/pdf},
  }

  @article{Simons1999,
  	title = {Gorillas in our midst: sustained inattentional blindness for dynamic events},
  	volume = {28},
  	doi = {10.1068/p281059},
  	abstract = {With each eye fixation, we experience a richly detailed visual world. Yet recent work on visual integration and change direction reveals that we are surprisingly unaware of the details of our environment from one view to the next: we often do not detect large changes to objects and scenes (`change blindness'). Furthermore, without attention, we may not even perceive objects (`inattentional blindness'). Taken together, these findings suggest that we perceive and remember only those objects and details that receive focused attention. In this paper, we briefly review and discuss evidence for these cognitive forms of `blindness'. We then present a new study that builds on classic studies of divided visual attention to examine inattentional blindness for complex objects and events in dynamic scenes. Our results suggest that the likelihood of noticing an unexpected object depends on the similarity of that object to other objects in the display and on how difficult the priming monitoring task is. Interestingly, spatial proximity of the critical unattended object to attended locations does not appear to affect detection, suggesting that observers attend to objects and events, not spatial positions. We discuss the implications of these results for visual representations and awareness of our visual environment.},
  	pages = {1059--74},
  	number = {9},
  	journaltitle = {Perception},
  	author = {Simons, Daniel J and Chabris, Christopher F},
  	date = {1999},
  	file = {Simons_Chabris_1999_Gorillas in our midst.pdf:/Users/jaime/Zotero/storage/RNQF2Y3N/Simons_Chabris_1999_Gorillas in our midst.pdf:application/pdf},
  }

  @book{Landy1995,
  	title = {Exploratory Vision: The Active Eye},
  	isbn = {0-387-94563-6},
  	url = {http://www.amazon.com/dp/0387945636},
  	author = {Landy, Michael S. and Maloney, Laurence T. and Pavel, Misha},
  	date = {1995},
  }

  @article{Pathak2017,
  	title = {Curiosity-driven Exploration by Self-supervised Prediction},
  	author = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A and Darrell, Trevor},
  	date = {2017},
  	file = {Pathak et al_2017_Curiosity-driven Exploration by Self-supervised Prediction.pdf:/Users/jaime/Zotero/storage/EKRSGVUG/Pathak et al_2017_Curiosity-driven Exploration by Self-supervised Prediction.pdf:application/pdf},
  }

  @article{Martyushev2006,
  	title = {Maximum entropy production principle in physics, chemistry and biology},
  	volume = {426},
  	issn = {03701573},
  	doi = {10.1016/j.physrep.2005.12.001},
  	abstract = {The tendency of the entropy to a maximum as an isolated system is relaxed to the equilibrium (the second law of thermodynamics) has been known since the mid-19th century. However, independent theoretical and applied studies, which suggested the maximization of the entropy production during nonequilibrium processes (the so-called maximum entropy production principle, {MEPP}), appeared in the 20th century. Publications on this topic were fragmented and different research teams, which were concerned with this principle, were unaware of studies performed by other scientists. As a result, the recognition and the use of {MEPP} by a wider circle of researchers were considerably delayed. The objectives of the present review consist in summation and analysis of studies dealing with {MEPP}. The first part of the review is concerned with the thermodynamic and statistical basis of the principle (including the relationship of {MEPP} with the second law of thermodynamics and Prigogine's principle). Various existing applications of the principle to analysis of nonequilibrium systems will be discussed in the second part. © 2005 Elsevier B.V. All rights reserved.},
  	pages = {1--45},
  	number = {1},
  	journaltitle = {Physics Reports},
  	author = {Martyushev, L M and Seleznev, V D},
  	date = {2006},
  	keywords = {Maximum entropy production principle ({MEPP}), {MEPP} applications, Ziegler's and Prigogine's principles},
  	file = {Martyushev_Seleznev_2006_Maximum entropy production principle in physics, chemistry and biology.pdf:/Users/jaime/Zotero/storage/PS5GRG4G/Martyushev_Seleznev_2006_Maximum entropy production principle in physics, chemistry and biology.pdf:application/pdf},
  }

  @article{Savinov2018,
  	title = {Episodic curiosity through reachability},
  	issn = {23318422},
  	abstract = {Rewards are sparse in the real world and most of today's reinforcement learning algorithms struggle with such sparsity. One solution to this problem is to allow the agent to create rewards for itself - thus making rewards dense and more suitable for learning. In particular, inspired by curious behaviour in animals, observing something novel could be rewarded with a bonus. Such bonus is summed up with the real task reward - making it possible for {RL} algorithms to learn from the combined reward. We propose a new curiosity method which uses episodic memory to form the novelty bonus. To determine the bonus, the current observation is compared with the observations in memory. Crucially, the comparison is done based on how many environment steps it takes to reach the current observation from those in memory - which incorporates rich information about environment dynamics. This allows us to overcome the known “couch-potato” issues of prior work - when the agent finds a way to instantly gratify itself by exploiting actions which lead to hardly predictable consequences. We test our approach in visually rich 3D environments in {VizDoom}, {DMLab} and {MuJoCo}. In navigational tasks from {VizDoom} and {DMLab}, our agent outperforms the state-of-the-art curiosity method {ICM}. In {MuJoCo}, an ant equipped with our curiosity module learns locomotion out of the first-person-view curiosity only. The code is available at https://github.com/google-research/episodic-curiosity.},
  	pages = {1--20},
  	journaltitle = {{arXiv}},
  	author = {Savinov, Nikolay and Raichuk, Anton and Marinier, Raphaël and Vincent, Damien and Pollefeys, Marc and Lillicrap, Timothy and Gelly, Sylvain},
  	date = {2018},
  }

  @book{Han2021,
  	title = {Deep Learning – Based Scene Simplification for Bionic Vision},
  	volume = {1},
  	publisher = {Association for Computing Machinery},
  	author = {Han, Nicole and Klein, Devi and Beyeler, Michael},
  	date = {2021},
  	doi = {10.6084/m9.figshare.13652927},
  	note = {Publication Title: Augmented Humans '21
  Issue: 1},
  	keywords = {acm reference format, deep learn-, ing, retinal implant, retinal implant, visually impaired, scene simplifi, scene simplification, simulated prosthetic vision, vision augmentation, visually impaired},
  	file = {Han et al_2021_Deep Learning – Based Scene Simplification for Bionic Vision.pdf:/Users/jaime/Zotero/storage/MPZMUKMS/Han et al_2021_Deep Learning – Based Scene Simplification for Bionic Vision.pdf:application/pdf},
  }

  @article{Meyes2019,
  	title = {Ablation studies in artificial neural networks},
  	issn = {23318422},
  	abstract = {Ablation studies have been widely used in the field of neuroscience to tackle complex biological systems such as the extensively studied Drosophila central nervous system, the vertebrate brain and more interestingly and most delicately, the human brain. In the past, these kinds of studies were utilized to uncover structure and organization in the brain, i.e. a mapping of features inherent to external stimuli onto different areas of the neocortex. considering the growth in size and complexity of state-of-the-art artificial neural networks ({ANNs}) and the corresponding growth in complexity of the tasks that are tackled by these networks, the question arises whether ablation studies may be used to investigate these networks for a similar organization of their inner representations. In this paper, we address this question and performed two ablation studies in two fundamentally different {ANNs} to investigate their inner representations of two well-known benchmark datasets from the computer vision domain. We found that features distinct to the local and global structure of the data are selectively represented in specific parts of the network. Furthermore, some of these representations are redundant, awarding the network a certain robustness to structural damages. We further determined the importance of specific parts of the network for the classification task solely based on the weight structure of single units. Finally, we examined the ability of damaged networks to recover from the consequences of ablations by means of recovery training. We argue that ablations studies are a feasible method to investigate knowledge representations in {ANNs} and are especially helpful to examine a networks robustness to structural damages, a feature of {ANNs} that will become increasingly important for future safety-critical applications. Our code is publicly available 1 to reproduce our results and build upon them.},
  	pages = {1--19},
  	journaltitle = {{arXiv}},
  	author = {Meyes, Richard and Lu, Melanie and de Puiseau, Constantin Waubert and Meisen, Tobias},
  	date = {2019},
  }

  @article{Montavon2017,
  	title = {Explaining nonlinear classification decisions with deep Taylor decomposition},
  	volume = {65},
  	issn = {00313203},
  	doi = {10.1016/j.patcog.2016.11.008},
  	abstract = {Nonlinear methods such as Deep Neural Networks ({DNNs}) are the gold standard for various challenging machine learning problems such as image recognition. Although these methods perform impressively well, they have a significant disadvantage, the lack of transparency, limiting the interpretability of the solution and thus the scope of application in practice. Especially {DNNs} act as black boxes due to their multilayer nonlinear structure. In this paper we introduce a novel methodology for interpreting generic multilayer neural networks by decomposing the network classification decision into contributions of its input elements. Although our focus is on image classification, the method is applicable to a broad set of input data, learning tasks and network architectures. Our method called deep Taylor decomposition efficiently utilizes the structure of the network by backpropagating the explanations from the output to the input layer. We evaluate the proposed method empirically on the {MNIST} and {ILSVRC} data sets.},
  	pages = {211--222},
  	journaltitle = {Pattern Recognition},
  	author = {Montavon, Grégoire and Lapuschkin, Sebastian and Binder, Alexander and Samek, Wojciech and Müller, Klaus Robert},
  	date = {2017},
  	keywords = {Deep neural networks, Heatmapping, Image recognition, Relevance propagation, Taylor decomposition},
  }

  @article{Pettigrew2010,
  	title = {Depth Correction : Methods for Approximating Depth Information in Web Camera Depth Maps},
  	author = {Pettigrew, L and Green, R},
  	date = {2010},
  	note = {{ISBN}: 9781424496310},
  	keywords = {imaging, stereo image processing, stereo vision},
  	file = {Pettigrew_Green_2010_Depth Correction.pdf:/Users/jaime/Zotero/storage/EYW7MFGK/Pettigrew_Green_2010_Depth Correction.pdf:application/pdf},
  }

  @article{Milani2016,
  	title = {Signal Processing : Image Communication Correction and interpolation of depth maps from structured light infrared sensors},
  	volume = {41},
  	issn = {0923-5965},
  	url = {http://dx.doi.org/10.1016/j.image.2015.11.008},
  	doi = {10.1016/j.image.2015.11.008},
  	pages = {28--39},
  	journaltitle = {Signal Processing : Image Communication},
  	author = {Milani, Simone and Calvagno, Giancarlo},
  	date = {2016},
  	note = {Publisher: Elsevier},
  	keywords = {3D acquisition, {DIBR} denoising, Infrared sensor, {MS} Kinect, Structured light camera},
  }

  @article{Garro2013,
  	title = {Edge-preserving interpolation of depth data exploiting color information},
  	doi = {10.1007/s12243-013-0389-0},
  	pages = {597--613},
  	author = {Garro, Valeria and Dal, Carlo and Pietro, Mutto and Cortelazzo, Guido M},
  	date = {2013},
  	keywords = {calibration, depth map, interpolation, super resolution, time of flight},
  	file = {Garro et al_2013_Edge-preserving interpolation of depth data exploiting color information.pdf:/Users/jaime/Zotero/storage/3KFBJRWF/Garro et al_2013_Edge-preserving interpolation of depth data exploiting color information.pdf:application/pdf},
  }

  @article{Wu2015,
  	title = {Shading Correction of Camera Captured Document Image with Depth Map Information},
  	volume = {9395},
  	doi = {10.1117/12.2083490},
  	pages = {1--7},
  	author = {Wu, Chyuan-tyng and Allebach, Jan P},
  	date = {2015},
  	keywords = {depth map, document images, shading correction},
  }

  @online{Juliani2018,
  	title = {Maximum Entropy Policies in Reinforcement Learning \& Everyday Life},
  	url = {https://awjuliani.medium.com/maximum-entropy-policies-in-reinforcement-learning-everyday-life-f5a1cc18d32d#:~:text=lives as well.-,In maximum entropy RL%2C the basic principle is that optimal,the behavior of artificial agents.},
  	author = {Juliani, Arthur},
  	date = {2018},
  }

  @article{Oliveira2011,
  	title = {Domain Transform for Edge-Aware Image and Video Processing},
  	volume = {1},
  	number = {212},
  	author = {Oliveira, Manuel M},
  	date = {2011},
  	keywords = {anisotropic diffusion, bilateral filter, domain transform, domain transform, edge-preserving filtering, aniso, edge-preserving filtering},
  	file = {Oliveira_2011_Domain Transform for Edge-Aware Image and Video Processing.pdf:/Users/jaime/Zotero/storage/B3AF9LYI/Oliveira_2011_Domain Transform for Edge-Aware Image and Video Processing.pdf:application/pdf},
  }

  @online{Pollan2021,
  	title = {The efficiency curse},
  	url = {https://michaelpollan.com/articles-archive/the-efficiency-curse/},
  	author = {Pollan, Michael},
  	urldate = {2021-03-04},
  	date = {2021},
  }

  @article{Man2019,
  	title = {{GroundNet}: Monocular ground plane normal estimation with geometric consistency},
  	doi = {10.1145/3343031.3351068},
  	abstract = {We focus on estimating the 3D orientation of the ground plane from a single image. We formulate the problem as an inter-mingled multi-task prediction problem by jointly optimizing for pixel-wise surface normal direction, ground plane segmentation, and depth estimates. Specifically, our proposed model, {GroundNet}, first estimates the depth and surface normal in two separate streams, from which two ground plane normals are then computed deterministically. To leverage the geometric correlation between depth and normal, we propose to add a consistency loss on top of the computed ground plane normals. In addition, a ground segmentation stream is used to isolate the ground regions so that we can selectively back-propagate parameter updates through only the ground regions in the image. Our method achieves the top-ranked performance on ground plane normal estimation and horizon line detection on the real-world outdoor datasets of {ApolloScape} and {KITTI}, improving the performance of previous art by up to 17.7\% relatively.},
  	pages = {2170--2178},
  	issue = {May 2019},
  	journaltitle = {{MM} 2019 - Proceedings of the 27th {ACM} International Conference on Multimedia},
  	author = {Man, Yunze and Weng, Xinshuo and Li, Xi and Kitani, Kris},
  	date = {2019},
  	note = {{ISBN}: 9781450368896},
  	keywords = {Ground plane normal estimation, Horizon line detection, Monocular vision, Multitask learning, Self-supervised learning via consistency},
  	file = {Man et al_2019_GroundNet.pdf:/Users/jaime/Zotero/storage/6V85IGH7/Man et al_2019_GroundNet.pdf:application/pdf},
  }

  @article{Dragon2014,
  	title = {Ground plane estimation using a Hidden Markov Model},
  	issn = {10636919},
  	doi = {10.1109/CVPR.2014.442},
  	abstract = {We focus on the problem of estimating the ground plane orientation and location in monocular video sequences from a moving observer. Our only assumptions are that the 3D ego motion t and the ground plane normal n are orthogonal, and that n and t are smooth over time. We formulate the problem as a state-continuous Hidden Markov Model ({HMM}) where the hidden state contains t and n and may be estimated by sampling and decomposing homographies. We show that using blocked Gibbs sampling, we can infer the hidden state with high robustness towards outliers, drifting trajectories, rolling shutter and an imprecise intrinsic calibration. Since our approach does not need any initial orientation prior, it works for arbitrary camera orientations in which the ground is visible.},
  	pages = {4026--4033},
  	journaltitle = {Proceedings of the {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition},
  	author = {Dragon, Ralf and Van Gool, Luc},
  	date = {2014},
  	note = {{ISBN}: 9781479951178},
  	keywords = {ground plane, hidden markov model, visual gyroscope, visual odometry},
  	file = {Dragon_Van Gool_2014_Ground plane estimation using a Hidden Markov Model.pdf:/Users/jaime/Zotero/storage/XH88EEKC/Dragon_Van Gool_2014_Ground plane estimation using a Hidden Markov Model.pdf:application/pdf},
  }

  @article{Suzuki2006,
  	title = {Omnidirectional active vision for evolutionary car driving},
  	abstract = {We describe a set of simulations to evolve omnidirectional active vision, an artificial retina scanning over images taken via an omnidirectional camera, being applied to a car driving task. While the retina can immediately access features in any direction, it is asked to select behaviorally-relevant features so as to drive the car on the road. Neural controllers which direct both the retinal movement and the system behavior, i.e., the speed and the steering angle of the car, are tested in three different circuits and developed through artificial evolution. We show that the evolved retina moving over the omnidirectional image successfully detects the task-relevant visual features so as to drive the car on the road. Behavioral analysis illustrates its effective strategy in algorithmic, computational, and memory resources. © 2006 The authors.},
  	pages = {153--161},
  	journaltitle = {Intelligent Autonomous Systems 9, {IAS} 2006},
  	author = {Suzuki, Mototaka and Van Der Blij, Jacob and Floreano, Dario},
  	date = {2006},
  	note = {{ISBN}: 1586035959},
  	keywords = {Neural networks, Active vision, Artificial evolution, Mobile robots, Omnidirectional camera},
  	file = {Suzuki et al_2006_Omnidirectional active vision for evolutionary car driving.pdf:/Users/jaime/Zotero/storage/VZBFAC9H/Suzuki et al_2006_Omnidirectional active vision for evolutionary car driving.pdf:application/pdf},
  }

  @article{Mitchell1993,
  	title = {The "Edge of Chaos": A Re-Examination},
  	author = {Mitchell, Melanie},
  	date = {1993},
  	file = {Mitchell_1993_the “ Edge of Chaos ”.pdf:/Users/jaime/Zotero/storage/U3UCJNSD/Mitchell_1993_the “ Edge of Chaos ”.pdf:application/pdf},
  }

  @article{Gare2000,
  	title = {Systems Theory and Complexity: Introduction},
  	volume = {6},
  	issn = {1085-5661},
  	doi = {10.1080/10855660020020221},
  	abstract = {In this paper the central ideas and history of complexity theory and systems theory are described. It is shown how these theories lend themselves to different interpretations, and different interpretations lead to different political conclusions.},
  	pages = {327--339},
  	number = {3},
  	journaltitle = {Democracy \& Nature},
  	author = {Gare, Arran},
  	date = {2000},
  	file = {Gare_2000_Systems Theory and Complexity.pdf:/Users/jaime/Zotero/storage/LXIW64MQ/Gare_2000_Systems Theory and Complexity.pdf:application/pdf},
  }

  @article{Hird2012,
  	title = {Knowing Waste: Towards an Inhuman Epistemology},
  	volume = {26},
  	issn = {02691728},
  	doi = {10.1080/02691728.2012.727195},
  	abstract = {Ten years after the publication of the special issue of Social Epistemology on feminist epistemology, this paper explores recent feminist interest in the inhuman. Feminist science studies, cultural studies, philosophy and environmental studies all build on the important work feminist epistemology has done to bring to the fore questions of feminist empiricism, situated knowledges and knowing as an intersubjective activity. Current research in feminist theory is expanding this epistemological horizon to consider the possibility of an inhuman epistemology. This paper explores these developments through the subject of waste. Waste, as both an epistemological and material phenomenon, invites timely questions about possibilities for acknowledging an inhuman epistemology. These questions appear to be particularly urgent from an environmental perspective. © 2012 Copyright Taylor and Francis Group, {LLC}.},
  	pages = {453--469},
  	number = {3},
  	journaltitle = {Social Epistemology},
  	author = {Hird, Myra J.},
  	date = {2012},
  	keywords = {Environment, Feminist Epistemology, Inhuman, Waste},
  	file = {Hird_2012_Knowing Waste.pdf:/Users/jaime/Zotero/storage/5LTEDW8U/Hird_2012_Knowing Waste.pdf:application/pdf},
  }

  @article{Buterin2018,
  	title = {A Flexible Design for Funding Public Goods},
  	url = {http://arxiv.org/abs/1809.06421},
  	abstract = {We propose a design for philanthropic or publicly-funded seeding to allow (near) optimal provision of a decentralized, self-organizing ecosystem of public goods. The concept extends ideas from Quadratic Voting to a funding mechanism for endogenous community formation. Individuals make public goods contributions to projects of value to them. The amount received by the project is (proportional to) the square of the sum of the square roots of contributions received. Under the "standard model" this yields first best public goods provision. Variations can limit the cost, help protect against collusion and aid coordination. We discuss applications to campaign finance, open source software ecosystems, news media finance and urban public projects. More broadly, we offer a resolution to the classic liberal-communitarian debate in political philosophy by providing neutral and non-authoritarian rules that nonetheless support collective organization.},
  	author = {Buterin, Vitalik and Hitzig, Zoe and Weyl, E. Glen},
  	date = {2018},
  	keywords = {free rider problem, mechanism design, public goods},
  	file = {Buterin et al_2018_A Flexible Design for Funding Public Goods.pdf:/Users/jaime/Zotero/storage/6TIBEHEP/Buterin et al_2018_A Flexible Design for Funding Public Goods.pdf:application/pdf},
  }

  @article{Jameson2001,
  	title = {Postmodernism and Consumer Society},
  	doi = {10.1057/978-1-137-04505-8_3},
  	abstract = {The essay entitled “Postmodernism and consumer society” by Fredric Jameson, attempts to clarify the concept of postmodernism. Jameson’s goal in this essay is to show how postmodernism is opposed to modernism in not just themes of art and literature, but also how these differences show themselves in the general culture.},
  	pages = {22--36},
  	journaltitle = {Postmodern Debates},
  	author = {Jameson, Fredric},
  	date = {2001},
  	file = {Jameson_2001_Postmodernism and Consumer Society.pdf:/Users/jaime/Zotero/storage/2XMAJEKH/Jameson_2001_Postmodernism and Consumer Society.pdf:application/pdf},
  }

  @article{Berg2019,
  	title = {Capitalism after Satoshi: Blockchains, dehierarchicalisation, innovation policy, and the regulatory state},
  	volume = {9},
  	issn = {2045211X},
  	doi = {10.1108/JEPP-03-2019-0012},
  	abstract = {Purpose: The purpose of this paper is to explore the long-run economic structure and economic policy consequences of wide-spread blockchain adoption. Design/methodology/approach: The approach uses institutional, organisational and evolutionary economic theory to predict consequences of blockchain innovation for economic structure (dehierarchicalisation) and then to further predict the effect of that structural change on the demand for economic policy. Findings: The paper makes two key predictions. First, that blockchain adoption will cause both market disintermediation and organisational dehierarchicalisation. And second, that these structural changes will unwind some of the rationale for economic policy developed through the twentieth century that sought to control the effects of market power and organisational hierarchy. Research limitations/implications: The core implication that the theoretical prediction made in this paper is that wide-spread blockchain technology adoption could reduce the need for counter-veiling economic policy, and therefore limiting the role of government. Originality/value: The paper takes a standard prediction made about blockchain adoption, namely disintermediation (or growth of markets), and extends it to point out that the same effect will occur to organisations. It then notes that much of the rationale for economic policy, and especially industry and regulatory policy through the twentieth century was justified in order to control economic power created by hierarchical organisations. The surprising implication, then, is that blockchain adoption weakens the rationale for such economic policy. This reveals the long-run relationship between digital technological innovation and the regulatory state.},
  	pages = {152--164},
  	number = {2},
  	journaltitle = {Journal of Entrepreneurship and Public Policy},
  	author = {Berg, Chris and Davidson, Sinclair and Potts, Jason},
  	date = {2019},
  	keywords = {Blockchain, Capitalism, Economic evolution, Institutions},
  	file = {Berg et al_2019_Capitalism after Satoshi.pdf:/Users/jaime/Zotero/storage/VP9MC64G/Berg et al_2019_Capitalism after Satoshi.pdf:application/pdf},
  }

  @article{Simon2012,
  	title = {The Architecture of Complexity},
  	volume = {106},
  	doi = {10.1007/978-3-642-27922-5_23},
  	abstract = {What is the contingency factor here? Integration - according to Simon, to study organizations means to study coordination},
  	pages = {335--361},
  	number = {6},
  	journaltitle = {The Roots of Logistics},
  	author = {Simon, Herbert A.},
  	date = {2012},
  	file = {Simon_2012_The Architecture of Complexity.pdf:/Users/jaime/Zotero/storage/AFRBGE8M/Simon_2012_The Architecture of Complexity.pdf:application/pdf},
  }

  @misc{Meadows1999,
  	title = {Leverage Points: Places to Intervene in a System},
  	url = {http://donellameadows.org/archives/leverage-points-places-to-intervene-in-a-system/},
  	author = {Meadows, Donella},
  	date = {1999},
  	file = {Leverage_Points.pdf:/Users/jaime/Zotero/storage/DXS25AS7/Leverage_Points.pdf:application/pdf},
  }

  @article{Nowak2006,
  	title = {Five rules for the evolution of cooperation},
  	volume = {314},
  	issn = {00368075},
  	doi = {10.1126/science.1133755},
  	abstract = {Cooperation is needed for evolution to construct new levels of organization. Genomes, cells, multicellular organisms, social insects, and human society are all based on cooperation. Cooperation means that selfish replicators forgo some of their reproductive potential to help one another. But natural selection implies competition and therefore opposes cooperation unless a specific mechanism is at work. Here I discuss five mechanisms for the evolution of cooperation: kin selection, direct reciprocity, indirect reciprocity, network reciprocity, and group selection. For each mechanism, a simple rule is derived that specifies whether natural selection can lead to cooperation.},
  	pages = {1560--1563},
  	number = {5805},
  	journaltitle = {Science},
  	author = {Nowak, Martin A.},
  	date = {2006},
  	pmid = {17158317},
  	file = {Nowak_2006_Five rules for the evolution of cooperation.pdf:/Users/jaime/Zotero/storage/VFR4L89S/Nowak_2006_Five rules for the evolution of cooperation.pdf:application/pdf},
  }

  @article{Gare2008,
  	title = {1 Process Philosophy and Ecological Ethics Arran Gare},
  	pages = {1--13},
  	author = {Gare, Arran},
  	date = {2008},
  	keywords = {political philosophy, process philosophy, alfred north whitehead, born, christopher alexander, ecological ethics, environmental movement, max},
  	file = {Gare_2008_1 Process Philosophy and Ecological Ethics Arran Gare.pdf:/Users/jaime/Zotero/storage/KSZDSWBQ/Gare_2008_1 Process Philosophy and Ecological Ethics Arran Gare.pdf:application/pdf},
  }

  @article{Saari1995,
  	title = {Mathematical complexity of simple economics},
  	volume = {42},
  	pages = {222--230},
  	number = {2},
  	journaltitle = {Notices of the {AMS}},
  	author = {Saari, Donald},
  	date = {1995},
  	keywords = {chaotic dynamics, dynamical systems, Key words and phrases. Price dynamics, price eq, Walras},
  	file = {Saari_1995_Mathematical complexity of simple economics.pdf:/Users/jaime/Zotero/storage/9LDKXCD7/Saari_1995_Mathematical complexity of simple economics.pdf:application/pdf},
  }

  @article{Chen2021,
  	title = {Stochastic sensitivity and dynamical complexity of newsvendor models subject to trade credit},
  	volume = {181},
  	issn = {03784754},
  	url = {https://doi.org/10.1016/j.matcom.2020.10.006},
  	doi = {10.1016/j.matcom.2020.10.006},
  	abstract = {To describe the dynamical ordering behaviour of a risk-averse newsvendor with trade credit in supply chain, two models are formulated for two scenarios based on the bounded rationality rule. Assuming the newsvendor chooses the ordering quantity by adaptively adjusting the choice of previous period, we first develop a deterministic model. Then, mathematical analysis of the model is provided, such as the stability of equilibrium points, Neimark–Sacker bifurcation and normal form. Besides, the diagrams such as bifurcation to chaos, Lyapunov exponent, attractors and time series are illustrated numerically. It is then followed by a stochastic model to reflect the unexpected random noises from external operational environment and the perturbation on the newsvendor behaviour. The sensitivity of equilibria, the confidence ellipse and the confidence band are investigated by utilising Stochastic Sensitivity Function technique. Analysis on stochastic sensitivity shows that the retailer's dynamical ordering exhibits more complex behaviour than the deterministic counterpart.},
  	pages = {471--486},
  	journaltitle = {Mathematics and Computers in Simulation},
  	author = {Chen, Jianxin and Zhang, Tonghua and Zhou, Yong wu},
  	date = {2021},
  	note = {Publisher: Elsevier B.V.},
  	keywords = {Conditional-Value-at Risk ({CVaR}) criterion, Dynamical ordering model, Supply chain financing},
  	file = {Chen et al_2021_Stochastic sensitivity and dynamical complexity of newsvendor models subject to.pdf:/Users/jaime/Zotero/storage/VW7LFKZA/Chen et al_2021_Stochastic sensitivity and dynamical complexity of newsvendor models subject to.pdf:application/pdf},
  }

  @article{Orsini2020,
  	title = {Forum: Complex systems and international governance},
  	volume = {22},
  	issn = {14682486},
  	doi = {10.1093/isr/viz005},
  	abstract = {This collection of essays brings together scholars from various disciplinary backgrounds, based on three continents, with different theoretical and methodological interests but all active on the topic of complex systems as applied to international relations. They investigate how complex systems have been and can be applied in practice and what differences it makes for the study of international affairs. Two important threads link all the contributions: (i) To which extent is this approach promising to understand global governance dynamics? (ii) How can this be implemented in practice?},
  	pages = {1008--1038},
  	number = {4},
  	journaltitle = {International Studies Review},
  	author = {Orsini, Amandine and Le Prestre, Philippe and Haas, Peter M. and Brosig, Malte and Pattberg, Philipp and Widerberg, Oscar and Gomez-Mera, Laura and Morin, Jean Frédéric and Harrison, Neil E. and Geyer, Robert and Chandler, David},
  	date = {2020},
  	keywords = {Complex systems, Global governance},
  	file = {Orsini et al_2020_Forum.pdf:/Users/jaime/Zotero/storage/NNKPEA62/Orsini et al_2020_Forum.pdf:application/pdf},
  }

  @article{Ueda2020,
  	title = {Illuminated Focus: Vision Augmentation using Spatial Defocusing via Focal Sweep Eyeglasses and High-Speed Projector},
  	volume = {26},
  	issn = {19410506},
  	doi = {10.1109/TVCG.2020.2973496},
  	abstract = {Aiming at realizing novel vision augmentation experiences, this paper proposes the {IlluminatedFocus} technique, which spatially defocuses real-world appearances regardless of the distance from the user's eyes to observed real objects. With the proposed technique, a part of a real object in an image appears blurred, while the fine details of the other part at the same distance remain visible. We apply Electrically Focus-Tunable Lenses ({ETL}) as eyeglasses and a synchronized high-speed projector as illumination for a real scene. We periodically modulate the focal lengths of the glasses (focal sweep) at more than 60 Hz so that a wearer cannot perceive the modulation. A part of the scene to appear focused is illuminated by the projector when it is in focus of the user's eyes, while another part to appear blurred is illuminated when it is out of the focus. As the basis of our spatial focus control, we build mathematical models to predict the range of distance from the {ETL} within which real objects become blurred on the retina of a user. Based on the blur range, we discuss a design guideline for effective illumination timing and focal sweep range. We also model the apparent size of a real scene altered by the focal length modulation. This leads to an undesirable visible seam between focused and blurred areas. We solve this unique problem by gradually blending the two areas. Finally, we demonstrate the feasibility of our proposal by implementing various vision augmentation applications.},
  	pages = {2051--2061},
  	number = {5},
  	journaltitle = {{IEEE} Transactions on Visualization and Computer Graphics},
  	author = {Ueda, Tatsuyuki and Iwai, Daisuke and Hiraki, Takefumi and Sato, Kosuke},
  	date = {2020},
  	pmid = {32078550},
  	keywords = {depth-of-field, focal sweep, high-speed projection, spatial augmented reality, spatial defocusing, Vision augmentation},
  	file = {Ueda et al_2020_Illuminated Focus.pdf:/Users/jaime/Zotero/storage/K4EYF7YX/Ueda et al_2020_Illuminated Focus.pdf:application/pdf},
  }

  @article{McShea2016,
  	title = {Hierarchy: The Source of Teleology in Evolution},
  	url = {https://doi.org/10.7208/chicago/9780226426198.003.0006},
  	pages = {86--102},
  	journaltitle = {Evolutionary Theory\_A Hierarchical Perspective},
  	author = {{McShea}, Daniel W.},
  	date = {2016},
  	keywords = {evolution, teleology, hierearchy, read},
  	file = {McShea_2016_Hierarchy.pdf:/Users/jaime/Zotero/storage/HHH8X94F/McShea_2016_Hierarchy.pdf:application/pdf},
  }

  @article{Lang2012,
  	title = {Depth matters: Influence of depth cues on visual saliency},
  	volume = {7573 {LNCS}},
  	issn = {03029743},
  	doi = {10.1007/978-3-642-33709-3_8},
  	abstract = {Most previous studies on visual saliency have only focused on static or dynamic 2D scenes. Since the human visual system has evolved predominantly in natural three dimensional environments, it is important to study whether and how depth information influences visual saliency. In this work, we first collect a large human eye fixation database compiled from a pool of 600 2D-vs-3D image pairs viewed by 80 subjects, where the depth information is directly provided by the Kinect camera and the eye tracking data are captured in both 2D and 3D free-viewing experiments. We then analyze the major discrepancies between 2D and 3D human fixation data of the same scenes, which are further abstracted and modeled as novel depth priors. Finally, we evaluate the performances of state-of-the-art saliency detection models over 3D images, and propose solutions to enhance their performances by integrating the depth priors. © 2012 Springer-Verlag.},
  	pages = {101--115},
  	issue = {{PART} 2},
  	journaltitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  	author = {Lang, Congyan and Nguyen, Tam V. and Katti, Harish and Yadati, Karthik and Kankanhalli, Mohan and Yan, Shuicheng},
  	date = {2012},
  	note = {{ISBN}: 9783642337086},
  	file = {Lang et al_2012_Depth matters.pdf:/Users/jaime/Zotero/storage/W2EAS5R3/Lang et al_2012_Depth matters.pdf:application/pdf},
  }

  @article{Ma2015,
  	title = {Learning-based saliency model with depth information},
  	volume = {15},
  	issn = {15347362},
  	doi = {10.1167/15.6.19},
  	abstract = {Most previous studies on visual saliency focused on two-dimensional (2D) scenes. Due to the rapidly growing three-dimensional (3D) video applications, it is very desirable to know how depth information affects human visual attention. In this study, we first conducted eye-fixation experiments on 3D images. Our fixation data set comprises 475 3D images and 16 subjects. We used a Tobii {TX}300 eye tracker (Tobii, Stockholm, Sweden) to track the eye movement of each subject. In addition, this database contains 475 computed depth maps. Due to the scarcity of publicdomain 3D fixation data, this data set should be useful to the 3D visual attention research community. Then, a learning-based visual attention model was designed to predict human attention. In addition to the popular 2D features, we included the depth map and its derived features. The results indicate that the extra depth information can enhance the saliency estimation accuracy specifically for close-up objects hidden in a complex-texture background. In addition, we examined the effectiveness of various low-, mid-, and high-level features on saliency prediction. Compared with both 2D and 3D state-of-the-art saliency estimation models, our methods show better performance on the 3D test images. The eye-tracking database and the {MATLAB} source codes for the proposed saliency model and evaluation methods are available on our website.},
  	pages = {1--22},
  	number = {6},
  	journaltitle = {Journal of Vision},
  	author = {Ma, Chih Yao and Hang, Hsueh Ming},
  	date = {2015},
  	pmid = {26024466},
  	keywords = {Depth saliency, Eye-fixation database, Saliency map, Visual attention},
  	file = {Ma_Hang_2015_Learning-based saliency model with depth information.pdf:/Users/jaime/Zotero/storage/W5ESZBBP/Ma_Hang_2015_Learning-based saliency model with depth information.pdf:application/pdf},
  }

  @article{Jeck2017,
  	title = {Attentive pointing in natural scenes correlates with other measures of attention},
  	volume = {135},
  	issn = {18785646},
  	url = {http://dx.doi.org/10.1016/j.visres.2017.04.001},
  	doi = {10.1016/j.visres.2017.04.001},
  	abstract = {Finger pointing is a natural human behavior frequently used to draw attention to specific parts of sensory input. Since this pointing behavior is likely preceded and/or accompanied by the deployment of attention by the pointing person, we hypothesize that pointing can be used as a natural means of providing self-reports of attention and, in the case of visual input, visual salience. We here introduce a new method for assessing attentional choice by asking subjects to point to and tap the first place they look at on an image appearing on an electronic tablet screen. Our findings show that the tap data are well-correlated with other measures of attention, including eye fixations and selections of interesting image points, as well as with predictions of a saliency map model. We also develop an analysis method for comparing attentional maps (including fixations, reported points of interest, finger pointing, and computed salience) that takes into account the error in estimating those maps from a finite number of data points. This analysis strengthens our original findings by showing that the measured correlation between attentional maps drawn from identical underlying processes is systematically underestimated. The underestimation is strongest when the number of samples is small but it is always present. Our analysis method is not limited to data from attentional paradigms but, instead, it is broadly applicable to measures of similarity made between counts of multinomial data or probability distributions.},
  	pages = {54--64},
  	journaltitle = {Vision Research},
  	author = {Jeck, Daniel M. and Qin, Michael and Egeth, Howard and Niebur, Ernst},
  	date = {2017},
  	pmid = {28427890},
  	note = {Publisher: Elsevier Ltd},
  	keywords = {Saliency map, Attention, Salience, Fixations, Interest points, Natural scenes, Pointing, Probability density estimation, Probability distributions, Tapping},
  	file = {Jeck et al_2017_Attentive pointing in natural scenes correlates with other measures of attention.pdf:/Users/jaime/Zotero/storage/FV2K8ZWQ/Jeck et al_2017_Attentive pointing in natural scenes correlates with other measures of attention.pdf:application/pdf},
  }

  @book{Smith2017,
  	title = {Society and Social Pathology},
  	isbn = {978-3-319-50325-7},
  	pagetotal = {249-322},
  	author = {Smith, R.C.},
  	date = {2017},
  	doi = {10.1007/978-3-319-50325-7},
  	note = {Publication Title: Society and Social Pathology},
  	file = {Smith_2017_Society and Social Pathology.pdf:/Users/jaime/Zotero/storage/ESII5TYQ/Smith_2017_Society and Social Pathology.pdf:application/pdf},
  }

  @article{Engelke2013,
  	title = {Comparative study of fixation density maps},
  	volume = {22},
  	issn = {10577149},
  	doi = {10.1109/TIP.2012.2227767},
  	abstract = {Fixation density maps ({FDM}) created from eye tracking experiments are widely used in image processing applications. The {FDM} are assumed to be reliable ground truths of human visual attention and as such, one expects a high similarity between {FDM} created in different laboratories. So far, no studies have analyzed the degree of similarity between {FDM} from independent laboratories and the related impact on the applications. In this paper, we perform a thorough comparison of {FDM} from three independently conducted eye tracking experiments. We focus on the effect of presentation time and image content and evaluate the impact of the {FDM} differences on three applications: visual saliency modeling, image quality assessment, and image retargeting. It is shown that the {FDM} are very similar and that their impact on the applications is low. The individual experiment comparisons, however, are found to be significantly different, showing that inter-laboratory differences strongly depend on the experimental conditions of the laboratories. The {FDM} are publicly available to the research community. © 2012 {IEEE}.},
  	pages = {1121--1133},
  	number = {3},
  	journaltitle = {{IEEE} Transactions on Image Processing},
  	author = {Engelke, Ulrich and Liu, Hantao and Wang, Junle and Le Callet, Patrick and Heynderickx, Ingrid and Zepernick, Hans Jürgen and Maeder, Anthony},
  	date = {2013},
  	pmid = {23193452},
  	keywords = {Eye tracking, fixation density maps, inter-laboratory differences, visual attention},
  	file = {Engelke et al_2013_Comparative study of fixation density maps.pdf:/Users/jaime/Zotero/storage/6PIC3ZPQ/Engelke et al_2013_Comparative study of fixation density maps.pdf:application/pdf},
  }

  @article{Jihong2006,
  	title = {A survey of vision aids for the blind},
  	volume = {1},
  	doi = {10.1109/WCICA.2006.1713189},
  	abstract = {The development of effective user interfaces, appropriate sensors, and information processing techniques make it is possible to enable the blind to achieve additional perception of the environment. Since the beginning of the 1970's, the research of vision aids for the visually impaired people has been broadly extended. After introducing the traditional methods for guiding blind, two typical modes of mobility aid are presented in this paper. One called as {ETA} (short for Electronic Travel Aids), bases on the other natural senses of the blind, such as hearing, touch, smell, feeling and etc, focusing on Meijer's {vOICe} system which is based on the blind's sensitive auditory senses and {ENVS} which is by means of haptic feedback. The other technique is artificial vision, using surgical methods of implanting visual prosthesis in the blind's healthy retina, cortex, or optic nerve. The prosthesis generates electrical impulse and evokes the perception of points of light in the patients' visual cortex. The first type is non-wounded for the blind while the second type is wounded for the blind. Besides these, comparisons between the above two techniques and the related researches have also been indicated in this paper. © 2006 {IEEE}.},
  	pages = {4312--4316},
  	journaltitle = {Proceedings of the World Congress on Intelligent Control and Automation ({WCICA})},
  	author = {Jihong, Liu and Xiaoye, Sun},
  	date = {2006},
  	note = {{ISBN}: 1424403324},
  	keywords = {Artificial vision, Auditory compensation, Haptic compensation, Phosphene},
  	file = {Jihong_Xiaoye_2006_A survey of vision aids for the blind.pdf:/Users/jaime/Zotero/storage/HYAYVYZ2/Jihong_Xiaoye_2006_A survey of vision aids for the blind.pdf:application/pdf},
  }

  @article{Polhill2016,
  	title = {Modelling systemic change in coupled socio-environmental systems},
  	volume = {75},
  	issn = {13648152},
  	url = {http://dx.doi.org/10.1016/j.envsoft.2015.10.017},
  	doi = {10.1016/j.envsoft.2015.10.017},
  	abstract = {Abrupt systemic changes in ecological and socio-economic systems are a regular occurrence. While there has been much attention to studying systemic changes primarily in ecology as well as in economics, the attempts to do so for coupled socio-environmental systems are rarer. This paper bridges the gap by reviewing how models can be instrumental in exploring significant, fundamental changes in such systems. The history of modelling systemic change in various disciplines contains a range of definitions and approaches. Even so, most of these efforts share some common challenges within the modelling context. We propose a framework drawing these challenges together, and use it to discuss the articles in this thematic issue on modelling systemic change in coupled social and environmental systems. The differing approaches used highlight that modelling systemic change is an area of endeavour that would benefit from greater synergies between the various disciplines concerned with systemic change.},
  	pages = {318--332},
  	journaltitle = {Environmental Modelling and Software},
  	author = {Polhill, J. Gary and Filatova, Tatiana and Schlüter, Maja and Voinov, Alexey},
  	date = {2016},
  	note = {Publisher: Elsevier Ltd},
  	keywords = {Complexity, Feedbacks, Ontology, Regime shift, Shock, Vocabulary},
  	file = {Polhill et al_2016_Modelling systemic change in coupled socio-environmental systems.pdf:/Users/jaime/Zotero/storage/CRJ5YG8Z/Polhill et al_2016_Modelling systemic change in coupled socio-environmental systems.pdf:application/pdf},
  }

  @article{Polhill2021,
  	title = {Using Agent-Based Models for Prediction in Complex and Wicked Systems},
  	volume = {24},
  	url = {http://jasss.soc.surrey.ac.uk/24/3/2.html},
  	number = {3},
  	journaltitle = {Journal of Artificial Societies and Social Simulation},
  	author = {Polhill, Gareth and Hare, Matthew and Bauermann, Tom and Anzola, David and Palmer, Erika and Salt, Doug and Antosz, Patrycja},
  	date = {2021},
  	keywords = {agent-based modelling, cellular automata, complex systems, prediction, turing, wicked systems},
  	file = {Polhill et al_2021_Using Agent-Based Models for Prediction in Complex and Wicked Systems.pdf:/Users/jaime/Zotero/storage/ZQS8BV7C/Polhill et al_2021_Using Agent-Based Models for Prediction in Complex and Wicked Systems.pdf:application/pdf},
  }

  @article{Ehrlich2017,
  	title = {Head-Mounted Display Technology for Low-Vision Rehabilitation and Vision Enhancement},
  	volume = {176},
  	issn = {18791891},
  	url = {http://dx.doi.org/10.1016/j.ajo.2016.12.021},
  	doi = {10.1016/j.ajo.2016.12.021},
  	abstract = {Purpose To describe the various types of head-mounted display technology, their optical and human-factors considerations, and their potential for use in low-vision rehabilitation and vision enhancement. Design Expert perspective. Methods An overview of head-mounted display technology by an interdisciplinary team of experts drawing on key literature in the field. Results Head-mounted display technologies can be classified based on their display type and optical design. See-through displays such as retinal projection devices have the greatest potential for use as low-vision aids. Devices vary by their relationship to the user's eyes, field of view, illumination, resolution, color, stereopsis, effect on head motion, and user interface. These optical and human-factors considerations are important when selecting head-mounted displays for specific applications and patient groups. Conclusions Head-mounted display technologies may offer advantages over conventional low-vision aids. Future research should compare head-mounted displays with commonly prescribed low-vision aids to compare their effectiveness in addressing the impairments and rehabilitation goals of diverse patient populations.},
  	pages = {26--32},
  	journaltitle = {American Journal of Ophthalmology},
  	author = {Ehrlich, Joshua R. and Ojeda, Lauro V. and Wicker, Donna and Day, Sherry and Howson, Ashley and Lakshminarayanan, Vasudevan and Moroi, Sayoko E.},
  	date = {2017},
  	pmid = {28048975},
  	note = {Publisher: Elsevier Ltd},
  	file = {Ehrlich et al_2017_Head-Mounted Display Technology for Low-Vision Rehabilitation and Vision.pdf:/Users/jaime/Zotero/storage/RY4532YU/Ehrlich et al_2017_Head-Mounted Display Technology for Low-Vision Rehabilitation and Vision.pdf:application/pdf},
  }

  @article{Massof1992,
  	title = {Obstacles encountered in the development of the low vision enhancement system},
  	volume = {69},
  	issn = {10405488},
  	doi = {10.1097/00006324-199201000-00005},
  	abstract = {The Johns Hopkins Wilmer Eye Institute and the {NASA} Stennis Space Center are collaborating on the development of a new high technology low vision aid called the Low Vision Enhancement System ({LVES}). The {LVES} consists of a binocular head-mounted video display system, video cameras mounted on the head-mounted display, and real-time video image processing in a system package that is battery powered and portable. Through a phased development approach, several generations of the {LVES} can be made available to the patient in a timely fashion. This paper describes the {LVES} project with major emphasis on technical problems encountered or anticipated during the development process. © 1992 American Academy of Optometry.},
  	pages = {32--41},
  	number = {1},
  	journaltitle = {Optometry and Vision Science},
  	author = {Massof, Robert W. and Rickman, Douglas L.},
  	date = {1992},
  	pmid = {1371334},
  	keywords = {Eyetracking, Head-mounted display, Image processing, Low vision, Vision enhancement},
  	file = {Massof_Rickman_1992_Obstacles encountered in the development of the low vision enhancement system.pdf:/Users/jaime/Zotero/storage/MMFWX3W4/Massof_Rickman_1992_Obstacles encountered in the development of the low vision enhancement system.pdf:application/pdf},
  }

  @article{Rothkopf2007,
  	title = {Task and context determine where you look},
  	volume = {7},
  	issn = {15347362},
  	doi = {10.1167/7.14.16},
  	abstract = {The deployment of human gaze has been almost exclusively studied independent of any specific ongoing task and limited to two-dimensional picture viewing. This contrasts with its use in everyday life, which mostly consists of purposeful tasks where gaze is crucially involved. To better understand deployment of gaze under such circumstances, we devised a series of experiments, in which subjects navigated along a walkway in a virtual environment and executed combinations of approach and avoidance tasks. The position of the body and the gaze were monitored during the execution of the task combinations and dependence of gaze on the ongoing tasks as well as the visual features of the scene was analyzed. Gaze distributions were compared to a random gaze allocation strategy as well as a specific "saliency model." Gaze distributions showed high similarity across subjects. Moreover, the precise fixation locations on the objects depended on the ongoing task to the point that the specific tasks could be predicted from the subject's fixation data. By contrast, gaze allocation according to a random or a saliency model did not predict the executed fixations or the observed dependence of fixation locations on the specific task. © {ARVO}.},
  	pages = {1--20},
  	number = {14},
  	journaltitle = {Journal of Vision},
  	author = {Rothkopf, Constantin A. and Ballard, Dana H. and Hayhoe, Mary M.},
  	date = {2007},
  	pmid = {18217811},
  	keywords = {Complex tasks, Contextual effects, Eye movements, Saliency, Task influence},
  	file = {Rothkopf et al_2007_Task and context determine where you look.pdf:/Users/jaime/Zotero/storage/KZZVCNXD/Rothkopf et al_2007_Task and context determine where you look.pdf:application/pdf},
  }

  @article{Brown2011,
  	title = {Seeing with sound? exploring different characteristics of a visual-to-auditory sensory substitution device},
  	volume = {40},
  	issn = {03010066},
  	doi = {10.1068/p6952},
  	abstract = {Sensory substitution devices convert live visual images into auditory signals, for example with a web camera (to record the images), a computer (to perform the conversion) and headphones (to listen to the sounds). In a series of three experiments, the performance of one such device ('The {vOICe}') was assessed under various conditions on blindfolded sighted participants. The main task that we used involved identifying and locating objects placed on a table by holding a webcam (like a flashlight) or wearing it on the head (like a miner's light). Identifying objects on a table was easier with a hand-held device, but locating the objects was easier with a head- mounted device. Brightness converted into loudness was less effective than the reverse contrast (dark being loud), suggesting that performance under these conditions (natural indoor lighting, novice users) is related more to the properties of the auditory signal (ie the amount of noise in it) than the cross-modal association between loudness and brightness. Individual differences in musical memory (detecting pitch changes in two sequences of notes) was related to the time taken to identify or recognise objects, but individual differences in self-reported vividness of visual imagery did not reliably predict performance across the experiments. In general, the results suggest that the auditory characteristics of the device may be more important for initial learning than visual associations.},
  	pages = {1120--1135},
  	number = {9},
  	journaltitle = {Perception},
  	author = {Brown, David and Macpherson, Tom and Ward, Jamie},
  	date = {2011},
  	pmid = {22208131},
  	file = {Brown et al_2011_Seeing with sound.pdf:/Users/jaime/Zotero/storage/NMI4JPQH/Brown et al_2011_Seeing with sound.pdf:application/pdf},
  }

  @article{Meijer1993,
  	title = {An Experimental System for Auditory Image},
  	volume = {39},
  	number = {2},
  	journaltitle = {Ieee Transactions (on Biomedical Engineering},
  	author = {Meijer, Peter},
  	date = {1993},
  	file = {Meijer_1993_An Experimental System for Auditory Image.pdf:/Users/jaime/Zotero/storage/EBJLBAYG/Meijer_1993_An Experimental System for Auditory Image.pdf:application/pdf},
  }

  @article{Carlone2019,
  	title = {Attention and Anticipation in Fast Visual-Inertial Navigation},
  	volume = {35},
  	issn = {15523098},
  	doi = {10.1109/TRO.2018.2872402},
  	abstract = {We study a visual-inertial navigation ({VIN}) problem in which a robot needs to estimate its state using an on-board camera and an inertial sensor, without any prior knowledge of the external environment. We consider the case in which the robot can allocate limited resources to {VIN}, due to tight computational constraints. Therefore, we answer the following question: under limited resources, what are the most relevant visual cues to maximize the performance of {VIN}? Our approach has four key ingredients. First, it is task-driven, in that the selection of the visual cues is guided by a metric quantifying the {VIN} performance. Second, it exploits the notion of anticipation, since it uses a simplified model for forward-simulation of robot dynamics, predicting the utility of a set of visual cues over a future time horizon. Third, it is efficient and easy to implement, since it leads to a greedy algorithm for the selection of the most relevant visual cues. Fourth, it provides formal performance guarantees: we leverage submodularity to prove that the greedy selection cannot be far from the optimal (combinatorial) selection. Simulations and real experiments on agile drones show that our approach ensures state-of-The-Art {VIN} performance while maintaining a lean processing time. In the easy scenarios, our approach outperforms appearance-based feature selection in terms of localization errors. In the most challenging scenarios, it enables accurate {VIN} while appearance-based feature selection fails to track robot's motion during aggressive maneuvers.},
  	pages = {1--20},
  	number = {1},
  	journaltitle = {{IEEE} Transactions on Robotics},
  	author = {Carlone, Luca and Karaman, Sertac},
  	date = {2019},
  	note = {Publisher: {IEEE}},
  	keywords = {Aerial robotics, computer vision, sensor fusion, simultaneous localization and mapping ({SLAM})},
  	file = {Carlone_Karaman_2019_Attention and Anticipation in Fast Visual-Inertial Navigation.pdf:/Users/jaime/Zotero/storage/XAMHNKNZ/Carlone_Karaman_2019_Attention and Anticipation in Fast Visual-Inertial Navigation.pdf:application/pdf},
  }

  @article{Lieby2011,
  	title = {Substituting depth for intensity and real-time phosphene rendering: Visual navigation under low vision conditions},
  	issn = {1557170X},
  	doi = {10.1109/IEMBS.2011.6091977},
  	abstract = {Navigation and way finding including obstacle avoidance is difficult when visual perception is limited to low resolution, such as is currently available on a bionic eye. Depth visualisation may be a suitable alternative. Such an approach can be evaluated using simulated phosphenes with a wearable mobile virtual reality kit. In this paper, we present two novel approaches: (i) an implementation of depth visualisation; and, (ii) novel methods for rapid rendering of simulated phosphenes with an empirical comparison between them. Our new software-based method for simulated phosphene rendering shows large speed improvements, facilitating the display in real-time of a large number of phosphenes with size and brightness dependent on pixel intensity, and with customised output dynamic range. Further, we describe the protocol, navigation environment and system used for visual navigation experiments to evaluate the use of depth on low resolution simulations of a bionic eye perceptual experience. Results for these experiments show that a depth-based representation is effective for navigation, and shows significant advantages over intensity-based approaches when overhanging obstacles are present. The results of the experiments were reported in [1], [2]. © 2011 {IEEE}.},
  	pages = {8017--8020},
  	journaltitle = {Proceedings of the Annual International Conference of the {IEEE} Engineering in Medicine and Biology Society, {EMBS}},
  	author = {Lieby, Paulette and Barnes, Nick and {McCarthy}, Chris and Liu, Nianjun and Dennett, Hugh and Walker, Janine G. and Botea, Viorica and Scott, Adele F.},
  	date = {2011},
  	pmid = {22256201},
  	note = {{ISBN}: 9781424441211},
  	keywords = {Image feature extraction, Image filtering, Image visualization},
  	file = {Lieby et al_2011_Substituting depth for intensity and real-time phosphene rendering.pdf:/Users/jaime/Zotero/storage/5W47P2LJ/Lieby et al_2011_Substituting depth for intensity and real-time phosphene rendering.pdf:application/pdf},
  }

  @article{Danilov2005,
  	title = {{BrainPort}: An Alternative Input to the Brain},
  	volume = {4},
  	pages = {537--550},
  	number = {4},
  	author = {Danilov, Yuri and Tyler, Mitchell},
  	date = {2005},
  	keywords = {Brain, brainport, Computer Systems, electrotactile stimulation, Humans, sensory input, tongue, Tongue, User-Computer Interface},
  	file = {Danilov_Tyler_2005_BRAINPORT.pdf:/Users/jaime/Zotero/storage/I83N66S4/Danilov_Tyler_2005_BRAINPORT.pdf:application/pdf},
  }

  @article{Nowik2020,
  	title = {Bionic eye review – An update},
  	volume = {78},
  	issn = {15322653},
  	doi = {10.1016/j.jocn.2020.05.041},
  	abstract = {Purpose: To date, reviews of bionic eye have concentrated on implants which were used in human trials in the developed countries. This is the main restriction of this systematic review examines, however this review discusses worldwide advances in retinal prosthetic research, assesses engineering features and clinical progress of recent implant trials, and identifies potential future research areas in the field of bionic implants. Methods: A literature review searching {PubMed}, Google Scholar, and {IEEExplore} was performed using the {PRISMA} Guidelines for Systematic Review. We included peer-reviewed papers in the review which demonstrated progress in human or animal trials and papers with described innovative bionic eye engineering design. For each trial, a characteristic of the device, engineering solution, and latest clinical outcomes were presented. Results: Eleven prosthetic projects fulfilled met our inclusion criteria and were ordered by stimulation location. Four have recently finished human trials, three are having conducted multi- or singlecenter human trials, and three are in preclinical animal testing stage. {FDA} has approved Argus {II} ({FDA} 2013, {CE} 2011); the Alpha-{IMS} ({CE} 2013) has been approved and obtained {BCVA} with Landolt-C test has taken into a multicenter clinical research. New approaches will be presented using alternating magnetic fields, low-intensity focused ultrasounds, optogenetics, implementing ionic gradients across neural cell membranes or influencing neurotransmitter levels will be presented in the review. Conclusion: Several bionic eye have successfully achieved visual perception in animals and/or humans. However, many things need to be improved and engineering difficulties are to be resolved before bionic eye will be capable of fully and safely bring back vision functions. New approaches could improve medical outcome of future bionic eye.},
  	pages = {8--19},
  	issue = {xxxx},
  	journaltitle = {Journal of Clinical Neuroscience},
  	author = {Nowik, Kamil and Langwińska-Wośko, Ewa and Skopiński, Piotr and Nowik, Katarzyna E. and Szaflik, Jacek P.},
  	date = {2020},
  	pmid = {32571603},
  	keywords = {Retinal prostheses, Artificial vision, Bionic eye, Retinitis pigmentosa},
  	file = {Nowik et al_2020_Bionic eye review – An update.pdf:/Users/jaime/Zotero/storage/DTIGTV93/Nowik et al_2020_Bionic eye review – An update.pdf:application/pdf},
  }

  @article{Lee2016,
  	title = {{RGB}-D camera based wearable navigation system for the visually impaired},
  	volume = {149},
  	issn = {1090235X},
  	doi = {10.1016/j.cviu.2016.03.019},
  	abstract = {In this paper, a novel wearable {RGB}-D camera based indoor navigation system for the visually impaired is presented. The system guides the visually impaired user from one location to another location without a prior map or {GPS} information. Accurate real-time egomotion estimation, mapping, and path planning in the presence of obstacles are essential for such a system. We perform real-time 6-{DOF} egomotion estimation using sparse visual features, dense point clouds, and the ground plane to reduce drift from a head-mounted {RGB}-D camera. The system also builds 2D probabilistic occupancy grid map for efficient traversability analysis which is a basis for dynamic path planning and obstacle avoidance. The system can store and reload maps generated by the system while traveling and continually expand the coverage area of navigation. Next, the shortest path between the start location to destination is generated. The system generates a safe and efficient way point based on the traversability analysis result and the shortest path and updates the way point while a user is constantly moving. Appropriate cues are generated and delivered to a tactile feedback system to guide the visually impaired user to the way point. The proposed wearable system prototype is composed of multiple modules including a head-mounted {RGB}-D camera, standard laptop that runs a navigation software, smart phone user interface, and haptic feedback vest. The proposed system achieves real-time navigation performance at 28.6Hz in average on a laptop, and helps the visually impaired extends the range of their activities and improve the orientation and mobility performance in a cluttered environment. We have evaluated the performance of the proposed system in mapping and localization with blind-folded and the visually impaired subjects. The mobility experiment results show that navigation in indoor environments with the proposed system avoids collisions successfully and improves mobility performance of the user compared to conventional and state-of-the-art mobility aid devices.},
  	pages = {3--20},
  	journaltitle = {Computer Vision and Image Understanding},
  	author = {Lee, Young Hoon and Medioni, Gérard},
  	date = {2016},
  	keywords = {Assistive technologies for the visually impaired, Visual {SLAM}, Wearable navigation system},
  	file = {Lee_Medioni_2016_RGB-D camera based wearable navigation system for the visually impaired.pdf:/Users/jaime/Zotero/storage/ZVB9JW6F/Lee_Medioni_2016_RGB-D camera based wearable navigation system for the visually impaired.pdf:application/pdf},
  }

  @article{PesnotLerousseau2021,
  	title = {Training-induced plasticity enables visualizing sounds with a visual-to-auditory conversion device},
  	volume = {11},
  	issn = {2045-2322},
  	url = {https://doi.org/10.1038/s41598-021-94133-4},
  	doi = {10.1038/s41598-021-94133-4},
  	abstract = {Sensory substitution devices aim at restoring visual functions by converting visual information into auditory or tactile stimuli. Although these devices show promise in the range of behavioral abilities they allow, the processes underlying their use remain underspecified. In particular, while an initial debate focused on the visual versus auditory or tactile nature of sensory substitution, since over a decade, the idea that it reflects a mixture of both has emerged. In order to investigate behaviorally the extent to which visual and auditory processes are involved, participants completed a Stroop-like crossmodal interference paradigm before and after being trained with a conversion device which translates visual images into sounds. In addition, participants' auditory abilities and their phenomenologies were measured. Our study revealed that, after training, when asked to identify sounds, processes shared with vision were involved, as participants’ performance in sound identification was influenced by the simultaneously presented visual distractors. In addition, participants’ performance during training and their associated phenomenology depended on their auditory abilities, revealing that processing finds its roots in the input sensory modality. Our results pave the way for improving the design and learning of these devices by taking into account inter-individual differences in auditory and visual perceptual strategies.},
  	pages = {14762},
  	number = {1},
  	journaltitle = {Scientific Reports},
  	author = {Pesnot Lerousseau, Jacques and Arnold, Gabriel and Auvray, Malika},
  	date = {2021},
  }

  @article{Caduff2008,
  	title = {On the assessment of landmark salience for human navigation},
  	volume = {9},
  	issn = {16124782},
  	doi = {10.1007/s10339-007-0199-2},
  	abstract = {In this paper, we propose a conceptual framework for assessing the salience of landmarks for navigation. Landmark salience is derived as a result of the observer's point of view, both physical and cognitive, the surrounding environment, and the objects contained therein. This is in contrast to the currently held view that salience is an inherent property of some spatial feature. Salience, in our approach, is expressed as a three-valued Saliency Vector. The components that determine this vector are Perceptual Salience, which defines the exogenous (or passive) potential of an object or region for acquisition of visual attention, Cognitive Salience, which is an endogenous (or active) mode of orienting attention, triggered by informative cues providing advance information about the target location, and Contextual Salience, which is tightly coupled to modality and task to be performed. This separation between voluntary and involuntary direction of visual attention in dependence of the context allows defining a framework that accounts for the interaction between observer, environment, and landmark. We identify the low-level factors that contribute to each type of salience and suggest a probabilistic approach for their integration. Finally, we discuss the implications, consider restrictions, and explore the scope of the framework. © 2007 Marta Olivetti Belardinelli and Springer-Verlag.},
  	pages = {249--267},
  	number = {4},
  	journaltitle = {Cognitive Processing},
  	author = {Caduff, David and Timpf, Sabine},
  	date = {2008},
  	pmid = {17999102},
  	keywords = {Attention, Information processing, Landmark, Navigation, Salience},
  	file = {Caduff_Timpf_2008_On the assessment of landmark salience for human navigation.pdf:/Users/jaime/Zotero/storage/SDGUMXMX/Caduff_Timpf_2008_On the assessment of landmark salience for human navigation.pdf:application/pdf},
  }

  @article{Kolic2021,
  	title = {A 44 channel suprachoroidal retinal prosthesis: laboratory based visual function and functional vision outcomes.},
  	volume = {62},
  	issn = {1552-5783},
  	abstract = {The 44 channel (44Ch) suprachoroidal retinal prosthesis is a second-generation bionic eye implant that follows on from a proof of concept study conducted between 2012 and 2014. The 44Ch implant is designed to provide artificial vision (phosphenes) to recipients with end-stage retinitis pigmentosa ({RP}). We aimed to compare the performance of visual function and functional vision tasks at pre-implantation and post device activation.    The 44Ch suprachoroidal retinal prosthesis was unilaterally implanted in 4 participants with advanced {RP}, age range 39 to 66 years, between February and August 2018. After a 16-week period of vision rehabilitation training, participants were assessed on three laboratory-based visual function tasks: moving bar ({MB}), square localisation ({SL}) and grating acuity ({GA}), followed by three functional vision tasks: table-top search ({TTS}), doorway detection ({DWD}) and obstacle avoidance ({OA}). Assessment time points were pre-implantation and from post-device activation at weeks 17 (W17), W20, W32, W44, W56 and W68. Data was pooled across all 4 participants and performance was compared between device on and off.    With device on, three of the four participants could discriminate {MB} speeds ranging from 7 to 30 degrees per second. Average pointing error for all four participants, across all visits on {SL}, was 10.3 ± 3.3° (device on) versus 27.7 ± 8.7° (device off), p \&lt; 0.001. Two of the four participants had measurable {GA} of 0.033 cycles-per-degree. On average, the device improved participants’ ability to locate objects on the {TTS}, detect and touch the doorway on {DWD} task and detect and avoid more obstacles during {OA}, at all-time points post-activation (p\&lt;0.04), compared to device off.    The 44Ch second generation suprachoroidal implant improved overall performance on both laboratory-based visual function and functional vision tasks, compared to device off. The device shows the capability to provide visual sensory information, to aid detection of obstacles and improve accuracy when reaching for an object, for people with profound vision loss due to {RP}.  This is a 2021 {ARVO} Annual Meeting abstract.},
  	pages = {3168},
  	number = {8},
  	journaltitle = {Investigative Ophthalmology \& Visual Science},
  	author = {Kolic, Maria and Baglin, Elizabeth K and Titchener, Samuel A and Kvansakul, Jessica and Abbott, Carla J and Barnes, Nick and {McGuinness}, Myra and Kentler, William G and Young, Kiera and Walker, Janine and Yeoh, Jonathan and Nayagam, David A X and Luu, Chi D and Ayton, Lauren N and Petoe, Matthew A and Allen, Penelope J},
  	date = {2021-06-21},
  }

  @article{Hanassy2013,
  	title = {{EyeMusic}: A colorful experience for the blind},
  	volume = {26},
  	issn = {2213-4808},
  	doi = {10.1163/22134808-000s0084},
  	abstract = {Various sensory substitution devices ({SSDs}) have been designed in the past decades with the aim of creating a visual experience for the blind. We will present the {EyeMusic}, a visual-to-auditory {SSD} for the blind, providing both shape and color information. Our design uses musical notes generated by natural instruments to convey the visual information. This creates a pleasant user experience and facilitates prolonged use. We further present behavioral results showing that shape and color information are indeed decodable from the generated auditory stimuli.},
  	pages = {116},
  	number = {1},
  	journaltitle = {Multisensory Research},
  	author = {Hanassy, Shlomi and Maidenbaum, Shachar and Tauber, Dina and Amedi, Amir and Abboud, Sami and Levy-Tzedek, Shelly},
  	date = {2013},
  	file = {Hanassy et al_2013_EyeMusic.pdf:/Users/jaime/Zotero/storage/ISBTSW9T/Hanassy et al_2013_EyeMusic.pdf:application/pdf},
  }

  @book{Neumann1990,
  	title = {Relationships between perception and action. Current approaches},
  	isbn = {3-540-52069-4 hardcover{\textbackslash}r0-387-52069-4 hardcover},
  	abstract = {This book presents a multidisciplinary approach to the study of relationships between perception and action. It reflects the renewed interest in problems of action control that has emerged in psychology during the last decade and created a new theoretical climate bringing psychology closer to the biological sciences. Each chapter presents both a particular point of view and a comprehensive overview of recent findings relevant to the author's approach. ({PsycINFO} Database Record (c) 2007 {APA}, all rights reserved)},
  	pagetotal = {X, 411; index},
  	author = {Neumann, O and Prinz, W},
  	date = {1990},
  	pmid = {812},
  	note = {Publication Title: With contributions by P. Bieri... [et al.]},
  	keywords = {Neurobiology, Neurophysiology, Neuropsychology, Perception, Sensory biology},
  	file = {Neumann_Prinz_1990_Relationships between perception and action.pdf:/Users/jaime/Zotero/storage/WPBMQJY5/Neumann_Prinz_1990_Relationships between perception and action.pdf:application/pdf},
  }

  @article{Parr2021,
  	title = {Memory and Markov Blankets},
  	volume = {23},
  	issn = {1099-4300},
  	url = {https://www.mdpi.com/1099-4300/23/9/1105},
  	doi = {10.3390/e23091105},
  	abstract = {In theoretical biology, we are often interested in random dynamical systems—like the brain—that appear to model their environments. This can be formalized by appealing to the existence of a (possibly non-equilibrium) steady state, whose density preserves a conditional independence between a biological entity and its surroundings. From this perspective, the conditioning set, or Markov blanket, induces a form of vicarious synchrony between creature and world—as if one were modelling the other. However, this results in an apparent paradox. If all conditional dependencies between a system and its surroundings depend upon the blanket, how do we account for the mnemonic capacity of living systems? It might appear that any shared dependence upon past blanket states violates the independence condition, as the variables on either side of the blanket now share information not available from the current blanket state. This paper aims to resolve this paradox, and to demonstrate that conditional independence does not preclude memory. Our argument rests upon drawing a distinction between the dependencies implied by a steady state density, and the density dynamics of the system conditioned upon its configuration at a previous time. The interesting question then becomes: What determines the length of time required for a stochastic system to ‘forget’ its initial conditions? We explore this question for an example system, whose steady state density possesses a Markov blanket, through simple numerical analyses. We conclude with a discussion of the relevance for memory in cognitive systems like us.},
  	pages = {1105},
  	number = {9},
  	journaltitle = {Entropy},
  	author = {Parr, Thomas and Da Costa, Lancelot and Heins, Conor and Ramstead, Maxwell James D and Friston, Karl J},
  	date = {2021-08-25},
  	file = {Parr et al_2021_Memory and Markov Blankets.pdf:/Users/jaime/Zotero/storage/TLGLA95G/Parr et al_2021_Memory and Markov Blankets.pdf:application/pdf},
  }

  @article{Anderson1972,
  	title = {More is Different},
  	volume = {177},
  	url = {https://www.jstor.org/stable/1734697},
  	pages = {393--396},
  	number = {4047},
  	journaltitle = {Science},
  	author = {Anderson, Philip W},
  	date = {1972},
  	file = {Anderson_1972_More is Different.pdf:/Users/jaime/Zotero/storage/ZN38YCPG/Anderson_1972_More is Different.pdf:application/pdf},
  }

  @article{Friston2021,
  	title = {Some Interesting Observations on the Free Energy Principle},
  	volume = {23},
  	issn = {10994300},
  	doi = {10.3390/e23081076},
  	abstract = {Biehl et al. (2021) present some interesting observations on an early formulation of the free energy principle. We use these observations to scaffold a discussion of the technical arguments that underwrite the free energy principle. This discussion focuses on solenoidal coupling between various (subsets of) states in sparsely coupled systems that possess a Markov blanket—and the distinction between exact and approximate Bayesian inference, implied by the ensuing Bayesian mechanics.},
  	pages = {1076},
  	number = {8},
  	journaltitle = {Entropy},
  	author = {Friston, Karl J. and Da Costa, Lancelot and Parr, Thomas},
  	date = {2021},
  	keywords = {bayesian, free energy principle, markov blanket, variational},
  	file = {Friston et al_2021_Some Interesting Observations on the Free Energy Principle.pdf:/Users/jaime/Zotero/storage/J9YKQDFA/Friston et al_2021_Some Interesting Observations on the Free Energy Principle.pdf:application/pdf},
  }

  @inproceedings{Burda2018,
  	title = {Large-Scale Study of Curiosity-Driven Learning},
  	url = {https://openreview.net/forum?id=rJNwDjAqYX},
  	abstract = {An agent trained only with curiosity, and no extrinsic reward, does surprisingly well on 54 popular environments, including the suite of Atari games, Mario etc.},
  	eventtitle = {International Conference on Learning Representations},
  	author = {Burda, Yuri and Edwards, Harri and Pathak, Deepak and Storkey, Amos and Darrell, Trevor and Efros, Alexei A.},
  	urldate = {2021-09-05},
  	date = {2018-09-27},
  	langid = {english},
  	file = {Burda et al_2018_Large-Scale Study of Curiosity-Driven Learning.pdf:/Users/jaime/Zotero/storage/W4FUAQU8/Burda et al_2018_Large-Scale Study of Curiosity-Driven Learning.pdf:application/pdf;Snapshot:/Users/jaime/Zotero/storage/V2JQ6337/forum.html:text/html},
  }

  @article{McShea2012,
  	title = {Upper-directed systems: a new approach to teleology in biology},
  	volume = {27},
  	issn = {0169-3867, 1572-8404},
  	url = {http://link.springer.com/10.1007/s10539-012-9326-2},
  	doi = {10.1007/s10539-012-9326-2},
  	shorttitle = {Upper-directed systems},
  	pages = {663--684},
  	number = {5},
  	journaltitle = {Biology \& Philosophy},
  	shortjournal = {Biol Philos},
  	author = {{McShea}, Daniel W.},
  	urldate = {2021-09-05},
  	date = {2012-09},
  	langid = {english},
  	file = {Upperdirected_systems_a_new_ap.PDF:/Users/jaime/Zotero/storage/BQMSDDUX/Upperdirected_systems_a_new_ap.PDF:application/pdf},
  }

  @article{Kirchhoff2018,
  	title = {The Markov blankets of life: autonomy, active inference and the free energy principle},
  	volume = {15},
  	issn = {1742-5689, 1742-5662},
  	url = {https://royalsocietypublishing.org/doi/10.1098/rsif.2017.0792},
  	doi = {10.1098/rsif.2017.0792},
  	shorttitle = {The Markov blankets of life},
  	abstract = {This work addresses the autonomous organization of biological systems. It does so by considering the boundaries of biological systems, from individual cells to
                Home sapiens
                , in terms of the presence of Markov blankets under the active inference scheme—a corollary of the free energy principle. A Markov blanket defines the boundaries of a system in a statistical sense. Here we consider how a collective of Markov blankets can self-assemble into a global system that itself has a Markov blanket; thereby providing an illustration of how autonomous systems can be understood as having layers of nested and self-sustaining boundaries. This allows us to show that: (i) any living system is a Markov blanketed system and (ii) the boundaries of such systems need not be co-extensive with the biophysical boundaries of a living organism. In other words, autonomous systems are hierarchically composed of Markov blankets of Markov blankets—all the way down to individual cells, all the way up to you and me, and all the way out to include elements of the local environment.},
  	pages = {20170792},
  	number = {138},
  	journaltitle = {Journal of The Royal Society Interface},
  	shortjournal = {J. R. Soc. Interface.},
  	author = {Kirchhoff, Michael and Parr, Thomas and Palacios, Ensor and Friston, Karl and Kiverstein, Julian},
  	urldate = {2021-09-05},
  	date = {2018-01},
  	langid = {english},
  	file = {Kirchhoff et al_2018_The Markov blankets of life.pdf:/Users/jaime/Zotero/storage/WNIMYQ93/Kirchhoff et al_2018_The Markov blankets of life.pdf:application/pdf},
  }

  @article{Colombo2021,
  	title = {First principles in the life sciences: the free-energy principle, organicism, and mechanism},
  	volume = {198},
  	issn = {0039-7857, 1573-0964},
  	url = {https://link.springer.com/10.1007/s11229-018-01932-w},
  	doi = {10.1007/s11229-018-01932-w},
  	shorttitle = {First principles in the life sciences},
  	abstract = {Abstract

                The free-energy principle states that all systems that minimize their free energy resist a tendency to physical disintegration. Originally proposed to account for perception, learning, and action, the free-energy principle has been applied to the evolution, development, morphology, anatomy and function of the brain, and has been called a
                postulate
                , an
                unfalsifiable principle
                , a
                natural law
                , and an
                imperative
                . While it might afford a theoretical foundation for understanding the relationship between environment, life, and mind, its epistemic status is unclear. Also unclear is how the free-energy principle relates to prominent theoretical approaches to life science phenomena, such as organicism and mechanism. This paper clarifies both issues, and identifies limits and prospects for the free-energy principle as a first principle in the life sciences.},
  	pages = {3463--3488},
  	issue = {S14},
  	journaltitle = {Synthese},
  	shortjournal = {Synthese},
  	author = {Colombo, Matteo and Wright, Cory},
  	urldate = {2021-09-05},
  	date = {2021-06},
  	langid = {english},
  	file = {Colombo_Wright_2021_First principles in the life sciences.pdf:/Users/jaime/Zotero/storage/PVUIC8NK/Colombo_Wright_2021_First principles in the life sciences.pdf:application/pdf},
  }

  @inproceedings{Cordonnier2020a,
  	title = {On the Relationship between Self-Attention and Convolutional Layers},
  	url = {https://openreview.net/forum?id=HJlnC1rKPB},
  	abstract = {A self-attention layer can perform convolution and often learns to do so in practice.},
  	eventtitle = {International Conference on Learning Representations},
  	author = {Cordonnier, Jean-Baptiste and Loukas, Andreas and Jaggi, Martin},
  	urldate = {2021-09-05},
  	date = {2020-09-25},
  	langid = {english},
  	keywords = {visualisation, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, convolution, self-attention},
  	file = {arXiv.org Snapshot:/Users/jaime/Zotero/storage/Z4QPVKIM/1911.html:text/html;Cordonnier et al_2019_On the Relationship between Self-Attention and Convolutional Layers.pdf:/Users/jaime/Zotero/storage/XRL7EKBY/Cordonnier et al_2019_On the Relationship between Self-Attention and Convolutional Layers.pdf:application/pdf;Cordonnier et al_2020_On the Relationship between Self-Attention and Convolutional Layers.pdf:/Users/jaime/Zotero/storage/R2HYVRGI/Cordonnier et al_2020_On the Relationship between Self-Attention and Convolutional Layers.pdf:application/pdf;Snapshot:/Users/jaime/Zotero/storage/TK5PVPPC/forum.html:text/html},
  }

  @book{Barabasi2016,
  	location = {Cambridge, United Kingdom},
  	title = {Network science},
  	isbn = {978-1-107-07626-6},
  	abstract = {"Networks are everywhere, from the Internet, to social networks, and the genetic networks that determine our biological existence. Illustrated throughout in full colour, this pioneering textbook, spanning a wide range of topics from physics to computer science, engineering, economics and the social sciences, introduces network science to an interdisciplinary audience. From the origins of the six degrees of separation to explaining why networks are robust to random failures, the author explores how viruses like Ebola and H1N1 spread, and why it is that our friends have more friends than we do. Using numerous real-world examples, this innovatively designed text includes clear delineation between undergraduate and graduate level material"--Page [4] of cover},
  	pagetotal = {456},
  	publisher = {Cambridge University Press},
  	author = {Barabási, Albert-László and Pósfai, Márton},
  	date = {2016},
  	note = {{OCLC}: ocn910772793},
  	keywords = {Computer networks, Information networks},
  }

  @article{Moran2021,
  	title = {From ants to fishing vessels: a simple model for herding and exploitation of finite resources},
  	volume = {129},
  	issn = {01651889},
  	url = {https://linkinghub.elsevier.com/retrieve/pii/S0165188921001044},
  	doi = {10.1016/j.jedc.2021.104169},
  	shorttitle = {From ants to fishing vessels},
  	pages = {104169},
  	journaltitle = {Journal of Economic Dynamics and Control},
  	shortjournal = {Journal of Economic Dynamics and Control},
  	author = {Moran, José and Fosset, Antoine and Kirman, Alan and Benzaquen, Michael},
  	urldate = {2021-09-05},
  	date = {2021-08},
  	langid = {english},
  	keywords = {Physics - Physics and Society, Quantitative Biology - Populations and Evolution},
  	file = {arXiv.org Snapshot:/Users/jaime/Zotero/storage/3SU8W9IZ/2009.html:text/html;Moran et al_2021_From ants to fishing vessels.pdf:/Users/jaime/Zotero/storage/3SIFU4D4/Moran et al_2021_From ants to fishing vessels.pdf:application/pdf},
  }

  @article{Barbrook-Johnson2021,
  	title = {Combining complexity-framed research methods for social research},
  	issn = {1364-5579, 1464-5300},
  	url = {https://www.tandfonline.com/doi/full/10.1080/13645579.2021.1946118},
  	doi = {10.1080/13645579.2021.1946118},
  	pages = {1--14},
  	journaltitle = {International Journal of Social Research Methodology},
  	shortjournal = {International Journal of Social Research Methodology},
  	author = {Barbrook-Johnson, Pete and Carrick, Jayne},
  	urldate = {2021-09-05},
  	date = {2021-07-10},
  	langid = {english},
  	keywords = {Complexity},
  	file = {Barbrook-Johnson_Carrick_2021_Combining complexity-framed research methods for social research.pdf:/Users/jaime/Zotero/storage/8EDX8U4D/Barbrook-Johnson_Carrick_2021_Combining complexity-framed research methods for social research.pdf:application/pdf},
  }

  @article{Asano2021,
  	title = {Emergent inequality and business cycles in a simple behavioral macroeconomic model},
  	volume = {118},
  	issn = {0027-8424, 1091-6490},
  	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.2025721118},
  	doi = {10.1073/pnas.2025721118},
  	abstract = {Standard macroeconomic models assume that households are rational in the sense that they are perfect utility maximizers and explain economic dynamics in terms of shocks that drive the economy away from the steady state. Here we build on a standard macroeconomic model in which a single rational representative household makes a savings decision of how much to consume or invest. In our model, households are myopic boundedly rational heterogeneous agents embedded in a social network. From time to time each household updates its savings rate by copying the savings rate of its neighbor with the highest consumption. If the updating time is short, the economy is stuck in a poverty trap, but for longer updating times economic output approaches its optimal value, and we observe a critical transition to an economy with irregular endogenous oscillations in economic output, resembling a business cycle. In this regime households divide into two groups: poor households with low savings rates and rich households with high savings rates. Thus, inequality and economic dynamics both occur spontaneously as a consequence of imperfect household decision-making. Adding a few “rational” agents with a fixed savings rate equal to the long-term optimum allows us to match business cycle timescales. Our work here supports an alternative program of research that substitutes utility maximization for behaviorally grounded decision-making.},
  	pages = {e2025721118},
  	number = {27},
  	journaltitle = {Proceedings of the National Academy of Sciences},
  	shortjournal = {Proc Natl Acad Sci {USA}},
  	author = {Asano, Yuki M. and Kolb, Jakob J. and Heitzig, Jobst and Farmer, J. Doyne},
  	urldate = {2021-09-05},
  	date = {2021-07-06},
  	langid = {english},
  	file = {Asano et al_2021_Emergent inequality and business cycles in a simple behavioral macroeconomic.pdf:/Users/jaime/Zotero/storage/X42DCRX6/Asano et al_2021_Emergent inequality and business cycles in a simple behavioral macroeconomic.pdf:application/pdf},
  }

  @article{Kanervisto2020,
  	title = {Action Space Shaping in Deep Reinforcement Learning},
  	url = {http://arxiv.org/abs/2004.00980},
  	abstract = {Reinforcement learning ({RL}) has been successful in training agents in various learning environments, including video-games. However, such work modifies and shrinks the action space from the game's original. This is to avoid trying "pointless" actions and to ease the implementation. Currently, this is mostly done based on intuition, with little systematic research supporting the design decisions. In this work, we aim to gain insight on these action space modifications by conducting extensive experiments in video-game environments. Our results show how domain-specific removal of actions and discretization of continuous actions can be crucial for successful learning. With these insights, we hope to ease the use of {RL} in new environments, by clarifying what action-spaces are easy to learn.},
  	journaltitle = {{arXiv}:2004.00980 [cs]},
  	author = {Kanervisto, Anssi and Scheller, Christian and Hautamäki, Ville},
  	urldate = {2021-09-10},
  	date = {2020-05-26},
  	eprinttype = {arxiv},
  	eprint = {2004.00980},
  	keywords = {reinforcement learning, Computer Science - Artificial Intelligence, action space, dml},
  	file = {arXiv.org Snapshot:/Users/jaime/Zotero/storage/QRN6TXXK/2004.html:text/html;Kanervisto et al_2020_Action Space Shaping in Deep Reinforcement Learning.pdf:/Users/jaime/Zotero/storage/PAHUEABE/Kanervisto et al_2020_Action Space Shaping in Deep Reinforcement Learning.pdf:application/pdf},
  }

  @article{Chandak2020,
  	title = {Reinforcement Learning When All Actions are Not Always Available},
  	url = {http://arxiv.org/abs/1906.01772},
  	abstract = {The Markov decision process ({MDP}) formulation used to model many real-world sequential decision making problems does not efficiently capture the setting where the set of available decisions (actions) at each time step is stochastic. Recently, the stochastic action set Markov decision process ({SAS}-{MDP}) formulation has been proposed, which better captures the concept of a stochastic action set. In this paper we argue that existing {RL} algorithms for {SAS}-{MDPs} can suffer from potential divergence issues, and present new policy gradient algorithms for {SAS}-{MDPs} that incorporate variance reduction techniques unique to this setting, and provide conditions for their convergence. We conclude with experiments that demonstrate the practicality of our approaches on tasks inspired by real-life use cases wherein the action set is stochastic.},
  	journaltitle = {{arXiv}:1906.01772 [cs, stat]},
  	author = {Chandak, Yash and Theocharous, Georgios and Metevier, Blossom and Thomas, Philip S.},
  	urldate = {2021-09-10},
  	date = {2020-01-20},
  	eprinttype = {arxiv},
  	eprint = {1906.01772},
  	keywords = {reinforcement learning, action space, Computer Science - Machine Learning, Statistics - Machine Learning},
  	file = {arXiv.org Snapshot:/Users/jaime/Zotero/storage/AFZLXUPI/1906.html:text/html;Chandak et al_2020_Reinforcement Learning When All Actions are Not Always Available.pdf:/Users/jaime/Zotero/storage/T3887AGR/Chandak et al_2020_Reinforcement Learning When All Actions are Not Always Available.pdf:application/pdf},
  }

  @article{Boutilier2021,
  	title = {Planning and Learning with Stochastic Action Sets},
  	url = {http://arxiv.org/abs/1805.02363},
  	abstract = {In many practical uses of reinforcement learning ({RL}) the set of actions available at a given state is a random variable, with realizations governed by an exogenous stochastic process. Somewhat surprisingly, the foundations for such sequential decision processes have been unaddressed. In this work, we formalize and investigate {MDPs} with stochastic action sets ({SAS}-{MDPs}) to provide these foundations. We show that optimal policies and value functions in this model have a structure that admits a compact representation. From an {RL} perspective, we show that Q-learning with sampled action sets is sound. In model-based settings, we consider two important special cases: when individual actions are available with independent probabilities; and a sampling-based model for unknown distributions. We develop poly-time value and policy iteration methods for both cases; and in the first, we offer a poly-time linear programming solution.},
  	journaltitle = {{arXiv}:1805.02363 [cs]},
  	author = {Boutilier, Craig and Cohen, Alon and Daniely, Amit and Hassidim, Avinatan and Mansour, Yishay and Meshi, Ofer and Mladenov, Martin and Schuurmans, Dale},
  	urldate = {2021-09-10},
  	date = {2021-02-12},
  	eprinttype = {arxiv},
  	eprint = {1805.02363},
  	keywords = {reinforcement learning, Computer Science - Artificial Intelligence, action space},
  	file = {arXiv.org Snapshot:/Users/jaime/Zotero/storage/BT9NRNJ5/1805.html:text/html;Boutilier et al_2021_Planning and Learning with Stochastic Action Sets.pdf:/Users/jaime/Zotero/storage/K4VKGGUH/Boutilier et al_2021_Planning and Learning with Stochastic Action Sets.pdf:application/pdf},
  }

  @video{NFB2021,
  	title = {Small Is Beautiful: Impressions of Fritz Schumacher},
  	url = {https://www.youtube.com/watch?v=AIlsgMngyhE},
  	shorttitle = {Small Is Beautiful},
  	abstract = {This film is a short documentary portrait of economist, technologist and lecturer Fritz Schumacher. Up to age 45, Schumacher was dedicated to economic growth. Then he came to believe that the modern technological explosion had grown out of all proportion to human need. Author of Small Is Beautiful - A Study of Economics as if People Mattered and founder of the London-based Intermediate Technology Development Group, he championed the cause of "appropriate" technology. The film introduces us to this gentle revolutionary a few months before his death.

  Directed by Donald Brittain, Barrie Howells and Douglas Kiefer - 1978 {\textbar} 30 min

  Watch more free films on {NFB}.ca → http://bit.ly/{YThpNFB}
  Subscribe to our newsletter → http://bit.ly/{NFBnewsletter}

  Follow us on Facebook → http://bit.ly/{ytfbNFB}
  Follow us on Instagram → http://bit.ly/2FdmRol
  Follow us on Twitter → http://bit.ly/{yttwNFB}

  Download our free {iOS} Apps → http://apple.co/2dbva4h
  Download our free Android Apps → http://bit.ly/2dbvHmO},
  	author = {{NFB}},
  	urldate = {2021-09-10},
  	date = {2021-08-03},
  	keywords = {buddhism, degrowth, economics, human, small},
  }

  @misc{Hasselt2007,
  	title = {Reinforcement Learning in Continuous Action Spaces},
  	abstract = {Abstract — Quite some research has been done on Reinforcement Learning in continuous environments, but the research on problems where the actions can also be chosen from a continuous space is much more limited. We present a new class of algorithms named Continuous Actor Critic Learning Automaton ({CACLA}) that can handle continuous states and actions. The resulting algorithm is straightforward to implement. An experimental comparison is made between this algorithm and other algorithms that can handle continuous action spaces. These experiments show that {CACLA} performs much better than the other algorithms, especially when it is combined with a Gaussian exploration method. I.},
  	author = {Hasselt, Hado Van and Wiering, Marco A.},
  	date = {2007},
  	file = {Citeseer - Snapshot:/Users/jaime/Zotero/storage/ZPC6KPQ7/download.html:text/html;Hasselt_Wiering_Dynamic Programming and Reinforcement Learning (ADPRL 2007) Reinforcement.pdf:/Users/jaime/Zotero/storage/Y8FVH5UE/Hasselt_Wiering_Dynamic Programming and Reinforcement Learning (ADPRL 2007) Reinforcement.pdf:application/pdf},
  }

  @article{Sharma2021,
  	title = {Survey of Recent Multi-Agent Reinforcement Learning Algorithms Utilizing Centralized Training},
  	url = {http://arxiv.org/abs/2107.14316},
  	doi = {10.1117/12.2585808},
  	abstract = {Much work has been dedicated to the exploration of Multi-Agent Reinforcement Learning ({MARL}) paradigms implementing a centralized learning with decentralized execution ({CLDE}) approach to achieve human-like collaboration in cooperative tasks. Here, we discuss variations of centralized training and describe a recent survey of algorithmic approaches. The goal is to explore how different implementations of information sharing mechanism in centralized learning may give rise to distinct group coordinated behaviors in multi-agent systems performing cooperative tasks.},
  	pages = {84},
  	journaltitle = {Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications {III}},
  	author = {Sharma, Piyush K. and Fernandez, Rolando and Zaroukian, Erin and Dorothy, Michael and Basak, Anjon and Asher, Derrik E.},
  	urldate = {2021-09-10},
  	date = {2021-04-12},
  	eprinttype = {arxiv},
  	eprint = {2107.14316},
  	keywords = {reinforcement learning, Computer Science - Artificial Intelligence, action space, Computer Science - Machine Learning, Computer Science - Multiagent Systems},
  	file = {arXiv.org Snapshot:/Users/jaime/Zotero/storage/CE4TFGYH/2107.html:text/html;Sharma et al_2021_Survey of Recent Multi-Agent Reinforcement Learning Algorithms Utilizing.pdf:/Users/jaime/Zotero/storage/ZJ34MTN8/Sharma et al_2021_Survey of Recent Multi-Agent Reinforcement Learning Algorithms Utilizing.pdf:application/pdf},
  }

  @inproceedings{Jin2020a,
  	location = {Cham},
  	title = {An Improved {CMA}-{ES} for Solving Large Scale Optimization Problem},
  	isbn = {978-3-030-53956-6},
  	doi = {10.1007/978-3-030-53956-6_34},
  	series = {Lecture Notes in Computer Science},
  	abstract = {In solving large scale optimization problems, {CMA}-{ES} has the disadvantages of high complexity and premature stagnation. To solve this problem, this paper proposes an improved {CMA}-{ES}, called {GI}-{ES}, for large-scale optimization problems. {GI}-{ES} uses all the historical information of the previous generation of individuals to evaluate the parameters of the distribution of the next generation. These estimates can be considered as approximate gradient information, which complete covariance information is not required. Thus {GI}-{ES} is friendly to large scale optimization problems. Comparative experiments have been done on state-of-the-art algorithms. The results proved the effectiveness and efficiency of {GI}-{ES} for large scale optimization problems.},
  	pages = {386--396},
  	booktitle = {Advances in Swarm Intelligence},
  	publisher = {Springer International Publishing},
  	author = {Jin, Jin and Yang, Chuan and Zhang, Yi},
  	editor = {Tan, Ying and Shi, Yuhui and Tuba, Milan},
  	date = {2020},
  	langid = {english},
  	keywords = {Approximate gradients, {CMA}-{ES}, Information utilization, Large scale optimization, cma-es},
  	file = {Jin et al_2020_An Improved CMA-ES for Solving Large Scale Optimization Problem.pdf:/Users/jaime/Zotero/storage/FPRCT4RN/Jin et al_2020_An Improved CMA-ES for Solving Large Scale Optimization Problem.pdf:application/pdf},
  }

  @inproceedings{Zahavy2018,
  	title = {Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning},
  	volume = {31},
  	url = {https://papers.nips.cc/paper/2018/hash/645098b086d2f9e1e0e939c27f9f2d6f-Abstract.html},
  	shorttitle = {Learn What Not to Learn},
  	booktitle = {Advances in Neural Information Processing Systems},
  	publisher = {Curran Associates, Inc.},
  	author = {Zahavy, Tom and Haroush, Matan and Merlis, Nadav and Mankowitz, Daniel J and Mannor, Shie},
  	urldate = {2021-09-10},
  	date = {2018},
  	keywords = {reinforcement learning, action space},
  	file = {Zahavy et al_2018_Learn What Not to Learn.pdf:/Users/jaime/Zotero/storage/CARS4E47/Zahavy et al_2018_Learn What Not to Learn.pdf:application/pdf},
  }

  @article{Dulac-Arnold2016,
  	title = {Deep Reinforcement Learning in Large Discrete Action Spaces},
  	url = {http://arxiv.org/abs/1512.07679},
  	abstract = {Being able to reason in an environment with a large number of discrete actions is essential to bringing reinforcement learning to a larger class of problems. Recommender systems, industrial plants and language models are only some of the many real-world tasks involving large numbers of discrete actions for which current methods are difficult or even often impossible to apply. An ability to generalize over the set of actions as well as sub-linear complexity relative to the size of the set are both necessary to handle such tasks. Current approaches are not able to provide both of these, which motivates the work in this paper. Our proposed approach leverages prior information about the actions to embed them in a continuous space upon which it can generalize. Additionally, approximate nearest-neighbor methods allow for logarithmic-time lookup complexity relative to the number of actions, which is necessary for time-wise tractable training. This combined approach allows reinforcement learning methods to be applied to large-scale learning problems previously intractable with current methods. We demonstrate our algorithm's abilities on a series of tasks having up to one million actions.},
  	journaltitle = {{arXiv}:1512.07679 [cs, stat]},
  	author = {Dulac-Arnold, Gabriel and Evans, Richard and van Hasselt, Hado and Sunehag, Peter and Lillicrap, Timothy and Hunt, Jonathan and Mann, Timothy and Weber, Theophane and Degris, Thomas and Coppin, Ben},
  	urldate = {2021-09-10},
  	date = {2016-04-04},
  	eprinttype = {arxiv},
  	eprint = {1512.07679},
  	keywords = {reinforcement learning, Computer Science - Artificial Intelligence, action space, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
  	file = {arXiv.org Snapshot:/Users/jaime/Zotero/storage/LTU82TFF/1512.html:text/html;Dulac-Arnold et al_2016_Deep Reinforcement Learning in Large Discrete Action Spaces.pdf:/Users/jaime/Zotero/storage/2BWTRP3U/Dulac-Arnold et al_2016_Deep Reinforcement Learning in Large Discrete Action Spaces.pdf:application/pdf},
  }

  @article{Ammanabrolu2020,
  	title = {Graph Constrained Reinforcement Learning for Natural Language Action Spaces},
  	url = {http://arxiv.org/abs/2001.08837},
  	abstract = {Interactive Fiction games are text-based simulations in which an agent interacts with the world purely through natural language. They are ideal environments for studying how to extend reinforcement learning agents to meet the challenges of natural language understanding, partial observability, and action generation in combinatorially-large text-based action spaces. We present {KG}-A2C, an agent that builds a dynamic knowledge graph while exploring and generates actions using a template-based action space. We contend that the dual uses of the knowledge graph to reason about game state and to constrain natural language generation are the keys to scalable exploration of combinatorially large natural language actions. Results across a wide variety of {IF} games show that {KG}-A2C outperforms current {IF} agents despite the exponential increase in action space size.},
  	journaltitle = {{arXiv}:2001.08837 [cs, stat]},
  	author = {Ammanabrolu, Prithviraj and Hausknecht, Matthew},
  	urldate = {2021-09-10},
  	date = {2020-01-23},
  	eprinttype = {arxiv},
  	eprint = {2001.08837},
  	keywords = {reinforcement learning, Computer Science - Artificial Intelligence, action space, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language, natural language},
  	file = {Ammanabrolu_Hausknecht_2020_Graph Constrained Reinforcement Learning for Natural Language Action Spaces.pdf:/Users/jaime/Zotero/storage/77A8AAGF/Ammanabrolu_Hausknecht_2020_Graph Constrained Reinforcement Learning for Natural Language Action Spaces.pdf:application/pdf;arXiv.org Snapshot:/Users/jaime/Zotero/storage/6XPKVMMP/2001.html:text/html},
  }

  @article{He2016,
  	title = {Deep Reinforcement Learning with a Natural Language Action Space},
  	url = {http://arxiv.org/abs/1511.04636},
  	abstract = {This paper introduces a novel architecture for reinforcement learning with deep neural networks designed to handle state and action spaces characterized by natural language, as found in text-based games. Termed a deep reinforcement relevance network ({DRRN}), the architecture represents action and state spaces with separate embedding vectors, which are combined with an interaction function to approximate the Q-function in reinforcement learning. We evaluate the {DRRN} on two popular text games, showing superior performance over other deep Q-learning architectures. Experiments with paraphrased action descriptions show that the model is extracting meaning rather than simply memorizing strings of text.},
  	journaltitle = {{arXiv}:1511.04636 [cs]},
  	author = {He, Ji and Chen, Jianshu and He, Xiaodong and Gao, Jianfeng and Li, Lihong and Deng, Li and Ostendorf, Mari},
  	urldate = {2021-09-10},
  	date = {2016-06-08},
  	eprinttype = {arxiv},
  	eprint = {1511.04636},
  	keywords = {reinforcement learning, Computer Science - Artificial Intelligence, action space, Computer Science - Machine Learning, Computer Science - Computation and Language, natural language},
  	file = {arXiv.org Snapshot:/Users/jaime/Zotero/storage/YTTN7XRF/1511.html:text/html;He et al_2016_Deep Reinforcement Learning with a Natural Language Action Space.pdf:/Users/jaime/Zotero/storage/JBEBYIT6/He et al_2016_Deep Reinforcement Learning with a Natural Language Action Space.pdf:application/pdf},
  }

  @inproceedings{Kanervisto2020a,
  	title = {Action Space Shaping in Deep Reinforcement Learning},
  	doi = {10.1109/CoG47356.2020.9231687},
  	abstract = {Reinforcement learning ({RL}) has been successful in training agents in various learning environments, including video games. However, such work modifies and shrinks the action space from the game's original. This is to avoid trying "pointless" actions and to ease the implementation. Currently, this is mostly done based on intuition, with little systematic research supporting the design decisions. In this work, we aim to gain insight on these action space modifications by conducting extensive experiments in video game environments. Our results show how domain-specific removal of actions and discretization of continuous actions can be crucial for successful learning. With these insights, we hope to ease the use of {RL} in new environments, by clarifying what action-spaces are easy to learn.},
  	eventtitle = {2020 {IEEE} Conference on Games ({CoG})},
  	pages = {479--486},
  	booktitle = {2020 {IEEE} Conference on Games ({CoG})},
  	author = {Kanervisto, Anssi and Scheller, Christian and Hautamäki, Ville},
  	date = {2020-08},
  	note = {{ISSN}: 2325-4289},
  	keywords = {Reinforcement learning, deep learning, reinforcement learning, action space, Cameras, Games, Keyboards, Mice, Poles and towers, shaping, Training, video game},
  	file = {IEEE Xplore Abstract Record:/Users/jaime/Zotero/storage/SRKXGSMT/9231687.html:text/html;Kanervisto et al_2020_Action Space Shaping in Deep Reinforcement Learning.pdf:/Users/jaime/Zotero/storage/C6AU8M9D/Kanervisto et al_2020_Action Space Shaping in Deep Reinforcement Learning.pdf:application/pdf},
  }

  @inproceedings{Guestrin2002,
  	title = {Coordinated Reinforcement Learning},
  	abstract = {We present several new algorithms for multiagent  reinforcement learning. A common feature of these  algorithms is a parameterized, structured representation  of a policy or value function. This structure  is leveraged in an approach we call coordinated reinforcement   learning, by which agents coordinate  both their action selection activities and their parameter  updates. Within the limits of our parametric  representations, the agents will determine  a jointly optimal action without explicitly considering  every possible action in their exponentially  large joint action space. Our methods differ from  many previous reinforcement learning approaches  to multiagent coordination in that structured communication  and coordination between agents appears  at the core of both the learning algorithm and  the execution architecture. Our experimental results,  comparing our approach to other {RL} methods,  illustrate both the quality of the policies obtained  and the additional benefits of coordination.},
  	pages = {227--234},
  	booktitle = {In Proceedings of the {ICML}-2002 The Nineteenth International Conference on Machine Learning},
  	author = {Guestrin, Carlos and Lagoudakis, Michail and Parr, Ronald},
  	date = {2002},
  	keywords = {reinforcement learning, cooperative, multi-agent},
  	file = {Citeseer - Snapshot:/Users/jaime/Zotero/storage/876W3LCH/download.html:text/html;Guestrin et al_2002_Coordinated Reinforcement Learning.pdf:/Users/jaime/Zotero/storage/856V3MGC/Guestrin et al_2002_Coordinated Reinforcement Learning.pdf:application/pdf},
  }

  @inproceedings{Gupta2017,
  	location = {Cham},
  	title = {Cooperative Multi-agent Control Using Deep Reinforcement Learning},
  	isbn = {978-3-319-71682-4},
  	doi = {10.1007/978-3-319-71682-4_5},
  	series = {Lecture Notes in Computer Science},
  	abstract = {This work considers the problem of learning cooperative policies in complex, partially observable domains without explicit communication. We extend three classes of single-agent deep reinforcement learning algorithms based on policy gradient, temporal-difference error, and actor-critic methods to cooperative multi-agent systems. To effectively scale these algorithms beyond a trivial number of agents, we combine them with a multi-agent variant of curriculum learning. The algorithms are benchmarked on a suite of cooperative control tasks, including tasks with discrete and continuous actions, as well as tasks with dozens of cooperating agents. We report the performance of the algorithms using different neural architectures, training procedures, and reward structures. We show that policy gradient methods tend to outperform both temporal-difference and actor-critic methods and that curriculum learning is vital to scaling reinforcement learning algorithms in complex multi-agent domains.},
  	pages = {66--83},
  	booktitle = {Autonomous Agents and Multiagent Systems},
  	publisher = {Springer International Publishing},
  	author = {Gupta, Jayesh K. and Egorov, Maxim and Kochenderfer, Mykel},
  	editor = {Sukthankar, Gita and Rodriguez-Aguilar, Juan A.},
  	date = {2017},
  	langid = {english},
  	keywords = {Reinforcement learning, read, multi-agent, marl},
  	file = {Gupta et al_2017_Cooperative Multi-agent Control Using Deep Reinforcement Learning.pdf:/Users/jaime/Zotero/storage/P2RE6L7R/Gupta et al_2017_Cooperative Multi-agent Control Using Deep Reinforcement Learning.pdf:application/pdf},
  }

  @incollection{Cosner2008,
  	location = {Berlin, Heidelberg},
  	title = {Reaction–Diffusion Equations and Ecological Modeling},
  	isbn = {978-3-540-74331-6},
  	url = {https://doi.org/10.1007/978-3-540-74331-6_3},
  	series = {Lecture Notes in Mathematics},
  	abstract = {Reaction-diffusion equations are widely used as models for spatial effects in ecology. They support three important types of ecological phenomena: the existence of a minimal patch size necessary to sustain a population, the propagation of wavefronts corresponding to biological invasions, and the formation of spatial patterns in the distributions of populations in homogeneous environments. Reaction-diffusion equations can be analyzed by means of methods from the theory of partial differential equations and dynamical systems. we will discuss the derivation of reaction-diffusion models in ecology, sketch the basic aspects of their analysis, and describe some of their applications and mathematical properties.},
  	pages = {77--115},
  	booktitle = {Tutorials in Mathematical Biosciences {IV}: Evolution and Ecology},
  	publisher = {Springer},
  	author = {Cosner, C.},
  	editor = {Friedman, Avner},
  	urldate = {2021-09-12},
  	date = {2008},
  	langid = {english},
  	doi = {10.1007/978-3-540-74331-6_3},
  	keywords = {ecology, dynamical systems, Biological Invasion, Comparison Principle, Ecological Modeling, Equilibrium Problem, Maximum Principle, partial differential equations, pde, reaction-diffusion},
  	file = {Cosner_2008_Reaction–Diffusion Equations and Ecological Modeling.pdf:/Users/jaime/Zotero/storage/38ATXUX4/Cosner_2008_Reaction–Diffusion Equations and Ecological Modeling.pdf:application/pdf},
  }

  @article{Ahilan2019,
  	title = {Feudal Multi-Agent Hierarchies for Cooperative Reinforcement Learning},
  	url = {http://arxiv.org/abs/1901.08492},
  	abstract = {We investigate how reinforcement learning agents can learn to cooperate. Drawing inspiration from human societies, in which successful coordination of many individuals is often facilitated by hierarchical organisation, we introduce Feudal Multi-agent Hierarchies ({FMH}). In this framework, a 'manager' agent, which is tasked with maximising the environmentally-determined reward function, learns to communicate subgoals to multiple, simultaneously-operating, 'worker' agents. Workers, which are rewarded for achieving managerial subgoals, take concurrent actions in the world. We outline the structure of {FMH} and demonstrate its potential for decentralised learning and control. We find that, given an adequate set of subgoals from which to choose, {FMH} performs, and particularly scales, substantially better than cooperative approaches that use a shared reward function.},
  	journaltitle = {{arXiv}:1901.08492 [cs]},
  	author = {Ahilan, Sanjeevan and Dayan, Peter},
  	urldate = {2021-09-12},
  	date = {2019-01-24},
  	eprinttype = {arxiv},
  	eprint = {1901.08492},
  	keywords = {reinforcement learning, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems, cooperative, multi-agent, feudal, marl},
  	file = {Ahilan and Dayan - Feudal Multi-Agent Hierarchies  for Cooperative Re.pdf:/Users/jaime/Zotero/storage/DB4UKQG6/Ahilan and Dayan - Feudal Multi-Agent Hierarchies  for Cooperative Re.pdf:application/pdf;arXiv.org Snapshot:/Users/jaime/Zotero/storage/VGC63WYL/1901.html:text/html},
  }

  @inproceedings{Grover2018,
  	title = {Learning Policy Representations in Multiagent Systems},
  	url = {https://proceedings.mlr.press/v80/grover18a.html},
  	eventtitle = {International Conference on Machine Learning},
  	pages = {1802--1811},
  	booktitle = {International Conference on Machine Learning},
  	publisher = {{PMLR}},
  	author = {Grover, Aditya and Al-Shedivat, Maruan and Gupta, Jayesh and Burda, Yuri and Edwards, Harrison},
  	urldate = {2021-09-13},
  	date = {2018-07-03},
  	langid = {english},
  	note = {{ISSN}: 2640-3498},
  	keywords = {multi-agent, marl},
  	file = {Grover et al_2018_Learning Policy Representations in Multiagent Systems.pdf:/Users/jaime/Zotero/storage/3BS8ABHC/Grover et al_2018_Learning Policy Representations in Multiagent Systems.pdf:application/pdf;Supplementary PDF:/Users/jaime/Zotero/storage/4XQG6FBN/Grover et al. - 2018 - Learning Policy Representations in Multiagent Syst.pdf:application/pdf},
  }

  @inproceedings{Dauce2020,
  	location = {Cham},
  	title = {Visual Search as Active Inference},
  	isbn = {978-3-030-64919-7},
  	doi = {10.1007/978-3-030-64919-7_17},
  	series = {Communications in Computer and Information Science},
  	abstract = {Visual search is an essential cognitive ability, offering a prototypical control problem to be addressed with Active Inference. Under a Naive Bayes assumption, the maximization of the information gain objective is consistent with the separation of the visual sensory flow in two independent pathways, namely the “What” and the “Where” pathways. On the “What” side, the processing of the central part of the visual field (the fovea) provides the current interpretation of the scene, here the category of the target. On the “Where” side, the processing of the full visual field (at lower resolution) is expected to provide hints about future central foveal processing given the potential realization of saccadic movements. A map of the classification accuracies, as obtained by such counterfactual saccades, defines a utility function on the motor space, whose maximal argument prescribes the next saccade. The comparison of the foveal and the peripheral predictions finally forms an estimate of the future information gain, providing a simple and resource-efficient way to implement information gain seeking policies in active vision. This dual-pathway information processing framework is found efficient on a synthetic visual search task with a variable (eccentricity-dependent) precision. More importantly, it is expected to draw connections toward a more general actor-critic principle in action selection, with the accuracy of the central processing taking the role of a value (or intrinsic reward) of the previous saccade.},
  	pages = {165--178},
  	booktitle = {Active Inference},
  	publisher = {Springer International Publishing},
  	author = {Daucé, Emmanuel and Perrinet, Laurent},
  	editor = {Verbelen, Tim and Lanillos, Pablo and Buckley, Christopher L. and De Boom, Cedric},
  	date = {2020},
  	langid = {english},
  	keywords = {Active Inference, Deep learning, Object detection, Visual search, Visuomotor control, active inference, visual search},
  	file = {Daucé_Perrinet_2020_Visual Search as Active Inference.pdf:/Users/jaime/Zotero/storage/4PGH3JLB/Daucé_Perrinet_2020_Visual Search as Active Inference.pdf:application/pdf},
  }

  @article{Choromanski2020,
  	title = {Rethinking Attention with Performers},
  	url = {https://arxiv.org/abs/2009.14794v3},
  	abstract = {We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach ({FAVOR}+), which may be of independent interest for scalable kernel methods. {FAVOR}+ can be also used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers.},
  	author = {Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and Belanger, David and Colwell, Lucy and Weller, Adrian},
  	urldate = {2021-09-13},
  	date = {2020-09-30},
  	langid = {english},
  	file = {Choromanski et al_2020_Rethinking Attention with Performers.pdf:/Users/jaime/Zotero/storage/BTEW8I6Y/Choromanski et al_2020_Rethinking Attention with Performers.pdf:application/pdf;Snapshot:/Users/jaime/Zotero/storage/SRC88EL7/2009.html:text/html},
  }

  @inproceedings{Cobbe2019,
  	title = {Quantifying Generalization in Reinforcement Learning},
  	url = {https://proceedings.mlr.press/v97/cobbe19a.html},
  	eventtitle = {International Conference on Machine Learning},
  	pages = {1282--1289},
  	booktitle = {International Conference on Machine Learning},
  	publisher = {{PMLR}},
  	author = {Cobbe, Karl and Klimov, Oleg and Hesse, Chris and Kim, Taehoon and Schulman, John},
  	urldate = {2021-09-13},
  	date = {2019-05-24},
  	langid = {english},
  	note = {{ISSN}: 2640-3498},
  	keywords = {reinforcement learning, generalisation, rl},
  	file = {Cobbe et al_2019_Quantifying Generalization in Reinforcement Learning.pdf:/Users/jaime/Zotero/storage/W2KYXWVC/Cobbe et al_2019_Quantifying Generalization in Reinforcement Learning.pdf:application/pdf;Supplementary PDF:/Users/jaime/Zotero/storage/2ZIHWQ9Q/Cobbe et al. - 2019 - Quantifying Generalization in Reinforcement Learni.pdf:application/pdf},
  }

  @article{Lowe2020,
  	title = {Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments},
  	url = {http://arxiv.org/abs/1706.02275},
  	abstract = {We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.},
  	journaltitle = {{arXiv}:1706.02275 [cs]},
  	author = {Lowe, Ryan and Wu, Yi and Tamar, Aviv and Harb, Jean and Abbeel, Pieter and Mordatch, Igor},
  	urldate = {2021-09-13},
  	date = {2020-03-14},
  	eprinttype = {arxiv},
  	eprint = {1706.02275},
  	keywords = {reinforcement learning, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, marl, rl, competition, cooperation},
  	file = {arXiv.org Snapshot:/Users/jaime/Zotero/storage/X8BB5UQ5/1706.html:text/html;Lowe et al_2020_Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments.pdf:/Users/jaime/Zotero/storage/7IVYU4D7/Lowe et al_2020_Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments.pdf:application/pdf},
  }

  @article{Leibo2017,
  	title = {Multi-agent Reinforcement Learning in Sequential Social Dilemmas},
  	url = {http://arxiv.org/abs/1702.03037},
  	abstract = {Matrix games like Prisoner's Dilemma have guided research on social dilemmas for decades. However, they necessarily treat the choice to cooperate or defect as an atomic action. In real-world social dilemmas these choices are temporally extended. Cooperativeness is a property that applies to policies, not elementary actions. We introduce sequential social dilemmas that share the mixed incentive structure of matrix game social dilemmas but also require agents to learn policies that implement their strategic intentions. We analyze the dynamics of policies learned by multiple self-interested independent learning agents, each using its own deep Q-network, on two Markov games we introduce here: 1. a fruit Gathering game and 2. a Wolfpack hunting game. We characterize how learned behavior in each domain changes as a function of environmental factors including resource abundance. Our experiments show how conflict can emerge from competition over shared resources and shed light on how the sequential nature of real world social dilemmas affects cooperation.},
  	journaltitle = {{arXiv}:1702.03037 [cs]},
  	author = {Leibo, Joel Z. and Zambaldi, Vinicius and Lanctot, Marc and Marecki, Janusz and Graepel, Thore},
  	urldate = {2021-09-13},
  	date = {2017-02-09},
  	eprinttype = {arxiv},
  	eprint = {1702.03037},
  	keywords = {reinforcement learning, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems, multi-agent, marl, Computer Science - Computer Science and Game Theory, social dilemma},
  	file = {arXiv.org Snapshot:/Users/jaime/Zotero/storage/9WVQAYXF/1702.html:text/html;Leibo et al_2017_Multi-agent Reinforcement Learning in Sequential Social Dilemmas.pdf:/Users/jaime/Zotero/storage/YAYSKEW3/Leibo et al_2017_Multi-agent Reinforcement Learning in Sequential Social Dilemmas.pdf:application/pdf},
  }

  @incollection{Busoniu2010,
  	location = {Berlin, Heidelberg},
  	title = {Multi-agent Reinforcement Learning: An Overview},
  	isbn = {978-3-642-14435-6},
  	url = {https://doi.org/10.1007/978-3-642-14435-6_7},
  	series = {Studies in Computational Intelligence},
  	shorttitle = {Multi-agent Reinforcement Learning},
  	abstract = {Multi-agent systems can be used to address problems in a variety of domains, including robotics, distributed control, telecommunications, and economics. The complexity of many tasks arising in these domains makes them difficult to solve with preprogrammed agent behaviors. The agents must instead discover a solution on their own, using learning. A significant part of the research on multi-agent learning concerns reinforcement learning techniques. This chapter reviews a representative selection of multi-agent reinforcement learning algorithms for fully cooperative, fully competitive, and more general (neither cooperative nor competitive) tasks. The benefits and challenges of multi-agent reinforcement learning are described. A central challenge in the field is the formal statement of a multi-agent learning goal; this chapter reviews the learning goals proposed in the literature. The problem domains where multi-agent reinforcement learning techniques have been applied are briefly discussed. Several multi-agent reinforcement learning algorithms are applied to an illustrative example involving the coordinated transportation of an object by two cooperative robots. In an outlook for the multi-agent reinforcement learning field, a set of important open issues are identified, and promising research directions to address these issues are outlined.},
  	pages = {183--221},
  	booktitle = {Innovations in Multi-Agent Systems and Applications - 1},
  	publisher = {Springer},
  	author = {Buşoniu, Lucian and Babuška, Robert and De Schutter, Bart},
  	editor = {Srinivasan, Dipti and Jain, Lakhmi C.},
  	urldate = {2021-09-13},
  	date = {2010},
  	langid = {english},
  	doi = {10.1007/978-3-642-14435-6_7},
  	keywords = {Reinforcement Learning, reinforcement learning, marl, Markov Decision Process, Multiagent System, Nash Equilibrium, Reward Function},
  	file = {Buşoniu et al_2010_Multi-agent Reinforcement Learning.pdf:/Users/jaime/Zotero/storage/9ASH6ZGZ/Buşoniu et al_2010_Multi-agent Reinforcement Learning.pdf:application/pdf},
  }

  @article{Tang2021,
  	title = {The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning},
  	url = {http://arxiv.org/abs/2109.02869},
  	shorttitle = {The Sensory Neuron as a Transformer},
  	abstract = {In complex systems, we often observe complex global behavior emerge from a collection of agents interacting with each other in their environment, with each individual agent acting only on locally available information, without knowing the full picture. Such systems have inspired development of artificial intelligence algorithms in areas such as swarm optimization and cellular automata. Motivated by the emergence of collective behavior from complex cellular systems, we build systems that feed each sensory input from the environment into distinct, but identical neural networks, each with no fixed relationship with one another. We show that these sensory networks can be trained to integrate information received locally, and through communication via an attention mechanism, can collectively produce a globally coherent policy. Moreover, the system can still perform its task even if the ordering of its inputs is randomly permuted several times during an episode. These permutation invariant systems also display useful robustness and generalization properties that are broadly applicable. Interactive demo and videos of our results: https://attentionneuron.github.io/},
  	journaltitle = {{arXiv}:2109.02869 [cs]},
  	author = {Tang, Yujin and Ha, David},
  	urldate = {2021-09-13},
  	date = {2021-09-07},
  	eprinttype = {arxiv},
  	eprint = {2109.02869},
  	keywords = {Computer Science - Neural and Evolutionary Computing},
  	file = {arXiv.org Snapshot:/Users/jaime/Zotero/storage/EK4YVRRU/2109.html:text/html;Tang_Ha_2021_The Sensory Neuron as a Transformer.pdf:/Users/jaime/Zotero/storage/GNKL8FME/Tang_Ha_2021_The Sensory Neuron as a Transformer.pdf:application/pdf},
  }

  @inproceedings{Ha2021,
  	title = {World Models and Attention for Reinforcement Learning},
  	url = {https://direct.mit.edu/isal/article/doi/10.1162/isal_a_00471/102973/World-Models-and-Attention-for-Reinforcement},
  	doi = {10.1162/isal_a_00471},
  	eventtitle = {{ALIFE} 2021: The 2021 Conference on Artificial Life},
  	publisher = {{MIT} Press},
  	author = {Ha, David},
  	urldate = {2021-09-13},
  	date = {2021-07-19},
  	langid = {english},
  	file = {Ha_2021_World Models and Attention for Reinforcement Learning.pdf:/Users/jaime/Zotero/storage/IV9QGSN4/Ha_2021_World Models and Attention for Reinforcement Learning.pdf:application/pdf;Snapshot:/Users/jaime/Zotero/storage/2KFC6TJS/102973.html:text/html},
  }

  @inproceedings{Gershenson2021,
  	title = {On the Scales of Selves: Information, Life, and Buddhist Philosophy},
  	url = {https://direct.mit.edu/isal/article/doi/10.1162/isal_a_00402/102919/On-the-Scales-of-Selves-Information-Life-and},
  	doi = {10.1162/isal_a_00402},
  	shorttitle = {On the Scales of Selves},
  	eventtitle = {{ALIFE} 2021: The 2021 Conference on Artificial Life},
  	publisher = {{MIT} Press},
  	author = {Gershenson, Carlos},
  	urldate = {2021-09-13},
  	date = {2021-07-19},
  	langid = {english},
  	keywords = {alife, illusion, ontology, self},
  	file = {Gershenson_2021_On the Scales of Selves.pdf:/Users/jaime/Zotero/storage/RCSJC7AT/Gershenson_2021_On the Scales of Selves.pdf:application/pdf;Snapshot:/Users/jaime/Zotero/storage/WXDPBW7P/102919.html:text/html},
  }

  @inproceedings{Lopes2021,
  	title = {Monks, Labs, Cyborgs: the Plasticity of Personhood in Tibetan Buddhism},
  	url = {https://direct.mit.edu/isal/article/doi/10.1162/isal_a_00455/102866/Monks-Labs-Cyborgs-the-Plasticity-of-Personhood-in},
  	doi = {10.1162/isal_a_00455},
  	shorttitle = {Monks, Labs, Cyborgs},
  	eventtitle = {{ALIFE} 2021: The 2021 Conference on Artificial Life},
  	publisher = {{MIT} Press},
  	author = {Lopes, Ana Cristina O.},
  	urldate = {2021-09-13},
  	date = {2021-07-19},
  	langid = {english},
  	keywords = {alife, ontology, self},
  	file = {Lopes_2021_Monks, Labs, Cyborgs.pdf:/Users/jaime/Zotero/storage/WXTZAQF6/Lopes_2021_Monks, Labs, Cyborgs.pdf:application/pdf;Snapshot:/Users/jaime/Zotero/storage/A75FYKUF/102866.html:text/html},
  }

  @inproceedings{Doctor2021,
  	title = {Can Being Aware of the Illusion of Self Augment an Agent's Affordances: Integrating Buddhist Philosophy, Cognitive Science, and Artificial Life},
  	url = {https://direct.mit.edu/isal/article/doi/10.1162/isal_a_00465/102894/Can-Being-Aware-of-the-Illusion-of-Self-Augment-an},
  	doi = {10.1162/isal_a_00465},
  	shorttitle = {Can Being Aware of the Illusion of Self Augment an Agent's Affordances},
  	eventtitle = {{ALIFE} 2021: The 2021 Conference on Artificial Life},
  	publisher = {{MIT} Press},
  	author = {Doctor, Thomas and Solomonova, Elizaveta and Duane, Bill and Witkowski, Olaf},
  	urldate = {2021-09-13},
  	date = {2021-07-19},
  	langid = {english},
  	keywords = {alife, ontology, self},
  	file = {Doctor et al_2021_Can Being Aware of the Illusion of Self Augment an Agent's Affordances.pdf:/Users/jaime/Zotero/storage/82HPHGMT/Doctor et al_2021_Can Being Aware of the Illusion of Self Augment an Agent's Affordances.pdf:application/pdf;Snapshot:/Users/jaime/Zotero/storage/4EEGBR5D/102894.html:text/html},
  }

  @online{zotero-1717,
  	title = {‪Design and control of self-organizing systems‬},
  	url = {https://scholar.google.com.mx/citations?view_op=view_citation&hl=en&user=fBRKCewAAAAJ&citation_for_view=fBRKCewAAAAJ:9yKSN-GCB0IC},
  	abstract = {‪C Gershenson, 2007‬ - ‪Cited by 313‬},
  	urldate = {2021-09-13},
  	keywords = {multi-agent, alife, control, self-organising},
  	file = {Snapshot:/Users/jaime/Zotero/storage/7BNABDAA/citations.html:text/html},
  }

  @article{Gershenson2020,
  	title = {Self-Organization and Artificial Life},
  	volume = {26},
  	issn = {1064-5462},
  	url = {https://doi.org/10.1162/artl_a_00324},
  	doi = {10.1162/artl_a_00324},
  	abstract = {Self-organization can be broadly defined as the ability of a system to display ordered spatiotemporal patterns solely as the result of the interactions among the system components. Processes of this kind characterize both living and artificial systems, making self-organization a concept that is at the basis of several disciplines, from physics to biology and engineering. Placed at the frontiers between disciplines, artificial life ({ALife}) has heavily borrowed concepts and tools from the study of self-organization, providing mechanistic interpretations of lifelike phenomena as well as useful constructivist approaches to artificial system design. Despite its broad usage within {ALife}, the concept of self-organization has been often excessively stretched or misinterpreted, calling for a clarification that could help with tracing the borders between what can and cannot be considered self-organization. In this review, we discuss the fundamental aspects of self-organization and list the main usages within three primary {ALife} domains, namely “soft” (mathematical/computational modeling), “hard” (physical robots), and “wet” (chemical/biological systems) {ALife}. We also provide a classification to locate this research. Finally, we discuss the usefulness of self-organization and related concepts within {ALife} studies, point to perspectives and challenges for future research, and list open questions. We hope that this work will motivate discussions related to self-organization in {ALife} and related fields.},
  	pages = {391--408},
  	number = {3},
  	journaltitle = {Artificial Life},
  	shortjournal = {Artificial Life},
  	author = {Gershenson, Carlos and Trianni, Vito and Werfel, Justin and Sayama, Hiroki},
  	urldate = {2021-09-13},
  	date = {2020-09-01},
  	file = {Gershenson et al_2020_Self-Organization and Artificial Life.pdf:/Users/jaime/Zotero/storage/ARG2J69A/Gershenson et al_2020_Self-Organization and Artificial Life.pdf:application/pdf;Snapshot:/Users/jaime/Zotero/storage/EW32FCBK/Self-Organization-and-Artificial-Life.html:text/html},
  }

  @inproceedings{Nicolis2021,
  	title = {Control of collective behaviours through artificial feedbacks},
  	url = {https://direct.mit.edu/isal/article/doi/10.1162/isal_a_00403/102918/Control-of-collective-behaviours-through},
  	doi = {10.1162/isal_a_00403},
  	eventtitle = {{ALIFE} 2021: The 2021 Conference on Artificial Life},
  	publisher = {{MIT} Press},
  	author = {Nicolis, Stamatios C. and Martín, Mariano Calvo and Campo, Alexandre and Deneubourg, Jean-Louis},
  	urldate = {2021-09-13},
  	date = {2021-07-19},
  	langid = {english},
  	keywords = {complex systems, artificial, social, systems},
  	file = {Nicolis et al_2021_Control of collective behaviours through artificial feedbacks.pdf:/Users/jaime/Zotero/storage/K4S99534/Nicolis et al_2021_Control of collective behaviours through artificial feedbacks.pdf:application/pdf;Snapshot:/Users/jaime/Zotero/storage/GUASNDTZ/102918.html:text/html},
  }

  @inproceedings{Sayama2021,
  	title = {How Artificial Life Researchers Can Help Address Complex Societal Challenges},
  	url = {https://direct.mit.edu/isal/article/doi/10.1162/isal_a_00467/102961/How-Artificial-Life-Researchers-Can-Help-Address},
  	doi = {10.1162/isal_a_00467},
  	eventtitle = {{ALIFE} 2021: The 2021 Conference on Artificial Life},
  	publisher = {{MIT} Press},
  	author = {Sayama, Hiroki},
  	urldate = {2021-09-13},
  	date = {2021-07-19},
  	langid = {english},
  	keywords = {social, challenges, complexity},
  	file = {Sayama_2021_How Artificial Life Researchers Can Help Address Complex Societal Challenges.pdf:/Users/jaime/Zotero/storage/K2FD93AX/Sayama_2021_How Artificial Life Researchers Can Help Address Complex Societal Challenges.pdf:application/pdf;Snapshot:/Users/jaime/Zotero/storage/22QPALGQ/102961.html:text/html},
  }

  @online{Seita,
  	title = {Scaling Multi-Agent Reinforcement Learning},
  	url = {http://bair.berkeley.edu/blog/2018/12/12/rllib/},
  	abstract = {The {BAIR} Blog},
  	titleaddon = {The Berkeley Artificial Intelligence Research Blog},
  	author = {Seita, Daniel},
  	urldate = {2021-09-13},
  	file = {Snapshot:/Users/jaime/Zotero/storage/TBFST956/rllib.html:text/html},
  }

  @article{Port2021,
  	title = {Deep Sensory Substitution: Noninvasively Enabling Biological Neural Networks to Receive Input from Artificial Neural Networks},
  	url = {http://arxiv.org/abs/2005.13291},
  	shorttitle = {Deep Sensory Substitution},
  	abstract = {As is expressed in the adage "a picture is worth a thousand words", when using spoken language to communicate visual information, brevity can be a challenge. This work describes a novel technique for leveraging machine-learned feature embeddings to sonify visual (and other types of) information into a perceptual audio domain, allowing users to perceive this information using only their aural faculty. The system uses a pretrained image embedding network to extract visual features and embed them in a compact subset of Euclidean space -- this converts the images into feature vectors whose \$L{\textasciicircum}2\$ distances can be used as a meaningful measure of similarity. A generative adversarial network ({GAN}) is then used to find a distance preserving map from this metric space of feature vectors into the metric space defined by a target audio dataset equipped with either the Euclidean metric or a mel-frequency cepstrum-based psychoacoustic distance metric. We demonstrate this technique by sonifying images of faces into human speech-like audio. For both target audio metrics, the {GAN} successfully found a metric preserving mapping, and in human subject tests, users were able to accurately classify audio sonifications of faces.},
  	journaltitle = {{arXiv}:2005.13291 [cs, eess, stat]},
  	author = {Port, Andrew and Kim, Chelhwon and Patel, Mitesh},
  	urldate = {2021-09-15},
  	date = {2021-08-25},
  	eprinttype = {arxiv},
  	eprint = {2005.13291},
  	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, assistive vision, sensory substitution},
  	file = {arXiv.org Snapshot:/Users/jaime/Zotero/storage/SU6RLAIZ/2005.html:text/html;Port et al_2021_Deep Sensory Substitution.pdf:/Users/jaime/Zotero/storage/9W78L9AG/Port et al_2021_Deep Sensory Substitution.pdf:application/pdf},
  }

  @article{Lerer2018,
  	title = {Maintaining cooperation in complex social dilemmas using deep reinforcement learning},
  	url = {http://arxiv.org/abs/1707.01068},
  	abstract = {Social dilemmas are situations where individuals face a temptation to increase their payoffs at a cost to total welfare. Building artificially intelligent agents that achieve good outcomes in these situations is important because many real world interactions include a tension between selfish interests and the welfare of others. We show how to modify modern reinforcement learning methods to construct agents that act in ways that are simple to understand, nice (begin by cooperating), provokable (try to avoid being exploited), and forgiving (try to return to mutual cooperation). We show both theoretically and experimentally that such agents can maintain cooperation in Markov social dilemmas. Our construction does not require training methods beyond a modification of self-play, thus if an environment is such that good strategies can be constructed in the zero-sum case (eg. Atari) then we can construct agents that solve social dilemmas in this environment.},
  	journaltitle = {{arXiv}:1707.01068 [cs]},
  	author = {Lerer, Adam and Peysakhovich, Alexander},
  	urldate = {2021-09-15},
  	date = {2018-03-02},
  	eprinttype = {arxiv},
  	eprint = {1707.01068},
  	keywords = {Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems, marl, cooperation, Computer Science - Computer Science and Game Theory, social dilemmas},
  	file = {arXiv.org Snapshot:/Users/jaime/Zotero/storage/MYVH4PE3/1707.html:text/html;Lerer_Peysakhovich_2018_Maintaining cooperation in complex social dilemmas using deep reinforcement.pdf:/Users/jaime/Zotero/storage/2BCU2JCU/Lerer_Peysakhovich_2018_Maintaining cooperation in complex social dilemmas using deep reinforcement.pdf:application/pdf},
  }

  @article{Peysakhovich2017,
  	title = {Prosocial learning agents solve generalized Stag Hunts better than selfish ones},
  	url = {http://arxiv.org/abs/1709.02865},
  	abstract = {Deep reinforcement learning has become an important paradigm for constructing agents that can enter complex multi-agent situations and improve their policies through experience. One commonly used technique is reactive training - applying standard {RL} methods while treating other agents as a part of the learner's environment. It is known that in general-sum games reactive training can lead groups of agents to converge to inefficient outcomes. We focus on one such class of environments: Stag Hunt games. Here agents either choose a risky cooperative policy (which leads to high payoffs if both choose it but low payoffs to an agent who attempts it alone) or a safe one (which leads to a safe payoff no matter what). We ask how we can change the learning rule of a single agent to improve its outcomes in Stag Hunts that include other reactive learners. We extend existing work on reward-shaping in multi-agent reinforcement learning and show that that making a single agent prosocial, that is, making them care about the rewards of their partners can increase the probability that groups converge to good outcomes. Thus, even if we control a single agent in a group making that agent prosocial can increase our agent's long-run payoff. We show experimentally that this result carries over to a variety of more complex environments with Stag Hunt-like dynamics including ones where agents must learn from raw input pixels.},
  	journaltitle = {{arXiv}:1709.02865 [cs]},
  	author = {Peysakhovich, Alexander and Lerer, Adam},
  	urldate = {2021-09-15},
  	date = {2017-12-08},
  	eprinttype = {arxiv},
  	eprint = {1709.02865},
  	keywords = {Computer Science - Artificial Intelligence, marl, cooperation, Computer Science - Computer Science and Game Theory, social},
  	file = {arXiv.org Snapshot:/Users/jaime/Zotero/storage/YK7XXLEH/1709.html:text/html;Peysakhovich_Lerer_2017_Prosocial learning agents solve generalized Stag Hunts better than selfish ones.pdf:/Users/jaime/Zotero/storage/DV52CLJS/Peysakhovich_Lerer_2017_Prosocial learning agents solve generalized Stag Hunts better than selfish ones.pdf:application/pdf},
  }

  @inproceedings{Kim2021,
  	title = {Rethinking the Self-Attention in Vision Transformers},
  	url = {https://openaccess.thecvf.com/content/CVPR2021W/ECV/html/Kim_Rethinking_the_Self-Attention_in_Vision_Transformers_CVPRW_2021_paper.html},
  	eventtitle = {Proceedings of the {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition},
  	pages = {3071--3075},
  	author = {Kim, Kyungmin and Wu, Bichen and Dai, Xiaoliang and Zhang, Peizhao and Yan, Zhicheng and Vajda, Peter and Kim, Seon Joo},
  	urldate = {2021-09-15},
  	date = {2021},
  	langid = {english},
  	keywords = {visualisation, self-attention},
  	file = {Kim et al_2021_Rethinking the Self-Attention in Vision Transformers.pdf:/Users/jaime/Zotero/storage/HAK4ZZTF/Kim et al_2021_Rethinking the Self-Attention in Vision Transformers.pdf:application/pdf;Snapshot:/Users/jaime/Zotero/storage/GJT24JB4/Kim_Rethinking_the_Self-Attention_in_Vision_Transformers_CVPRW_2021_paper.html:text/html},
  }

  @article{Ramachandran2019a,
  	title = {Stand-Alone Self-Attention in Vision Models},
  	url = {http://arxiv.org/abs/1906.05909},
  	abstract = {Convolutions are a fundamental building block of modern computer vision systems. Recent approaches have argued for going beyond convolutions in order to capture long-range dependencies. These efforts focus on augmenting convolutional models with content-based interactions, such as self-attention and non-local means, to achieve gains on a number of vision tasks. The natural question that arises is whether attention can be a stand-alone primitive for vision models instead of serving as just an augmentation on top of convolutions. In developing and testing a pure self-attention vision model, we verify that self-attention can indeed be an effective stand-alone layer. A simple procedure of replacing all instances of spatial convolutions with a form of self-attention applied to {ResNet} model produces a fully self-attentional model that outperforms the baseline on {ImageNet} classification with 12\% fewer {FLOPS} and 29\% fewer parameters. On {COCO} object detection, a pure self-attention model matches the {mAP} of a baseline {RetinaNet} while having 39\% fewer {FLOPS} and 34\% fewer parameters. Detailed ablation studies demonstrate that self-attention is especially impactful when used in later layers. These results establish that stand-alone self-attention is an important addition to the vision practitioner's toolbox.},
  	journaltitle = {{arXiv}:1906.05909 [cs]},
  	author = {Ramachandran, Prajit and Parmar, Niki and Vaswani, Ashish and Bello, Irwan and Levskaya, Anselm and Shlens, Jonathon},
  	urldate = {2021-09-15},
  	date = {2019-06-13},
  	eprinttype = {arxiv},
  	eprint = {1906.05909},
  	keywords = {visualisation, Computer Science - Computer Vision and Pattern Recognition, self-attention, vision},
  	file = {arXiv.org Snapshot:/Users/jaime/Zotero/storage/PE824M7M/1906.html:text/html;Ramachandran et al_2019_Stand-Alone Self-Attention in Vision Models.pdf:/Users/jaime/Zotero/storage/5PB7UMQB/Ramachandran et al_2019_Stand-Alone Self-Attention in Vision Models.pdf:application/pdf},
  }

  @article{Bloch2019,
  	title = {Advances in retinal prosthesis systems},
  	volume = {11},
  	issn = {2515-8414},
  	url = {https://doi.org/10.1177/2515841418817501},
  	doi = {10.1177/2515841418817501},
  	abstract = {Retinal prosthesis systems have undergone significant advances in the past quarter century, resulting in the development of several different novel surgical and engineering approaches. Encouraging results have demonstrated partial visual restoration, with improvement in both coarse objective function and performance of everyday tasks. To date, four systems have received marketing approval for use in Europe or the United States, with numerous others undergoing preclinical and clinical evaluation, reflecting the established safety profile of these devices for chronic implantation. This progress represents the first notion that the field of visual restorative medicine could offer blind patients a hope of real and measurable benefit. However, there are numerous complex engineering and biophysical obstacles still to be overcome, to reconcile the gap that remains between artificial and natural vision. Current developments in the form of enhanced image processing algorithms and data transfer approaches, combined with emerging nanofabrication and conductive polymerization techniques, herald an exciting and innovative future for retinal prosthetics. This review provides an update of retinal prosthetic systems currently undergoing development and clinical trials while also addressing future challenges in the field, such as the assessment of functional outcomes in ultra-low vision and strategies for tackling existing hardware and software constraints.},
  	pages = {2515841418817501},
  	journaltitle = {Therapeutic Advances in Ophthalmology},
  	shortjournal = {Ophthalmol Eye Dis},
  	author = {Bloch, Edward and Luo, Yvonne and da Cruz, Lyndon},
  	urldate = {2021-09-15},
  	date = {2019-01-01},
  	langid = {english},
  	note = {Publisher: {SAGE} Publications Ltd {STM}},
  	keywords = {retinal prosthesis, microelectrode, photovoltaic, tissue electronics, prostheses, review, survey},
  	file = {Bloch et al_2019_Advances in retinal prosthesis systems.pdf:/Users/jaime/Zotero/storage/R8M4MXNJ/Bloch et al_2019_Advances in retinal prosthesis systems.pdf:application/pdf},
  }

  @article{Waschkowski2014,
  	title = {Development of very large electrode arrays for epiretinal stimulation ({VLARS})},
  	volume = {13},
  	issn = {1475-925X},
  	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3976033/},
  	doi = {10.1186/1475-925X-13-11},
  	abstract = {Background
  Retinal implants have been developed to treat blindness causing retinal degenerations such as Retinitis pigmentosa ({RP}). The retinal stimulators are covering only a small portion of the retina usually in its center. To restore not only central vision but also a useful visual field retinal stimulators need to cover a larger area of the retina. However, large area retinal stimulators are much more difficult to implant into an eye. Some basic questions concerning this challenge should be answered in a series of experiments.

  Methods
  Large area retinal stimulators were fabricated as flexible multielectrode arrays ({MEAs}) using silicon technology with polyimide as the basic material for the substrate. Electrodes were made of gold covered with reactively sputtered iridium oxide. Several prototype designs were considered and implanted into enucleated porcine eyes. The prototype {MEAs} were also used as recording devices.

  Results
  Large area retinal stimulator {MEAs} were fabricated with a diameter of 12 mm covering a visual angle of 37.6° in a normal sighted human eye. The structures were flexible enough to be implanted in a folded state through an insertion nozzle. The implants could be positioned onto the retinal surface and fixated here using a retinal tack. Recording of spontaneous activity of retinal neurons was possible in vitro using these devices.

  Conclusions
  Large flexible {MEAs} covering a wider area of the retina as current devices could be fabricated using silicon technology with polyimide as a base material. Principal surgical techniques were established to insert such large devices into an eye and the devices could also be used for recording of retinal neural activity.},
  	pages = {11},
  	journaltitle = {{BioMedical} Engineering {OnLine}},
  	shortjournal = {Biomed Eng Online},
  	author = {Waschkowski, Florian and Hesse, Stephan and Rieck, Anne Christine and Lohmann, Tibor and Brockmann, Claudia and Laube, Thomas and Bornfeld, Norbert and Thumann, Gabriele and Walter, Peter and Mokwa, Wilfried and Johnen, Sandra and Roessler, Gernot},
  	urldate = {2021-09-15},
  	date = {2014-02-06},
  	pmid = {24502253},
  	pmcid = {PMC3976033},
  	file = {Waschkowski et al_2014_Development of very large electrode arrays for epiretinal stimulation (VLARS).pdf:/Users/jaime/Zotero/storage/6ZL5WZE8/Waschkowski et al_2014_Development of very large electrode arrays for epiretinal stimulation (VLARS).pdf:application/pdf},
  }

  @article{DeBruyne2021,
  	title = {A Tale of Two (and More) Altruists},
  	url = {http://arxiv.org/abs/2109.04324},
  	abstract = {We introduce a minimalist dynamical model of wealth evolution and wealth sharing among \$N\$ agents as a platform to compare the relative merits of altruism and individualism. In our model, the wealth of each agent independently evolves by diffusion. For a population of altruists, whenever any agent reaches zero wealth (that is, the agent goes bankrupt), the remaining wealth of the other \$N-1\$ agents is equally shared among all. The population is collectively defined to be bankrupt when its total wealth falls below a specified small threshold value. For individualists, each time an agent goes bankrupt (s)he is considered to be "dead" and no wealth redistribution occurs. We determine the evolution of wealth in these two societies. Altruism leads to more global median wealth at early times; eventually, however, the longest-lived individualists accumulate most of the wealth and are richer and more long lived than the altruists.},
  	journaltitle = {{arXiv}:2109.04324 [cond-mat, physics:physics, q-fin]},
  	author = {De Bruyne, B. and Randon-Furling, J. and Redner, S.},
  	urldate = {2021-09-15},
  	date = {2021-09-09},
  	eprinttype = {arxiv},
  	eprint = {2109.04324},
  	keywords = {Physics - Physics and Society, cooperation, social, Condensed Matter - Statistical Mechanics, Quantitative Finance - Statistical Finance, agent, altruism, dynamical},
  	file = {arXiv.org Snapshot:/Users/jaime/Zotero/storage/R6UZDBX6/2109.html:text/html;De Bruyne et al_2021_A Tale of Two (and More) Altruists.pdf:/Users/jaime/Zotero/storage/RCI36D7C/De Bruyne et al_2021_A Tale of Two (and More) Altruists.pdf:application/pdf},
  }

  @inproceedings{Braylan2015,
  	title = {Frame Skip Is a Powerful Parameter for Learning to Play Atari},
  	rights = {Authors who publish a paper in an  {AAAI} Technical Report  agree to the following terms:     Author(s) agree to grant to {AAAI} (1) the perpetual, nonexclusive world rights to use the submitted paper as part of an {AAAI} publication, in all languages and for all editions. (2) The right to use the paper, together with the author's name and pertinent biographical data, in advertising and promotion of it and the {AAAI} publication. (3) The right to publish or cause to be published the paper in connection with any republication of the {AAAI} publication in any medium including electronic. (4) The right to, and authorize others to, publish or cause to be published the paper in whole or in part, individually or in conjunction with other works, in any medium including electronic.   The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered.   The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify {AAAI}, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense {AAAI} may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to {AAAI} in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys’ fees incurred therein.   Author(s) retain all proprietary rights (such as patent rights).   In the event the above article/paper is not accepted and published by {AAAI}, or is withdrawn by the author(s) before acceptance by {AAAI}, this agreement becomes null and void.},
  	url = {https://www.aaai.org/ocs/index.php/WS/AAAIW15/paper/view/10156},
  	abstract = {We show that setting a reasonable frame skip can be critical to the performance of agents learning to play Atari 2600 games. In all of the six games in our experiments, frame skip is a strong determinant of success. For two of these games, setting a large frame skip leads to state-of-the-art performance.},
  	eventtitle = {Workshops at the Twenty-Ninth {AAAI} Conference on Artificial Intelligence},
  	booktitle = {Workshops at the Twenty-Ninth {AAAI} Conference on Artificial Intelligence},
  	author = {Braylan, Alex and Hollenbeck, Mark and Meyerson, Elliot and Miikkulainen, Risto},
  	urldate = {2021-09-16},
  	date = {2015-04-01},
  	langid = {english},
  	file = {Braylan et al_2015_Frame Skip Is a Powerful Parameter for Learning to Play Atari.pdf:/Users/jaime/Zotero/storage/2UQMYWKA/Braylan et al_2015_Frame Skip Is a Powerful Parameter for Learning to Play Atari.pdf:application/pdf;Snapshot:/Users/jaime/Zotero/storage/7KVL8GEZ/10156.html:text/html},
  }

  @inproceedings{Lample2017,
  	title = {Playing {FPS} Games with Deep Reinforcement Learning},
  	rights = {Authors who publish a paper in this conference agree to the following terms:   Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence ({AAAI}), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.  The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered.  The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify {AAAI}, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense {AAAI} may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to {AAAI} in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys’ fees incurred therein.  Author(s) retain all proprietary rights other than copyright (such as patent rights).  Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.  Author(s) may reproduce, or have reproduced, their article/paper for the author’s personal use, or for company use provided that {AAAI} copyright and the source are indicated, and that the copies are not used in a way that implies {AAAI} endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author’s employer, and then only on the author’s or the employer’s own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the {AAAI} electronic server, and shall not post other {AAAI} copyrighted materials not of the author’s or the employer’s creation (including tables of contents with links to other papers) without {AAAI}’s written permission.  Author(s) may make limited distribution of all or portions of their article/paper prior to publication.  In the case of work performed under U.S. Government contract, {AAAI} grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.  In the event the above article/paper is not accepted and published by {AAAI}, or is withdrawn by the author(s) before acceptance by {AAAI}, this agreement becomes null and void.},
  	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14456},
  	abstract = {Advances in deep reinforcement learning have allowed autonomous agents to perform well on Atari games, often outperforming humans, using only raw pixels to make their decisions. However, most of these games take place in 2D environments that are fully observable to the agent. In this paper, we present the first architecture to tackle 3D environments in first-person shooter games, that involve partially observable states. Typically, deep reinforcement learning methods only utilize visual input for training. We present a method to augment these models to exploit game feature information such as the presence of enemies or items, during the training phase. Our model is trained to simultaneously learn these features along with minimizing a Q-learning objective, which is shown to dramatically improve the training speed and performance of our agent. Our architecture is also modularized to allow different models to be independently trained for different phases of the game. We show that the proposed architecture substantially outperforms built-in {AI} agents of the game as well as average humans in deathmatch scenarios.},
  	eventtitle = {Thirty-First {AAAI} Conference on Artificial Intelligence},
  	booktitle = {Thirty-First {AAAI} Conference on Artificial Intelligence},
  	author = {Lample, Guillaume and Chaplot, Devendra Singh},
  	urldate = {2021-09-16},
  	date = {2017-02-13},
  	langid = {english},
  	file = {Lample_Chaplot_2017_Playing FPS Games with Deep Reinforcement Learning.pdf:/Users/jaime/Zotero/storage/UYUP2VU7/Lample_Chaplot_2017_Playing FPS Games with Deep Reinforcement Learning.pdf:application/pdf;Snapshot:/Users/jaime/Zotero/storage/BPTLH55H/14456.html:text/html},
  }

  @article{Striem-Amit2012,
  	title = {‘Visual’ Acuity of the Congenitally Blind Using Visual-to-Auditory Sensory Substitution},
  	volume = {7},
  	issn = {1932-6203},
  	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0033136},
  	doi = {10.1371/journal.pone.0033136},
  	abstract = {Sensory Substitution Devices ({SSDs}) convey visual information through sounds or touch, thus theoretically enabling a form of visual rehabilitation in the blind. However, for clinical use, these devices must provide fine-detailed visual information which was not yet shown for this or other means of visual restoration. To test the possible functional acuity conveyed by such devices, we used the Snellen acuity test conveyed through a high-resolution visual-to-auditory {SSD} (The {vOICe}). We show that congenitally fully blind adults can exceed the World Health Organization ({WHO}) blindness acuity threshold using {SSDs}, reaching the highest acuity reported yet with any visual rehabilitation approach. This demonstrates the potential capacity of {SSDs} as inexpensive, non-invasive visual rehabilitation aids, alone or when supplementing visual prostheses.},
  	pages = {e33136},
  	number = {3},
  	journaltitle = {{PLOS} {ONE}},
  	shortjournal = {{PLOS} {ONE}},
  	author = {Striem-Amit, Ella and Guendelman, Miriam and Amedi, Amir},
  	urldate = {2021-09-16},
  	date = {2012-03-16},
  	langid = {english},
  	note = {Publisher: Public Library of Science},
  	keywords = {Blindness, Emotions, Facial expressions, Medical implants, Prosthetics, Sensory perception, Vision, Visual acuity},
  	file = {Snapshot:/Users/jaime/Zotero/storage/V5UKGCIS/article.html:text/html;Striem-Amit et al_2012_‘Visual’ Acuity of the Congenitally Blind Using Visual-to-Auditory Sensory.pdf:/Users/jaime/Zotero/storage/C77J7N8M/Striem-Amit et al_2012_‘Visual’ Acuity of the Congenitally Blind Using Visual-to-Auditory Sensory.pdf:application/pdf},
  }

  @article{Renier2005,
  	title = {Cross-modal activation of visual cortex during depth perception using auditory substitution of vision},
  	volume = {26},
  	issn = {1053-8119},
  	doi = {10.1016/j.neuroimage.2005.01.047},
  	abstract = {Previous neuroimaging studies identified multimodal brain areas in the visual cortex that are specialized for processing specific information, such as visual-haptic object recognition. Here, we test whether visual brain areas are involved in depth perception when auditory substitution of vision is used. Nine sighted volunteers were trained blindfolded to use a prosthesis substituting vision with audition both to recognize two-dimensional figures and to estimate distance of an object in a real three-dimensional environment. Using positron emission tomography, regional cerebral blood flow was assessed while the prosthesis was used to explore virtual 3D images; subjects focused either on 2D features (target search) or on depth (target distance comparison). Activation foci were found in visual association areas during both the target search task, which recruited the occipito-parietal cortex, and the depth perception task, which recruited occipito-parietal and occipito-temporal areas. This indicates that some brain areas of the visual cortex are relatively multimodal and may be recruited for depth processing via a sense other than vision.},
  	pages = {573--580},
  	number = {2},
  	journaltitle = {{NeuroImage}},
  	shortjournal = {Neuroimage},
  	author = {Renier, Laurent and Collignon, Olivier and Poirier, Colline and Tranduy, Dai and Vanlierde, Annick and Bol, Anne and Veraart, Claude and De Volder, Anne G.},
  	date = {2005-06},
  	pmid = {15907314},
  	keywords = {Acoustic Stimulation, Adult, Auditory Perception, Cerebrovascular Circulation, Data Interpretation, Statistical, Depth Perception, Form Perception, Humans, Image Interpretation, Computer-Assisted, Male, Nerve Net, Neuronal Plasticity, Positron-Emission Tomography, Prostheses and Implants, Psychomotor Performance, Recruitment, Neurophysiological, Size Perception, Visual Cortex},
  	file = {Renier et al_2005_Cross-modal activation of visual cortex during depth perception using auditory.pdf:/Users/jaime/Zotero/storage/7ANLL9Z7/Renier et al_2005_Cross-modal activation of visual cortex during depth perception using auditory.pdf:application/pdf},
  }

  @article{Renier2010,
  	title = {Vision substitution and depth perception: early blind subjects experience visual perspective through their ears},
  	volume = {5},
  	issn = {1748-3115},
  	doi = {10.3109/17483100903253936},
  	shorttitle = {Vision substitution and depth perception},
  	abstract = {{AIM}: Sensory substitution ({SS}) represents a unique opportunity to provide congenitally blind persons with visual-like experience. Although visual experience influences the way we perceive the external world, little is known about the effects of {SS} experience.
  {PURPOSE}: To investigate the effects of perceptual experience (visual versus sensory substitution) on depth perception through an {SS} system, object localization abilities of early blind (n = 10), and blindfolded sighted control subjects (n = 20) were assessed before and after a practicing period with a visual-to-auditory {SS} device.
  {METHOD}: During the pre- and post-test, subjects had to replace, by hand, an object previously localized using the device. The practicing phase consisted of three sessions during which subjects tried to localize and grasp an object using the device. Results. At the pre-test, sighted subjects spontaneously used efficiently different pictorial depth cues to estimate object distance while the blind subjects were affected by their lack of visual experience and were significantly less accurate. Post-test showed that the brief practicing phase sufficed to enable blind subjects to acquire the rules of visual depth and to use them efficiently with the device.
  {CONCLUSIONS}: These results suggest the possibility to compensate for some effects of early and long-lasting blindness by providing visual-like experience via {SS}. Theoretical implications are discussed.},
  	pages = {175--183},
  	number = {3},
  	journaltitle = {Disability and Rehabilitation. Assistive Technology},
  	shortjournal = {Disabil Rehabil Assist Technol},
  	author = {Renier, Laurent and De Volder, Anne G.},
  	date = {2010-05},
  	pmid = {20214472},
  	keywords = {Adaptation, Physiological, Adult, Age Factors, Analysis of Variance, Blindness, Communication Aids for Disabled, Depth Perception, Distance Perception, Female, Humans, Male, Sensory Aids, Time Factors, Visual Perception},
  }

  @article{Kvansakul2020,
  	title = {Sensory augmentation to aid training with retinal prostheses},
  	volume = {17},
  	issn = {1741-2552},
  	doi = {10.1088/1741-2552/ab9e1d},
  	abstract = {{OBJECTIVE}: Retinal prosthesis recipients require rehabilitative training to learn the non-intuitive nature of prosthetic 'phosphene vision'. This study investigated whether the addition of auditory cues, using The {vOICe} sensory substitution device ({SSD}), could improve functional performance with simulated phosphene vision.
  {APPROACH}: Forty normally sighted subjects completed two visual tasks under three conditions. The phosphene condition converted the image to simulated phosphenes displayed on a virtual reality headset. The {SSD} condition provided auditory information via stereo headphones, translating the image into sound. Horizontal information was encoded as stereo timing differences between ears, vertical information as pitch, and pixel intensity as audio intensity. The third condition combined phosphenes and {SSD}. Tasks comprised light localisation from the Basic Assessment of Light and Motion ({BaLM}) and the Tumbling-E from the Freiburg Acuity and Contrast Test ({FrACT}). To examine learning effects, twenty of the forty subjects received {SSD} training prior to assessment.
  {MAIN} {RESULTS}: Combining phosphenes with auditory {SSD} provided better light localisation accuracy than either phosphenes or {SSD} alone, suggesting a compound benefit of integrating modalities. Although response times for {SSD}-only were significantly longer than all other conditions, combined condition response times were as fast as phosphene-only, highlighting that audio-visual integration provided both response time and accuracy benefits. Prior {SSD} training provided a benefit to localisation accuracy and speed in {SSD}-only (as expected) and Combined conditions compared to untrained {SSD}-only. Integration of the two modalities did not improve spatial resolution task performance, with resolution limited to that of the higher resolution modality ({SSD}).
  {SIGNIFICANCE}: Combining phosphene (visual) and {SSD} (auditory) modalities was effective even without {SSD} training and led to an improvement in light localisation accuracy and response times. Spatial resolution performance was dominated by auditory {SSD}. The results suggest there may be a benefit to including auditory cues when training vision prosthesis recipients.},
  	pages = {045001},
  	number = {4},
  	journaltitle = {Journal of Neural Engineering},
  	shortjournal = {J Neural Eng},
  	author = {Kvansakul, Jessica and Hamilton, Lachlan and Ayton, Lauren N. and {McCarthy}, Chris and Petoe, Matthew A.},
  	date = {2020-07-13},
  	pmid = {32554868},
  	keywords = {Humans, Phosphenes, Reaction Time, Task Performance and Analysis, Vision, Ocular, Visual Prosthesis},
  	file = {Kvansakul et al_2020_Sensory augmentation to aid training with retinal prostheses.pdf:/Users/jaime/Zotero/storage/3S75KVAF/Kvansakul et al_2020_Sensory augmentation to aid training with retinal prostheses.pdf:application/pdf},
  }

  @inproceedings{Zunino2015,
  	title = {Seeing the Sound: A New Multimodal Imaging Device for Computer Vision},
  	url = {https://www.cv-foundation.org/openaccess/content_iccv_2015_workshops/w20/html/Zunino_Seeing_the_Sound_ICCV_2015_paper.html},
  	shorttitle = {Seeing the Sound},
  	eventtitle = {Proceedings of the {IEEE} International Conference on Computer Vision Workshops},
  	pages = {6--14},
  	author = {Zunino, Andrea and Crocco, Marco and Martelli, Samuele and Trucco, Andrea and Del Bue, Alessio and Murino, Vittorio},
  	urldate = {2021-09-16},
  	date = {2015},
  	file = {Snapshot:/Users/jaime/Zotero/storage/S5LMD9VW/Zunino_Seeing_the_Sound_ICCV_2015_paper.html:text/html;Zunino et al_2015_Seeing the Sound.pdf:/Users/jaime/Zotero/storage/YB4F6QT4/Zunino et al_2015_Seeing the Sound.pdf:application/pdf},
  }

  @article{Arumugam2021,
  	title = {An Information-Theoretic Perspective on Credit Assignment in Reinforcement Learning},
  	url = {http://arxiv.org/abs/2103.06224},
  	abstract = {How do we formalize the challenge of credit assignment in reinforcement learning? Common intuition would draw attention to reward sparsity as a key contributor to difficult credit assignment and traditional heuristics would look to temporal recency for the solution, calling upon the classic eligibility trace. We posit that it is not the sparsity of the reward itself that causes difficulty in credit assignment, but rather the {\textbackslash}emph\{information sparsity\}. We propose to use information theory to define this notion, which we then use to characterize when credit assignment is an obstacle to efficient learning. With this perspective, we outline several information-theoretic mechanisms for measuring credit under a fixed behavior policy, highlighting the potential of information theory as a key tool towards provably-efficient credit assignment.},
  	journaltitle = {{arXiv}:2103.06224 [cs, math]},
  	author = {Arumugam, Dilip and Henderson, Peter and Bacon, Pierre-Luc},
  	urldate = {2021-09-19},
  	date = {2021-03-10},
  	eprinttype = {arxiv},
  	eprint = {2103.06224},
  	keywords = {Computer Science - Information Theory, Computer Science - Machine Learning, information theory, reinforcement learning, rewards},
  	file = {Arumugam et al_2021_An Information-Theoretic Perspective on Credit Assignment in Reinforcement.pdf:/Users/jaime/Zotero/storage/2N942VTV/Arumugam et al_2021_An Information-Theoretic Perspective on Credit Assignment in Reinforcement.pdf:application/pdf;arXiv.org Snapshot:/Users/jaime/Zotero/storage/TTE4WGBI/2103.html:text/html},
  }

  @inproceedings{Barnes2013,
  	title = {An overview of vision processing in implantable prosthetic vision},
  	doi = {10.1109/ICIP.2013.6738315},
  	abstract = {Electrically stimulating prosthetic vision devices offer a potential therapy to blind individuals. There are currently two multi-centre trials of devices by Second Sight Medical Products, and by Zrenner's group at University of Tuebingen. In Australia, Bionic Vision Australia has a retinal implant trial with three patients. Current implants provide restricted information for implantees, and some limitations are likely to remain in the future. To provide a substantial benefit to individual's abilities to perform key tasks such as orientation and mobility, activities of daily living, reading and face recognition there is much work to be done. Vision processing's role is to ensure the key visual information is available to undertake tasks given these limitations. This paper frames the background and challenges in vision processing for implantable prosthetic vision, and gives an overview of recent work.},
  	eventtitle = {2013 {IEEE} International Conference on Image Processing},
  	pages = {1532--1535},
  	booktitle = {2013 {IEEE} International Conference on Image Processing},
  	author = {Barnes, Nick},
  	date = {2013-09},
  	note = {{ISSN}: 2381-8549},
  	keywords = {blindness, retinal implants, Vision processing for implantable prosthetic vision, Visual prosthesis},
  	file = {Barnes_2013_An overview of vision processing in implantable prosthetic vision.pdf:/Users/jaime/Zotero/storage/UF29RN9R/Barnes_2013_An overview of vision processing in implantable prosthetic vision.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/jaime/Zotero/storage/YT3AHII5/6738315.html:text/html},
  }

  @article{Ayton2020,
  	title = {An update on retinal prostheses},
  	volume = {131},
  	issn = {1388-2457},
  	url = {https://www.sciencedirect.com/science/article/pii/S1388245719313264},
  	doi = {10.1016/j.clinph.2019.11.029},
  	abstract = {Retinal prostheses are designed to restore a basic sense of sight to people with profound vision loss. They require a relatively intact posterior visual pathway (optic nerve, lateral geniculate nucleus and visual cortex). Retinal implants are options for people with severe stages of retinal degenerative disease such as retinitis pigmentosa and age-related macular degeneration. There have now been three regulatory-approved retinal prostheses. Over five hundred patients have been implanted globally over the past 15 years. Devices generally provide an improved ability to localize high-contrast objects, navigate, and perform basic orientation tasks. Adverse events have included conjunctival erosion, retinal detachment, loss of light perception, and the need for revision surgery, but are rare. There are also specific device risks, including overstimulation (which could cause damage to the retina) or delamination of implanted components, but these are very unlikely. Current challenges include how to improve visual acuity, enlarge the field-of-view, and reduce a complex visual scene to its most salient components through image processing. This review encompasses the work of over 40 individual research groups who have built devices, developed stimulation strategies, or investigated the basic physiology underpinning retinal prostheses. Current technologies are summarized, along with future challenges that face the field.},
  	pages = {1383--1398},
  	number = {6},
  	journaltitle = {Clinical Neurophysiology},
  	shortjournal = {Clinical Neurophysiology},
  	author = {Ayton, Lauren N. and Barnes, Nick and Dagnelie, Gislin and Fujikado, Takashi and Goetz, Georges and Hornig, Ralf and Jones, Bryan W. and Muqit, Mahiul M. K. and Rathbun, Daniel L. and Stingl, Katarina and Weiland, James D. and Petoe, Matthew A.},
  	urldate = {2021-09-22},
  	date = {2020-06-01},
  	langid = {english},
  	keywords = {Ophthalmology, Retinal disease, Retinal prosthesis, Vision restoration},
  	file = {Ayton et al_2020_An update on retinal prostheses.pdf:/Users/jaime/Zotero/storage/XXWQEE4J/Ayton et al_2020_An update on retinal prostheses.pdf:application/pdf},
  }

  @inproceedings{Yun2018,
  	title = {Occluded object reconstruction for first responders with augmented reality glasses using conditional generative adversarial networks},
  	volume = {10649},
  	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10649/106490T/Occluded-object-reconstruction-for-first-responders-with-augmented-reality-glasses/.full},
  	doi = {10.1117/12.2305151},
  	abstract = {Firefighters suffer a variety of life-threatening risks, including line-of-duty deaths, injuries, and exposures to hazardous substances. Support for reducing these risks is important. We built a partially occluded object reconstruction method on augmented reality glasses for first responders. We used a deep learning based on conditional generative adversarial networks to train associations between the various images of flammable and hazardous objects and their partially occluded counterparts. Our system then reconstructed an image of a new flammable object. Finally, the reconstructed image was superimposed on the input image to provide "transparency". The system imitates human learning about the laws of physics through experience by learning the shape of flammable objects and the flame characteristics.},
  	eventtitle = {Pattern Recognition and Tracking {XXIX}},
  	pages = {225--231},
  	booktitle = {Pattern Recognition and Tracking {XXIX}},
  	publisher = {{SPIE}},
  	author = {Yun, Kyongsik and Lu, Thomas and Chow, Edward},
  	urldate = {2021-09-22},
  	date = {2018-04-30},
  	file = {Snapshot:/Users/jaime/Zotero/storage/D75I6Q9F/12.2305151.html:text/html;Yun et al_2018_Occluded object reconstruction for first responders with augmented reality.pdf:/Users/jaime/Zotero/storage/H7YWU9AZ/Yun et al_2018_Occluded object reconstruction for first responders with augmented reality.pdf:application/pdf},
  }

  @article{Bhattarai2020,
  	title = {A Deep Learning Framework for Detection of Targets in Thermal Images to Improve Firefighting},
  	volume = {8},
  	issn = {2169-3536},
  	doi = {10.1109/ACCESS.2020.2993767},
  	abstract = {Intelligent detection and processing capabilities can be instrumental in improving the safety, efficiency, and successful completion of rescue missions conducted by firefighters in emergency first response settings. The objective of this research is to create an automated system that is capable of real-time, intelligent object detection and recognition and facilitates the improved situational awareness of firefighters during an emergency response. We have explored state-of-the-art machine/deep learning techniques to achieve this objective. The goal of this work is to enhance the situational awareness of firefighters by effectively exploiting the infrared video that is actively recorded by firefighters on the scene. To accomplish this, we use a trained deep Convolutional Neural Network ({CNN}) system to classify and identify objects of interest from thermal imagery in real-time. In the midst of those critical circumstances created by a structure fire, this system is able to accurately inform the decision-making process of firefighters with up-to-date scene information by extracting, processing, and analyzing crucial information. Utilizing the new information produced by the framework, firefighters are able to make more informed inferences about the circumstances for their safe navigation through such hazardous and potentially catastrophic environments.},
  	pages = {88308--88321},
  	journaltitle = {{IEEE} Access},
  	author = {Bhattarai, Manish and {MartíNez}-Ramón, Manel},
  	date = {2020},
  	note = {Conference Name: {IEEE} Access},
  	keywords = {Cameras, Decision making, Deep convolutional neural networks, Dynamics, Feature extraction, firefighters, firefighting environment, infrared images, Machine learning, Object detection, Real-time systems, situational awareness},
  	file = {Bhattarai_MartíNez-Ramón_2020_A Deep Learning Framework for Detection of Targets in Thermal Images to Improve.pdf:/Users/jaime/Zotero/storage/K88JC9JP/Bhattarai_MartíNez-Ramón_2020_A Deep Learning Framework for Detection of Targets in Thermal Images to Improve.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/jaime/Zotero/storage/KZNYL3QP/9090877.html:text/html},
  }

  @inproceedings{Bhattarai2020a,
  	title = {An embedded deep learning system for augmented reality in firefighting applications},
  	doi = {10.1109/ICMLA51294.2020.00193},
  	abstract = {Firefighting is a dynamic activity, in which numerous operations occur simultaneously. Maintaining situational awareness (i.e., knowledge of current conditions and activities at the scene) is critical to the accurate decision-making necessary for the safe and successful navigation of a fire environment by firefighters. Conversely, the disorientation caused by hazards such as smoke and extreme heat can lead to injury or even fatality. This research implements recent advancements in technology such as deep learning, point cloud and thermal imaging, and augmented reality platforms to improve a firefighter's situational awareness and scene navigation through improved interpretation of that scene. We have designed and built a prototype embedded system that can leverage data streamed from cameras built into a firefighter's personal protective equipment ({PPE}) to capture thermal, {RGB} color, and depth imagery and then deploy already developed deep learning models to analyze the input data in real time. The embedded system analyzes and returns the processed images via wireless streaming, where they can be viewed remotely and relayed back to the firefighter using an augmented reality platform that visualizes the results of the analyzed inputs and draws the firefighter's attention to objects of interest, such as doors and windows otherwise invisible through smoke and flames.},
  	eventtitle = {2020 19th {IEEE} International Conference on Machine Learning and Applications ({ICMLA})},
  	pages = {1224--1230},
  	booktitle = {2020 19th {IEEE} International Conference on Machine Learning and Applications ({ICMLA})},
  	author = {Bhattarai, Manish and Jensen-Curtis, Aura Rose and Martínez-Ramón, Manel},
  	date = {2020-12},
  	keywords = {augmented reality, Cameras, deep learning, Deep learning, embedded platform, Embedded systems, firefighting, Gears, Prototypes, Real-time systems, situational awareness, Streaming media},
  	file = {Bhattarai et al_2020_An embedded deep learning system for augmented reality in firefighting.pdf:/Users/jaime/Zotero/storage/D9H2WZ5T/Bhattarai et al_2020_An embedded deep learning system for augmented reality in firefighting.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/jaime/Zotero/storage/HWRBI48M/9356175.html:text/html},
  }

  @article{Gorjestani2003,
  	title = {Driver Assistive Systems for Snowplows},
  	url = {http://conservancy.umn.edu/handle/11299/901},
  	abstract = {A comprehensive driver assistive system which utilizes dual frequency, carrier phase real time kinematic ({RTK}) differential global positioning system ({DGPS}), high accuracy digital geospatial databases, advanced automotive radar, and a driver interface with visual, haptic, and audible components has been used to assist specialty vehicle operators perform their tasks under these low visibility conditions. The system is able to provide a driver with high fidelity representations of the local geospatial landscape through a custom designed Head Up Display ({HUD}). Lane boundaries, turn lanes, intersections, mailboxes, and other elements of the geospatial landscape, including those sensed by automotive radar, are projected onto the {HUD} in the proper perspective. This allows a driver to safely guide his or her vehicle in low to zero visibility conditions in a desired lane while avoiding collisions. Four areas of research, are described herein: driver assistive displays, the integration of a geospatial database for improved radar processing, snowplow dynamics for slippery conditions, and a virtual bumper based collision avoidance/gang plowing system. (Gang plowing is the flying in formation of snowplows as a means to rapidly clear multilane roads.) Results from this research have vastly improved the performance and reliability of the driver assistive system. Research on the use of a specialized driver assistance system to assist specialty vehicle operators in low visibility conditions, including the design of a custom Head Up Display ({HUD}) projecting elements of the landscape in proper perspective. Driver assistive displays, the integration of a geospatial database for improved radar processing, snowplow dynamics for slippery conditions, and a virtual bumper based on collision avoidance/gang plowing system are discussed.},
  	author = {Gorjestani, Alec and Alexander, Lee and Newstrom, Bryan and Cheng, Pi-Ming and Sergi, Mike and Shankwitz, Craig and Donath, Max},
  	urldate = {2021-09-22},
  	date = {2003-03-01},
  	langid = {english},
  	note = {Accepted: 2007-08-08T21:09:57Z},
  	file = {Gorjestani et al_2003_Driver Assistive Systems for Snowplows.pdf:/Users/jaime/Zotero/storage/K7M5GASF/Gorjestani et al_2003_Driver Assistive Systems for Snowplows.pdf:application/pdf;Snapshot:/Users/jaime/Zotero/storage/D5X79LGH/901.html:text/html},
  }

  @article{Khetarpal2018a,
  	title = {Attend Before you Act: Leveraging human visual attention for continual learning},
  	url = {http://arxiv.org/abs/1807.09664},
  	shorttitle = {Attend Before you Act},
  	abstract = {When humans perform a task, such as playing a game, they selectively pay attention to certain parts of the visual input, gathering relevant information and sequentially combining it to build a representation from the sensory data. In this work, we explore leveraging where humans look in an image as an implicit indication of what is salient for decision making. We build on top of the {UNREAL} architecture in {DeepMind} Lab's 3D navigation maze environment. We train the agent both with original images and foveated images, which were generated by overlaying the original images with saliency maps generated using a real-time spectral residual technique. We investigate the effectiveness of this approach in transfer learning by measuring performance in the context of noise in the environment.},
  	journaltitle = {{arXiv}:1807.09664 [cs]},
  	author = {Khetarpal, Khimya and Precup, Doina},
  	urldate = {2021-09-22},
  	date = {2018-07-25},
  	eprinttype = {arxiv},
  	eprint = {1807.09664},
  	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
  	file = {arXiv.org Snapshot:/Users/jaime/Zotero/storage/KA57MHHH/1807.html:text/html;Khetarpal_Precup_2018_Attend Before you Act.pdf:/Users/jaime/Zotero/storage/5R82BNES/Khetarpal_Precup_2018_Attend Before you Act.pdf:application/pdf},
  }

  @article{Fischler1981,
  	title = {Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography},
  	volume = {24},
  	issn = {0001-0782},
  	url = {https://doi.org/10.1145/358669.358692},
  	doi = {10.1145/358669.358692},
  	shorttitle = {Random sample consensus},
  	abstract = {A new paradigm, Random Sample Consensus ({RANSAC}), for fitting a model to experimental data is introduced. {RANSAC} is capable of interpreting/smoothing data containing a significant percentage of gross errors, and is thus ideally suited for applications in automated image analysis where interpretation is based on the data provided by error-prone feature detectors. A major portion of this paper describes the application of {RANSAC} to the Location Determination Problem ({LDP}): Given an image depicting a set of landmarks with known locations, determine that point in space from which the image was obtained. In response to a {RANSAC} requirement, new results are derived on the minimum number of landmarks needed to obtain a solution, and algorithms are presented for computing these minimum-landmark solutions in closed form. These results provide the basis for an automatic system that can solve the {LDP} under difficult viewing},
  	pages = {381--395},
  	number = {6},
  	journaltitle = {Communications of the {ACM}},
  	shortjournal = {Commun. {ACM}},
  	author = {Fischler, Martin A. and Bolles, Robert C.},
  	urldate = {2021-09-22},
  	date = {1981-06-01},
  	keywords = {automated cartography, camera calibration, image matching, location determination, model fitting, scene analysis},
  	file = {Fischler_Bolles_1981_Random sample consensus.pdf:/Users/jaime/Zotero/storage/AIYULFTR/Fischler_Bolles_1981_Random sample consensus.pdf:application/pdf},
  }

  @article{Achanta2012,
  	title = {{SLIC} Superpixels Compared to State-of-the-Art Superpixel Methods},
  	volume = {34},
  	issn = {1939-3539},
  	doi = {10.1109/TPAMI.2012.120},
  	abstract = {Computer vision applications have come to rely increasingly on superpixels in recent years, but it is not always clear what constitutes a good superpixel algorithm. In an effort to understand the benefits and drawbacks of existing methods, we empirically compare five state-of-the-art superpixel algorithms for their ability to adhere to image boundaries, speed, memory efficiency, and their impact on segmentation performance. We then introduce a new superpixel algorithm, simple linear iterative clustering ({SLIC}), which adapts a k-means clustering approach to efficiently generate superpixels. Despite its simplicity, {SLIC} adheres to boundaries as well as or better than previous methods. At the same time, it is faster and more memory efficient, improves segmentation performance, and is straightforward to extend to supervoxel generation.},
  	pages = {2274--2282},
  	number = {11},
  	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
  	author = {Achanta, Radhakrishna and Shaji, Appu and Smith, Kevin and Lucchi, Aurelien and Fua, Pascal and Süsstrunk, Sabine},
  	date = {2012-11},
  	note = {Conference Name: {IEEE} Transactions on Pattern Analysis and Machine Intelligence},
  	keywords = {Approximation algorithms, clustering, Clustering algorithms, Complexity theory, Image color analysis, Image edge detection, Image segmentation, k-means, Measurement uncertainty, segmentation, Superpixels},
  	file = {Achanta et al_2012_SLIC Superpixels Compared to State-of-the-Art Superpixel Methods.pdf:/Users/jaime/Zotero/storage/VYQ8YRE5/Achanta et al_2012_SLIC Superpixels Compared to State-of-the-Art Superpixel Methods.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/jaime/Zotero/storage/AZC59D7U/6205760.html:text/html},
  }






</script>
<script src="lib/blazy.js"></script>
<script>
  // blazy code
  var bLazy = new Blazy({
    success: function(){
      updateCounter();
    }
  });

  // not needed, only here to illustrate amount of loaded images
  var imageLoaded = 0;

  function updateCounter() {
    imageLoaded++;
    console.log("blazy image loaded: "+imageLoaded);
  }
</script>
